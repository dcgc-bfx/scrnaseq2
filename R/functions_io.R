# 10x feature types and assay names
Assays_10x = list("Gene Expression" = "RNA",
                  "Multiplexing Capture" = "MPLX",
                  "VDJ T" = "VDJT",
                  "VDJ B" = "VDJB",
                  "Antibody Capture" = "ADT",
                  "Peaks" = "ATAC",
                  "Motifs" = "TFMotifs",
                  "CRISPR Guide Capture" = "CRISPR",
                  "Antigen Capture" = "BEAM",
                  "Custom" = "CUSTOM",
                  "Negative Control Codeword" = "ControlCodeword",
                  "Negative Control Probe" = "ControlProbe",
                  "Unassigned Codeword" = "BlankCodeword",
                  "Blank Codeword" = "BlankCodeword")

Assays_Scale = list("Gene Expression" = "RNA",
                  "Antibody Capture" = "ADT",
                  "Chromatin Accessibility" = "ATAC",
                  "CRISPR Guide Capture" = "CRISPR")

Assays_Parse = list("Gene Expression" = "RNA",
                    "Antibody Capture" = "ADT",
                    "Chromatin Accessibility" = "ATAC",
                    "CRISPR Guide Capture" = "CRISPR")

Assays_Smartseq = list("Gene Expression" = "RNA",
                    "Antibody Capture" = "ADT",
                    "Chromatin Accessibility" = "ATAC")

#' Reads metadata from an anndata object in hdf5 format (generated by scanpy).
#' 
#' Largely copied and adapted from Azimuth::LoadH5ADobs!
#' 
#' @param h5ad_file Path to an anndata object in hdf5 format
#' @param type Can be 'obs' (for barcodes) or 'var' (for features).
#' @return Metadata (data.frame format)
ReadMetadata_h5ad = function(h5ad_file, type) {
  # Checks
  assertthat::is.readable(h5ad_file)
  
  # Open hdf5 file with anndata object
  hdf5_fh = hdf5r::H5File$new(h5ad_file, mode = "r+")
  hd5_data = hdf5_fh[[paste0("/", type)]]
  
  # Create metadata matrix
  index.var = hdf5r::h5attr(hd5_data, "_index")
  index = hd5_data[[index.var]][]
  groups = names(hd5_data)
  matrix = as.data.frame(x = matrix(
    data = NA,
    nrow = length(index),
    ncol = length(groups)
  ))
  colnames(matrix) = groups
  rownames(matrix) = index
  
  # Get columns and values
  if ("__categories" %in% names(x = hd5_data)) {
    hd5_data_cate = hd5_data[["__categories"]]
    for (i in seq_along(groups)) {
      g.i = groups[i]
      value_i = hd5_data[[g.i]][]
      if (g.i %in% names(x = hd5_data_cate)) {
        value_i = factor(x = value_i, labels = hd5_data[[g.i]][])
      }
      matrix[, i] = value_i
    }
  } else {
    for (i in seq_along(groups)) {
      g.i = groups[i]
      if (all(names(hd5_data[[g.i]]) == c("categories", "codes"))) {
        if (length(unique(hd5_data[[g.i]][["codes"]][])) == length(hd5_data[[g.i]][["categories"]][])) {
          value_i = factor(x = hd5_data[[g.i]][["codes"]][], labels = hd5_data[[g.i]][["categories"]][])
        }
        else {
          value_i = hd5_data[[g.i]][["codes"]][]
        }
      }
      else {
        value_i = tryCatch(expr = hd5_data[[g.i]][],
                           error=function(e) return("unknown")
        )
      }
      matrix[, i] = value_i
    }
  }
  
  hdf5_fh$close_all()
  return(as.data.frame(matrix))
}

#' Reads metadata from a character-separated file.
#' 
#' @param h5ad_file Path to a character-separated file. First column must contain the respective barcode or feature id.
#' @return Metadata (data.frame format)
ReadMetadata_csv = function(csv_file) {
  # Checks
  assertthat::is.readable(csv_file)
  
  # Read table
  meta_data = readr::read_delim(csv_file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types=readr::cols())
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatString("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Reads metadata from an Excel file.
#' 
#' @param excel_file Path to an Excel file. First column must contain the respective barcode or feature id.
#' @param sheet Sheet number. description
#' @return Metadata (data.frame format)
ReadMetadata_excel = function(excel_file, sheet=1) {
  # Checks
  assertthat::is.readable(excel_file)
  
  # Read table
  meta_data = readxl::read_excel(excel_file, sheet=sheet, col_names=TRUE)
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatString("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Reads metadata from a metadata table saved as R rds file.
#' 
#' This method preserves factor levels.
#' 
#' @param rds_file Path to an R rds file. First column must contain the respective barcode or feature id.
#' @return Metadata (data.frame format)
ReadMetadata_rds = function(rds_file) {
  # Checks
  assertthat::is.readable(rds_file)
  
  # Read table
  meta_data = readRDS(rds_file)
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatString("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Reads metadata.
#' 
#' @param file Path to a character-separated file (csv, tsv, csv.gz, tsv.gz), an Excel file (xls, xslx) or an R rds file containing a table (preserves factor levels). First column must contain the respective barcode or feature id. For Excel files, a sheet can be specified by appending ':<sheet_number>'.
#' @return Metadata (data.frame format)
ReadMetadata = function(file) {
  # Sheet number appended?
  sheet = 1
  if (grepl(":\\d+$", file)) {
    sheet = gsub(pattern=".+:(\\d+)$", replacement="\\1", x=file)
    file = gsub(pattern=":\\d+$", replacement="", x=file)
  }
  
  # Checks
  extension = tools::file_ext(gsub(pattern="\\.gz$", replacement="", x=file))
  valid_extensions = c("csv", "tsv", "xls", "xlsx", "rds")
  assertthat::assert_that(extension %in% valid_extensions,
                          msg=FormatString("Metadata file type must be: {valid_extensions*} (file can be gzipped)."))
  
  # Read metadata
  if (extension %in% c("csv", "tsv")) {
    meta_data = ReadMetadata_csv(file)
  } else if(extension %in% c("xls", "xlsx")) {
    meta_data = ReadMetadata_excel(file, sheet=sheet)
  } else if(extension %in% c("rds")) {
    meta_data = ReadMetadata_rds(file)
  }
  
  # Assert that it is not empty
  assertthat::assert_that(assertthat::not_empty(meta_data),
                          msg=FormatString("Metadata file {file} is empty."))

  return(meta_data)
}

#' Reads the datasets table.
#' 
#' TODO: Code may be a bit redundant with ReadMetadata. On the other hand, it does some specific checks.
#' 
#' @param file Path to a character-separated file (csv, tsv, csv.gz, tsv.gz) or an Excel file (xls, xslx). For Excel files, a sheet can be specified by appending ':<sheet_number>'. Needs to contain the following columns:
#' @return Metadata (data.frame format)
ReadDatasetsTable = function(file) {
  # Sheet number appended?
  sheet = 1
  if (grepl(":\\d+$", file)) {
    sheet = gsub(pattern=".+:(\\d+)$", replacement="\\1", x=file)
    file = gsub(pattern=":\\d+$", replacement="", x=file)
  }
  
  # Checks
  extension = tools::file_ext(gsub(pattern="\\.gz$", replacement="", x=file))
  valid_extensions = c("csv", "tsv", "xls", "xlsx", "rds")
  assertthat::assert_that(extension %in% valid_extensions,
                          msg=FormatString("Datasets file type must be: {valid_extensions*} (file can be gzipped)."))
  
  # Read datasets table
  if (extension %in% c("csv", "tsv")) {
    datasets_table = readr::read_delim(file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types="text")
  } else if(extension %in% c("xls", "xlsx")) {
    datasets_table = readxl::read_excel(file, sheet=sheet, col_names=TRUE)
  }
  assertthat::assert_that(assertthat::not_empty(datasets_table),
                          msg=FormatString("Datasets file {file} is empty."))
  
  # Check that all columns are present
  
  
  return(datasets_table)
}

#' Reads counts from a character-separated file.
#' 
#' @param csv_file Path to a character-separated counts file. First column contains the feature id (barcode id if transpose is set), all other columns contain the barcode (feature) counts.
#' @param transpose If TRUE then rows are barcodes and columns are features (default: FALSE)
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL) 
#' @return Sparse counts matrix (dgCMatrix format). The dataset path is attached as attribute.
ReadCounts_csv = function(csv_file, transpose=FALSE, strip_suffix=NULL) {
  library(magrittr)
  
  # Checks
  assertthat::is.readable(csv_file)
  
  # Read character-separated counts table with gene id in the first column and counts in the other columns 
  counts_data = readr::read_delim(csv_file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types=readr::cols())
  row_ids = counts_data[, 1, drop=TRUE]
  col_ids = colnames(counts_data)
  col_ids = col_ids[-1]
  
  # Check that barcodes and features are unique
  assertthat::assert_that(sum(duplicated(col_ids)) == 0,
                          msg=FormatString("Dataset {csv_file} contains at least two barcodes with the same name."))
  
  assertthat::assert_that(sum(duplicated(row_ids)) == 0,
                          msg=FormatString("Dataset {csv_file} contains at least two features with the same name.")) 
  
  # Strip suffix from barcodes if requested
  if (!is.null(strip_suffix)) {
    col_ids = trimws(col_ids, which="right", whitespace=strip_suffix)
  }
  
  assertthat::assert_that(sum(duplicated(col_ids)) == 0,
                          msg=FormatString("Dataset {csv_file} contains at least two barcodes with the same name after removing the suffix {strip_suffix}."))
  
  # Check that all columns are numeric
  is_numeric = sapply(counts_data, is.numeric)
  assertthat::assert_that(all(is_numeric[-1]),
                          msg=FormatString("There are non-numeric columns in dataset {csv_file}! Only the first column may be non-numeric."))
  
  
  # Create sparse matrix
  if (transpose) {
    counts_data = Matrix::Matrix(data=counts_data[, -1, drop=FALSE] %>% as.matrix() %>% t(), 
                                 sparse=TRUE,
                                 dimnames = list(col_ids, row_ids))
  } else {
    counts_data = Matrix::Matrix(data=counts_data[, -1, drop=FALSE] %>% as.matrix(), 
                                 sparse=TRUE,
                                 dimnames = list(row_ids, col_ids))
  }
  
  # Attach path
  attr(counts_data, "path") = csv_file
  
  return(list(All=counts_data))
}
  
#' Reads counts that are in market exchange format.
#' 
#' @param mtx_directory Path to counts directory in market exchange format.
#' @param mtx_file_name Name of the matrix mtx file (default: matrix.mtx.gz)
#' @param transpose If TRUE then rows are cells and columns are genes (default: FALSE)
#' @param barcodes_file_name Name of the barcodes character-separated file (default: barcodes.tsv.gz)
#' @param barcodes_column_names How to name the columns in the barcodes file. When TRUE, use the first line as header. When FALSE, use generic names. Alternatively, a character vector with column names can be provided (default: FALSE).
#' @param features_file_name Name of the features character-separated file (default: features.tsv.gz)
#' @param features_column_names How to name the columns in the features file. When TRUE, use the first line as header. When FALSE, use generic names. Alternatively, a character vector with column names can be provided (default: FALSE).
#' @param feature_type_column If there is data for multiple feature types, which column (number) in the features file is used to identify the type. If there is no column, set to NULL and type will be "Gene Expression" (default: NULL)
#' @param delim Delimiter used in barcodes and feature files (default: \t)
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL) 
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes, features and path is attached as attributes.
ReadCounts_mtx = function(mtx_directory, mtx_file_name="matrix.mtx.gz", transpose=FALSE, barcodes_file_name="barcodes.tsv.gz", barcodes_column_names=FALSE, features_file_name="features.tsv.gz", features_column_names=FALSE, feature_type_column=NULL, delim="\t", strip_suffix=NULL) {
  # Checks
  for(f in file.path(mtx_directory, c(mtx_file_name, barcodes_file_name, features_file_name))) assertthat::is.readable(f)
  
  # Read market exchange format file
  counts_data = Matrix::readMM(file=file.path(mtx_directory, mtx_file_name))
  if (transpose) {
    counts_data = Matrix::t(counts_data)
  }
  counts_data = as(counts_data, "dgCMatrix")
  
  # Read barcodes file
  barcodes_data = readr::read_delim(file=file.path(mtx_directory, barcodes_file_name), col_names=barcodes_column_names, delim=delim, progress=FALSE, show_col_types=FALSE)
  barcodes_col_nms = colnames(barcodes_data)
  if (is.logical(barcodes_column_names) && barcodes_column_names==FALSE) {
    barcodes_col_nms = rep("barcode_info", length(barcodes_col_nms)) %>% make.unique(sep="_")

  }
  barcodes_col_nms[1] = "orig_barcode"
  colnames(barcodes_data) = barcodes_col_nms
  barcodes_data = as.data.frame(barcodes_data)
  
  if (!is.null(strip_suffix)) {
    rownames(barcodes_data) = trimws(barcodes_data[, 1, drop=TRUE], which="right", whitespace=strip_suffix)
  } else {
    rownames(barcodes_data) = barcodes_data[, 1, drop=TRUE]
  }
  
  # Read features file
  features_data = readr::read_delim(file=file.path(mtx_directory, features_file_name), col_names=features_column_names, delim=delim, progress=FALSE, show_col_types=FALSE)
  feature_col_nms = colnames(features_data)
  if (is.logical(features_column_names) && features_column_names==FALSE)  {
    feature_col_nms = make.unique(rep("feature_info", length(feature_col_nms)), sep="_")
    
  }
  feature_col_nms[1] = "feature_id"
  colnames(features_data) = feature_col_nms
  features_data = as.data.frame(features_data)
  rownames(features_data) = features_data[, 1, drop=TRUE]
  
  # Split by feature type column
  feature_sets = list("All" = rep(TRUE, nrow(features_data)))
  if (!is.null(feature_type_column)) {
    feature_types = unique(features_data[, feature_type_column, drop=TRUE])
    
    feature_sets = purrr::map(feature_types, function(f) {
      return(features_data[, feature_type_column, drop=TRUE] == f)
    })
    names(feature_sets) = feature_types
  }
  
  # Generate list of matrix (matrices)
  counts_lst = purrr::map(feature_sets, function(f) {
    # Add counts
    cts = counts_data[f, ]
    col_ids = rownames(barcodes_data[, 1, drop=FALSE])
    row_ids = rownames(features_data[f, 1, drop=FALSE])
    dimnames(cts) = list(row_ids, col_ids)
    
    # Add barcode metadata
    bc_meta = barcodes_data[colnames(cts), , drop=FALSE]
    attr(cts, "barcode_metadata") = bc_meta
    
    # Add feature metadata
    fc_meta = features_data[f, , drop=FALSE]
    keep = purrr::map_lgl(colnames(fc_meta), function(n) {
      return(all(!is.na(fc_meta[, n, drop=TRUE]) & nchar(fc_meta[, n, drop=TRUE]) > 0))
    })
    attr(cts, "feature_metadata") = fc_meta[, keep, drop=FALSE]
    
    # Attach path
    attr(cts, "path") = mtx_directory
    
    return(cts)
  })
  names(counts_lst) = names(counts_lst)
  
  return(counts_lst)
}

#' Reads counts from an anndata object in hdf5 format (generated by scanpy and co).
#' 
#' Note: This function does not read the entire counts data into memory. Instead it returns an iterator object that can be used to
#' retrieve values directly from file (random access). It is recommend to convert this object into a BPcells on-disk storage object.
#' 
#' Does not discriminate between feature types.
#' 
#' @param h5ad_file Path to an anndata object in hdf5 format.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL) 
#' @return Sparse counts matrix (IterableMatrix format). Additional information on barcodes, features and path is attached as attributes.
ReadCounts_h5ad = function(h5ad_file, strip_suffix=NULL) {
  library(magrittr)
  
  # Checks
  assertthat::is.readable(h5ad_file)
  
  # Read barcodes and features data separately
  barcodes_data = ReadMetadata_h5ad(h5ad_file=h5ad_file, type='obs')
  barcodes_col_nms = colnames(barcodes_data)
  barcodes_col_nms[1] = "orig_barcode"
  colnames(barcodes_data) = barcodes_col_nms
  if (!is.null(strip_suffix)) {
    rownames(barcodes_data) = trimws(barcodes_data[, 1, drop=TRUE], which="right", whitespace=strip_suffix)
  } else {
    rownames(barcodes_data) = barcodes_data[, 1, drop=TRUE]
  }
  
  
  features_data = ReadMetadata_h5ad(h5ad_file=h5ad_file, type='var')
  features_data = features_data %>% dplyr::select(feature_id=gene_id,
                                  feature_name=gene_name,
                                  feature_type=gene_id,
                                  setdiff(colnames(features_data), c("gene_id", "gene_name")))
  features_data$feature_type = NA
  rownames(features_data) = features_data$feature_id
  
  # Read counts and attach barcodes/features data
  counts_data=BPCells::open_matrix_anndata_hdf5(h5ad_file)
  rownames(counts_data) = rownames(features_data)
  colnames(counts_data) = rownames(barcodes_data)
  attr(counts_data, "barcode_metadata") = barcodes_data
  attr(counts_data, "feature_metadata") = features_data
  
  # Attach path
  attr(counts_data, "path") = h5ad_file
  
  return(list(All=counts_data))
}


#' Reads counts data produced by plate-based methods like SmartSeq.
#' 
#' @param path Path to counts data. Can be a character-separated file or a matrix exchange format directory (with files matrix.mtx.gz, barcodes.tsv.gz and features.tsv.gz).
#' @param assays This simply sets the assay. Smartseq technologies currently do not support multi-assay datasets.
#' @param version Set to '2' for Smartseq2 or '3' for Smartseq3.
#' @param transpose  If TRUE then rows are cells and columns are genes (default: FALSE)
#' @return A sparse counts matrix (dgCMatrix format). Additional information on barcodes, features, assay technology and path is attached as attributes.
ReadCounts_SmartSeq = function(path, assays, version, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)
  assertthat::assert_that(version %in% c("2", "3"),
                          msg="Smartseq version must be '2' or '3'.")
  
  
  # Convert to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_Smartseq), Assays_Smartseq)
  valid_assays = names(assay_to_feature_type)
  assertthat::assert_that(assays %in% valid_assays,
                          msg=FormatString("'{assays*} must be: {valid_assays*}."))

  if (dir.exists(path)) {
    # market exchange format
    counts_lst = ReadCounts_mtx(mtx_directory=path,
                   transpose=transpose,
                   mtx_directory="matrix.mtx.gz",
                   barcodes_file_name="barcodes.tsv.gz",
                   barcodes_column_names=FALSE,
                   features_file_name="features.tsv.gz",
                   features_column_names=FALSE,
                   feature_type_column=NULL,
                   delim="\t",
                   strip_suffix=NULL)
    counts_lst = counts_lst[1]
  } else {
    # character-separated file
    counts_lst = ReadCounts_csv(csv_file=path, transpose=transpose)
    barcode_metadata = data.frame(orig_barcode=colnames(counts_lst[[1]]),
                                  row.names=colnames(counts_lst[[1]]))
    feature_metadata = data.frame(feature_id=rownames(counts_lst[[1]]),
                                  row.names=rownames(counts_lst[[1]]))
    attr(counts_lst[[1]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[1]], "feature_metadata") = feature_metadata
  }
  
  # Assay
  names(counts_lst) = assays[1]
  
  # Add attributes technology and assay
  attr(counts_lst[[1]], "technology") = paste0("Smartseq", version)
  attr(counts_lst[[1]], "assay") = assays[1]
  
  return(counts_lst)
}

#' Reads 10x counts that are in market exchange format.
#' 
#' @param mtx_directory Path to 10x counts directory in market exchange format.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes, features and path is attached as attributes.
ReadCounts_10x_mtx = function(mtx_directory, strip_suffix=NULL) {
  # Determine the name of the matrix file
  mtx_file_name = dplyr::case_when(file.exists(file.path(mtx_directory, "matrix.mtx.gz")) ~ "matrix.mtx.gz",
                                   file.exists(file.path(mtx_directory, "matrix.mtx")) ~ "matrix.mtx")
  
  # Determine the name of the barcodes file
  barcodes_file_name = dplyr::case_when(
    file.exists(file.path(mtx_directory, "barcodes.tsv.gz")) ~ "barcodes.tsv.gz",
    file.exists(file.path(mtx_directory, "barcodes.tsv")) ~ "barcodes.tsv"
  )
  
  # Determine the name of the features file
  features_file_name = dplyr::case_when(
    file.exists(file.path(mtx_directory, "features.tsv.gz")) ~ "features.tsv.gz",
    file.exists(file.path(mtx_directory, "features.tsv")) ~ "features.tsv",
    file.exists(file.path(mtx_directory, "genes.tsv")) ~ "genes.tsv",
    file.exists(file.path(mtx_directory, "peaks.bed")) ~ "peaks.bed",
    file.exists(file.path(mtx_directory, "motifs.tsv")) ~ "motifs.tsv"
  )
  
  # Determine the column name of the features file
  if (features_file_name == "peaks.bed") {
    # 10x atac
    features_column_names = c("chr", "start", "end")
  } else if (features_file_name == "motifs.tsv") {
    # 10x atac
    features_column_names = c("tf_full_name", "tf_name")
  } else {
    feature_metadata = readr::read_delim(file.path(mtx_directory, features_file_name), delim="\t", col_names=FALSE, n_max=3, progress=FALSE, show_col_types=FALSE)
    
    if (ncol(feature_metadata) == 6) {
      # 10x multiome
      features_column_names = c("feature_id",
                                "feature_name",
                                "feature_type",
                                "chr",
                                "start",
                                "end")
    } else {
      # 10x other
      features_column_names = c("feature_id", "feature_name", "feature_type")
    }
  }
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=FALSE,
    features_file_name=features_file_name,
    features_column_names=features_column_names,
    feature_type_column=3,
    delim="\t",
    strip_suffix=strip_suffix
  )
  
  return(counts_lst)
}

#' Reads 10x counts that are in hdf5 format.
#' 
#' Note: This function does not read the entire counts data into memory. Instead it returns an iterator object that can be used to
#' retrieve values directly from file (random access). It is recommend to convert this object into a BPcells on-disk storage object.
#' 
#' @param h5_file Path to a 10x h5 counts file.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix per feature type (IterableMatrix format). Additional information on barcodes, features and path is attached as attributes.
ReadCounts_10x_h5 = function(h5_file, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(h5_file)
  
  # Read barcodes and features data separately
  hdf5_fh = hdf5r::H5File$new(h5_file, mode = "r+")
  
  # No barcodes data, just a vector of the barcodes
  h5_group = names(hdf5_fh)[1]
  h5_group = hdf5_fh[[h5_group]]
  barcodes = h5_group[["barcodes"]][]
  if (!is.null(strip_suffix)) {
    barcodes_data = data.frame(row.names=trimws(barcodes, which="right", whitespace=strip_suffix), orig_barcode=barcodes)
  } else {
    barcodes_data = data.frame(row.names=barcodes, orig_barcode=barcodes)
  }
  
  # Read feature data
  if ("features" %in% names(h5_group)) {
    hdf5_features = h5_group[["features"]]
    feature_id=hdf5_features[["id"]][]
    feature_name=hdf5_features[["name"]][]
    feature_type=hdf5_features[["feature_type"]][]
  } else if ("genes" %in% names(h5_group)) {
    hdf5_features = h5_group[["genes"]]
    feature_id=h5_group[["genes"]][]
    feature_name=h5_group[["gene_names"]][]
    feature_type="Gene Expression"
  }
  
  features_data = data.frame(
    feature_id, feature_name, feature_type
  )
  
  if ("_all_tag_keys" %in% names(hdf5_features)) {
    non_standard_features = hdf5_features[["_all_tag_keys"]][]
  } else {
    non_standard_features = c()
  }

  if (length(non_standard_features) > 0) {
    non_standard_features_data = purrr::map(non_standard_features, function(f) {
      return(hdf5_features[[f]][])
    })
    names(non_standard_features_data) = non_standard_features
    
    if ("interval" %in% names(non_standard_features_data)) {
      interval_data = list(
        chr = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\1",
          non_standard_features_data[["interval"]]
        ),
        start = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\2",
          non_standard_features_data[["interval"]]
        ),
        end = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\3",
          non_standard_features_data[["interval"]]
        )
      )
      
      idx = which(names(non_standard_features_data) == "interval")
      if (idx == 1) {
        pre_idx = c()
      } else {
        pre_idx = 1:(idx - 1)
      }
      if (idx == length(non_standard_features_data)) {
        post_idx = c()
      } else {
        post_idx = (idx + 1):length(non_standard_features_data)
      }
      non_standard_features_data = c(non_standard_features_data[pre_idx],
                                     interval_data,
                                     non_standard_features_data[post_idx])
    }
    features_data = cbind(features_data, non_standard_features_data)
  }
  rownames(features_data) = features_data$feature_id
  hdf5_fh$close_all()
  
  feature_types = unique(features_data$feature_type)
  counts_lst = purrr::map(feature_types, function(f) {
    # Subset counts
    if (length(feature_types) > 1) {
      cts = BPCells::open_matrix_10x_hdf5(h5_file, feature_type=f)
    } else {
      cts = BPCells::open_matrix_10x_hdf5(h5_file)
    }
    
    # Strip barcode suffix
    if (!is.null(strip_suffix)) {
      colnames(cts) = trimws(colnames(cts), which="right", whitespace=strip_suffix)
    }
    
    # Add barcode metadata
    bc_meta = barcodes_data[colnames(cts), , drop=FALSE]
    attr(cts, "barcode_metadata") = bc_meta
    
    # Add feature metadata
    fc_meta = features_data[features_data$feature_type == f, , drop=FALSE]
    keep = purrr::map_lgl(colnames(fc_meta), function(n) {
      return(all(!is.na(fc_meta[, n, drop=TRUE]) & nchar(fc_meta[, n, drop=TRUE]) > 0))
    })
    attr(cts, "feature_metadata") = fc_meta[, keep, drop=FALSE]
    
    # Attach path
    attr(cts, "path") = h5_file
    
    return(cts)
  })
  names(counts_lst) = feature_types
  
  return(counts_lst)
}

#' Reads counts data produced by 10x (non-spatial datasets).
#' 
#' @param path Path to 10x counts data. Can be a 10x hdf5 file (recommended for big datasets) or a 10x matrix exchange format directory.
#' @param assays Which assays to read. Default NULL is to read all assays.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix per assay. Format is either IterableMatrix (when reading a h5 file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes, features, assay, technology and path is attached as attributes.
ReadCounts_10x = function(path, assays=NULL, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # Converts assay to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_10x), Assays_10x)
  feature_type_to_assay = unlist(Assays_10x)
  valid_assays = names(assay_to_feature_type)
  
  # If assays are specified, check that they are valid
  if (!is.null(assays)) {
    assertthat::assert_that(all(assays %in% valid_assays),
                            msg=FormatString("'{assays} must be: {valid_assays*}."))
  }

  # Read counts
  if (dir.exists(path)) {
    # 10x market exchange format
    counts_lst = ReadCounts_10x_mtx(mtx_directory=path, strip_suffix=strip_suffix)
  } else {
    # 10x h5 file
    counts_lst = ReadCounts_10x_h5(h5_file=path, strip_suffix=strip_suffix)
  }
  
  # 10x Xenium hack: the assay BlankCodeword can be feature type "Unassigned Codeword" (old) and "Blank Codeword" (new)
  if ("BlankCodeword" %in% assays) {
    if ("Unassigned Codeword" %in% names(counts_lst)) {
      d = assay_to_feature_type == "Blank Codeword"
    } else if ("Blank Codeword" %in% names(counts_lst)) {
      d = assay_to_feature_type == "Unassigned Codeword"
    }
    assay_to_feature_type = assay_to_feature_type[!d]
  }
  
  # Subset feature types (which correspond to assays)
  if (!is.null(assays)) {
    feature_types = assay_to_feature_type[assays]
    f = feature_types %in% names(counts_lst)
    assertthat::assert_that(all(f),
                            msg=FormatString("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
    counts_lst = counts_lst[feature_types]
  }
  
  # Change names from feature type to assay
  feature_types = names(counts_lst)
  f = feature_types %in% names(feature_type_to_assay)
  assertthat::assert_that(all(f),
                          msg=FormatString("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_10x in functions_io.R."))
  names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "10x"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Reads counts data produced by the 10x Visium platform.
#' 
#' @param path Path to 10x counts data for 10x Visium. Can be a 10x hdf5 file (recommended for big datasets) or a 10x matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return List with a counts matrix per assay. Format is either IterableMatrix (when reading hdf5) or dgCMatrix (when reading matrix exchange format directory). Additional information on barcodes, features, assay and technology and path is attached as attributes.
ReadCounts_10xVisium = function(path, assays=NULL, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # Read counts
  counts_lst = ReadCounts_10x(path, assays=assays, strip_suffix=strip_suffix)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_visium"
  }
  
  return(counts_lst)
}

#' Reads counts data produced by the 10x VisiumHD platform.
#' 
#' @param path Path to the 'binned_outputs' directory. Bin sizes specified by the bin_sizes parameter are read (hdf5 files only).
#' @param assays Which assays to read. If NULL, read all assays.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @param bin_sizes Bin sizes to read (default: c(8, 16)). If set to NULL, read all bin sizes.
#' @return List with a counts matrix per assay and bin size. Format is either IterableMatrix (when reading hdf5) or dgCMatrix (when reading matrix exchange format directory). Additional information on barcodes, features, assay and technology and path is attached as attributes.
ReadCounts_10xVisiumHD = function(path, assays=NULL, strip_suffix=NULL, bin_sizes=c(8, 16)) {
  # Checks
  assertthat::is.readable(path)
  
  # If set to NULL, collect all available bin sizes
  if (is.null(bin_sizes)) {
    subdirs = list.dirs(path, full.names=FALSE, recursive=FALSE)
    subdirs = subdirs[grepl("square_\\d+um", subdirs)]
    bin_sizes = gsub("square_(\\d+)um", "\\1", subdirs) %>% as.integer()
  }
  
  # Expand paths for bin sizes and check whether they exist
  bin_sizes = sprintf("%03d", bin_sizes)
  paths = file.path(path, paste0("square_", bin_sizes, "um"), "filtered_feature_bc_matrix.h5")
  for(p in paths) {
    assertthat::is.readable(p)
  }

  # Read counts for all bin sizes; the result is a list of bin sizes where each entry is a list of assays read for each bin size
  counts_lst = purrr::map(paths, ReadCounts_10x, assays=assays, strip_suffix=strip_suffix)
  
  # For multiple bin sizes, rename the assays accordingly
  if (length(bin_sizes) > 1) {
    for (i in seq_along(counts_lst)) {
      names(counts_lst[[i]]) = paste0(names(counts_lst[[i]]), bin_sizes[i])
      for (j in seq_along(counts_lst[[i]])) {
        attr(counts_lst[[i]][[j]], "assay") = paste0(attr(counts_lst[[i]][[j]], "assay"), bin_sizes[i])
      }
    }
  }
  
  # Flatten the counts list
  counts_lst = purrr::flatten(counts_lst)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_visiumhd"
  }
  
  return(counts_lst)
}

#' Reads counts data produced by 10x Xenium.
#' 
#' @param path Path to 10x counts data for 10x Xenium. Can be a 10x hdf5 file (recommended for big datasets) or a 10x matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix per assay. Format is either IterableMatrix (when reading a h5 file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes, features, assay and technology and path is attached as attributes.
ReadCounts_10xXenium = function(path, assays=NULL, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)

  # Read counts
  counts_lst = ReadCounts_10x(path, assays=assays, strip_suffix=strip_suffix)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_xenium"
  }
  
  return(counts_lst)
}

#' Reads Parse Biosciences counts that are in market exchange format.
#' 
#' @param mtx_directory Path to Parse Biosciences counts directory in market exchange format. Typically contains the files count_matrix.mtx, cell_metadata.csv and all_genes.csv.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes and features is attached as attributes barcode_metadata and feature_metadata. Additional information on barcodes, features and path is attached as attributes.
ReadCounts_ParseBio_mtx = function(mtx_directory, strip_suffix=NULL) {
  # Determine the name of the matrix file
  mtx_file_name = "count_matrix.mtx"
  
  # Determine the name of the barcodes file
  barcodes_file_name = "cell_metadata.csv"
  
  # Determine the name of the features file
  features_file_name = "all_genes.csv"
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    transpose=TRUE,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=TRUE,
    features_file_name=features_file_name,
    features_column_names=TRUE,
    feature_type_column=NULL,
    delim = ",",
    strip_suffix=strip_suffix
  )
  
  return(counts_lst)
}

#' Reads Parse Biosciences counts that are in h5ad anndata format.
#' 
#' Note: This function does not read the entire counts data into memory. Instead it returns an iterator object that can be used to
#' retrieve values directly from file (random access). It is recommend to convert this object into a BPcells on-disk storage object.
#' 
#' Does not discriminate between feature types.
#' 
#' @param h5ad_file Path to an anndata object in hdf5 format.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return Sparse counts matrix (IterableMatrix format). Additional information on barcodes, features and path is attached as attributes.
ReadCounts_ParseBio_h5ad = function(h5ad_file, strip_suffix=NULL) {
  return(ReadCounts_h5ad(h5ad_file, strip_suffix=strip_suffix))
}

#' Reads counts data produced by Parse Biosciences.
#' 
#' @param path Path to Parse Biosciences counts data. Can be a Parse Biosciences anndata.h5ad file (recommended for big datasets) or a Parse Biosciences matrix exchange format directory.
#' @param assays This simply sets the assay. Parse Bioscience currently does not support multi-assay datasets.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix. Format is either IterableMatrix (when reading an anndata.h5ad file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes, features, assay and technology and path is attached as attributes.
ReadCounts_ParseBio = function(path, assays, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # If assays are specified, check that they are valid
  assay_to_feature_type = setNames(names(Assays_Parse), Assays_Parse)
  valid_assays = names(assay_to_feature_type)
  assertthat::assert_that(all(assays %in% valid_assays),
                          msg=FormatString("'{assay} must be: {valid_assays*}."))
  
  # Read counts
  if (dir.exists(path)) {
    # Parse Bio market exchange format
    counts_lst = ReadCounts_ParseBio_mtx(mtx_directory=path, strip_suffix=strip_suffix)
  } else {
    # Parse Bio anndata h5 file
    counts_lst = ReadCounts_ParseBio_h5ad(h5ad_file=path, strip_suffix=strip_suffix)
  }
  
  # No multi-assay datasets but keep for now
  #
  # Subset feature types (which correspond to assays)
  #if (!is.null(assays)) {
  #  feature_types = assay_to_feature_type[assays]
  #  f = feature_types %in% names(counts_lst)
  #  assertthat::assert_that(all(f),
  #                          msg=FormatString("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
  #  counts_lst = counts_lst[feature_types]
  #}
  
  # Change names from feature type to assay
  #feature_types = names(counts_lst)
  #f = feature_types %in% names(feature_type_to_assay)
  #assertthat::assert_that(all(f),
  #                        msg=FormatString("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_Parse in functions_io.R."))
  #names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  counts_lst = counts_lst[1]
  names(counts_lst) = assays[1]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "Parse Biosciences"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Reads Scale Bio counts that are in market exchange format.
#' 
#' @param mtx_directory Path to Scale Bio counts directory in market exchange format. Typically contains the files matrix.mtx.gz, barcodes.tsv.gz and features.tsv.gz.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes, features and path is attached as attributes.
ReadCounts_ScaleBio_mtx = function(mtx_directory, strip_suffix=NULL) {
  # Determine the name of the matrix file
  mtx_file_name = "matrix.mtx"
  
  # Determine the name of the barcodes file
  barcodes_file_name = "barcodes.tsv"
  
  # Determine the name of the features file
  features_file_name = "features.tsv"
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    transpose=FALSE,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=FALSE,
    features_file_name=features_file_name,
    features_column_names=c("feature_id", "feature_name", "feature_type"),
    feature_type_column=3,
    delim = "\t",
    strip_suffix=strip_suffix
  )
  
  return(counts_lst)
}

#' Reads counts data produced by Scale Bio
#' 
#' @param path Path to Scale Bio counts data. Must be a Scale Bio matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL).
#' @return  One sparse counts matrix per assay (dgCMatrix format). Additional information on barcodes, features, assay and technology and path is attached as attributes.
ReadCounts_ScaleBio = function(path, assays, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # Converts assay to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_Scale), Assays_Scale)
  feature_type_to_assay = unlist(Assays_Scale)
  valid_assays = names(assay_to_feature_type)
  
  # If assays are specified, check that they are valid
  if (!is.null(assays)) {
    assertthat::assert_that(all(assays %in% valid_assays),
                            msg=FormatString("'{assays} must be: {valid_assays*}."))
  }

  # Read counts
  counts_lst = ReadCounts_ScaleBio_mtx(mtx_directory=path, strip_suffix=strip_suffix)
  
  # Subset feature types (which correspond to assays)
  if (!is.null(assays)) {
    feature_types = assay_to_feature_type[assays]
    f = feature_types %in% names(counts_lst)
    assertthat::assert_that(all(f),
                            msg=FormatString("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
    counts_lst = counts_lst[feature_types]
  }
  
  # Change names from feature type to assay
  feature_types = names(counts_lst)
  f = feature_types %in% names(feature_type_to_assay)
  assertthat::assert_that(all(f),
                          msg=FormatString("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_Scale in functions_io.R."))
  names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "ScaleBio"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Reads counts data produced by Smartseq, 10x, 10x Visium, 10x VisiumHD, 10x Xenium, Parse Biosciences, Scale Bio.
#' 
#' @param path Path to counts data. Can be: character-separated file (Smartseq), matrix exchange format directory (SmartSeq, 10x, Parse Biosciences, ScaleBio), hdf5 file (10x), h5ad file (Parse Biosciences). For 10x VisiumHD, this can be either the 'binned_outputs' directory in which case all bin sizes specified by the visiumhd_bin_sizes parameter are read (hdf5 files only) or a path to the counts data of a specific bin size as matrix exchange format directory or hdf5 file.
#' @param technology Technology. Can be: 'smartseq2', 'smartseq3', '10x', '10x_visium', '10x_visiumhd', '10x_xenium', 'parse' or 'scale'.
#' @param assays If there are multiple assays in the dataset, which assay to read. Multiple assays can be specified. If there is only one assay, this simply sets the assay type.
#' @param barcode_metadata Table with additional barcode metadata. Can also be a list specifying metadata for each assay. First column must contain the barcode. Missing barcodes will be filled with NA.
#' @param feature_metadata Table with additional feature metadata. Can also be a list specifying metadata for each assay. First column must contain the feature id. Missing features will be filled with NA.
#' @param barcode_suffix Suffix to add to the barcodes (default: NULL).
#' @return  One sparse counts matrix per assay. Format can be dgCMatrix (general) or IterableMatrix (when reading an anndata.h5ad or h5 file). Additional information on barcodes, features, assay and technology and path is attached as attributes. For 10x VisiumHD data, the different bin sizes are returned as individual assays. 
ReadCounts = function(path, technology, assays, barcode_metadata=NULL, feature_metadata=NULL, barcode_suffix=NULL) {
  library(magrittr)

  # Checks
  valid_technologies = c("smartseq2", "smartseq3", "10x", "10x_visium", "10x_visiumhd", "10x_xenium", "parse", "scale")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatString("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read counts
  strip_suffix = NULL
  if (technology == "smartseq2") {
    counts_lst = ReadCounts_SmartSeq(path=path, assays=assays[1], version="2")
  } else if (technology == "smartseq3") {
    counts_lst = ReadCounts_SmartSeq(path=path, assays=assays[1], version="3")
  } else if(technology == "10x") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10x(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "10x_visium") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10xVisium(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "10x_visiumhd") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10xVisiumHD(path=path, assays=assays, strip_suffix=strip_suffix, bin_sizes=visiumhd_bin_sizes)
  } else if(technology == "10x_xenium") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10xXenium(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "parse") {
    strip_suffix = NULL
    counts_lst = ReadCounts_ParseBio(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "scale") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_ScaleBio(path=path, assays=assays, strip_suffix=strip_suffix)
  }
  
  assertthat::assert_that(assertthat::not_empty(counts_lst),
                          msg=FormatString("Count not read counts for dataset {path}, assay {assay}."))
  
  # Add barcode metadata to counts objects
  if (!is.null(barcode_metadata)) {
    # Merge list of barcode metadata files
    if (inherits(barcode_metadata, "list")) {
      # Collect all barcodes: Get first column of each table
      barcodes = purrr::map(barcode_metadata, 1) %>% unlist() %>% unique()
      # Now merge all tables
      merged_barcode_metadata = data.frame(orig_barcode=barcodes)
      for(i in seq_along(barcode_metadata)) {
        x_id = colnames(merged_barcode_metadata)[1]
        y_id = colnames(barcode_metadata[[i]])[1]
        merged_barcode_metadata = dplyr::left_join(x=merged_barcode_metadata,
                                                   y=barcode_metadata[[i]],
                                                   by=setNames(y_id, x_id),
                                                   suffix=paste0(".", c(i-1, i)))
      }
      barcode_metadata = merged_barcode_metadata
    }
    
    for(i in seq_along(counts_lst)) {
      # Do we have already other barcode metadata
      if ("barcode_metadata" %in% names(attributes(counts_lst[[i]]))) {
        metadata = attr(counts_lst[[i]], "barcode_metadata")
      } else {
        metadata = data.frame(orig_barcode=colnames(counts_lst[[i]]))
        rownames(metadata) = metadata$orig_barcode
      }
      
      x_id = colnames(metadata)[1]
      x_rownames = rownames(metadata)
      y_id = colnames(barcode_metadata)[1]
      metadata = dplyr::left_join(x=metadata,
                                   y=barcode_metadata,
                                   by=setNames(y_id, x_id))
      rownames(metadata) = x_rownames
      attr(counts_lst[[i]], "barcode_metadata") = metadata
    }
  }

  # Add feature metadata to counts objects
  if (!is.null(feature_metadata)) {
    # Merge list of feature metadata files
    if (inherits(feature_metadata, "list")) {
      # Collect all barcodes: Get first column of each table
      features = purrr::map(feature_metadata, 1) %>% unlist() %>% unique()
      # Now merge all tables
      merged_feature_metadata = data.frame(feature_id=features)
      for(i in seq_along(feature_metadata)) {
        x_id = colnames(merged_feature_metadata)[1]
        y_id = colnames(feature_metadata[[i]])[1]
        merged_barcode_metadata = dplyr::left_join(x=merged_feature_metadata,
                                                   y=feature_metadata[[i]],
                                                   by=setNames(y_id, x_id),
                                                   suffix=paste0(".", c(i-1, i)))
      }
      feature_metadata = merged_feature_metadata
    }
    
    for(i in seq_along(counts_lst)) {
      # Do we have already other feature metadata
      if ("feature_metadata" %in% names(attributes(counts_lst[[i]]))) {
        metadata = attr(counts_lst[[i]], "feature_metadata")
      } else {
        metadata = data.frame(feature_id=rownames(counts_lst[[i]]))
        rownames(metadata) = metadata$feature_id
      }
      
      x_id = colnames(metadata)[1]
      x_rownames = rownames(metadata)
      y_id = colnames(feature_metadata)[1]
      metadata = dplyr::left_join(x=metadata,
                                  y=feature_metadata,
                                  by=setNames(y_id, x_id))
      rownames(metadata) = x_rownames
      attr(counts_lst[[i]], "feature_metadata") = metadata
    }
  }
  
  # Make feature names Seurat-compatible (replace '_' with '-') and unique
  for(i in seq_along(counts_lst)) {
    # Get attributes
    barcode_metadata = attr(counts_lst[[i]], "barcode_metadata")
    feature_metadata = attr(counts_lst[[i]], "feature_metadata")
    assay = attr(counts_lst[[i]], "assay")
    technology = attr(counts_lst[[i]], "technology")
    
    
    feature_names = rownames(counts_lst[[i]])
    if (any(grepl(pattern="_", x=feature_names, fixed=TRUE))) {
      warning(FormatString("Feature names contain '_' for dataset {path}, assay {assay}. All occurences will be replaced with '-'."))
      feature_names = gsub(pattern="_", replacement="-", x=feature_names, fixed=TRUE)
    }
    if (any(duplicated(feature_names))) {
      warning(FormatString("Features contains duplicate values for dataset {path}, assay {assay}. Feature names will be made unique."))
      feature_names = make.unique(feature_names)
    }
    
    rownames(counts_lst[[i]]) = feature_names
    rownames(feature_metadata) = feature_names
    
    # Restore attributes (when modifying the object they get erased)
    attr(counts_lst[[i]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]], "assay") = assay
    attr(counts_lst[[i]], "technology") = technology
  }

  # Add barcode suffix
  if (!is.null(barcode_suffix)) {
    for(i in seq_along(counts_lst)) {
      # Get attributes
      barcode_metadata = attr(counts_lst[[i]], "barcode_metadata")
      feature_metadata = attr(counts_lst[[i]], "feature_metadata")
      assay = attr(counts_lst[[i]], "assay")
      technology = attr(counts_lst[[i]], "technology")
      
      # Counts
      colnames(counts_lst[[i]]) = paste0(colnames(counts_lst[[i]]), barcode_suffix)
      
      # Metadata
      rownames(barcode_metadata) = paste0(rownames(barcode_metadata), barcode_suffix)
      
      # Restore attributes (when modifying the object they get erased)
      attr(counts_lst[[i]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]], "assay") = assay
      attr(counts_lst[[i]], "technology") = technology
    }
  }
  
  return(counts_lst)
}

#' Write counts data to disk in BPCells matrix directory format.
#' 
#' @param counts A counts matrix. Format can be a standard matrix, a sparse matrix, AnnDataMatrixH5 (when reading anndata.h5ad) or MatrixSubset (when reading hdf5).
#' @param path Where to write counts on disk.
#' @param overwrite Overwrite existing output paths. Default is FALSE.
#' @return An IterableMatrix pointing to the BPCells matrix directory.
WriteCounts_MatrixDir = function(counts, path, overwrite=FALSE) {
  library(BPCells)
  
  # If path exists and overwrite is FALSE, just open the matrix directory
  if (dir.exists(path) & overwrite==FALSE) {
    counts = BPCells::open_matrix_dir(path)
    return(counts)
  }
  
  # Check that the counts object has the correct format
  if (is(counts, "IterableMatrix")) {
    # Produced by BPCells::open_matrix_10x_hdf5 or BPCells::open_matrix_anndata_hdf5
    # Nothing to do
  } else if (is.matrix(counts)) {
    # Standard matrix - convert to sparse matrix
    counts = as(counts, "dgCMatrix")
  } else if(is(counts, "sparseMatrix")) {
    # Sparse matrix - convert to dgCMatrix
    if (!is(counts, "dgCMatrix")) {
      counts = as(counts, "dgCMatrix")
    }
  }else {
    # No matrix - try to convert to sparse matrix
    counts = as(as.matrix(counts), "dgCMatrix")
  }
  
  # Sparse matrices in dgCMatrix format must be converted to BPCells internal format for better efficiency
  if (is(counts, "dgCMatrix")) {
    counts = as(counts, "IterableMatrix")
  }
  
  # Test if we have non-negative integers, then convert matrix from double to integer to save disk space (default is double)
  vals = as(counts[1:min(1000, nrow(counts)), 1:min(1000, ncol(counts))], "dgCMatrix")
  if (all(vals >= 0) & all(vals == round(vals))) {
    counts = BPCells::convert_matrix_type(counts, type="uint32_t")
  }
  
  # Write to directory
  counts = BPCells::write_matrix_dir(mat=counts, dir=path, overwrite=overwrite)
  
  # Return IterableMatrix pointing to this directory
  return(counts)
}

#' Write counts data to disk in matrix market format.
#' 
#' @param counts A counts matrix. Format can be a standard matrix, a sparse matrix, AnnDataMatrixH5 (when reading anndata.h5ad) or MatrixSubset (when reading hdf5).
#' @param path Where to write counts on disk.
#' @param overwrite Overwrite existing output paths. Default is FALSE.
#' @param barcode_data If not NULL, do not write the colnames to barcodes.tsv.gz. Instead write barcode_data to barcodes.tsv.gz. Make sure it matches the counts matrix.
#' @param feature_data If not NULL, do not write the rownames to features.tsv.gz. Instead write feature_data to features.tsv.gz. Make sure it matches the counts matrix. 
#' @return A matrix market directory with matrix.mtx.gz, barcodes.tsv.gz and features.tsv.gz
WriteCounts_MatrixMarket = function(counts, path, overwrite=FALSE, barcode_data=NULL, feature_data=NULL) {
  if (!dir.exists(path) | overwrite==TRUE) {
    if (!is(counts, "dgCMatrix")) {
      counts = as(counts, "dgCMatrix")
    }
    
    # Create directory
    d = file.path(path)
    dir.create(d, showWarnings=FALSE)
    
    # Write matrix.mtx.gz
    mh = file.path(d, "matrix.mtx")
    Matrix::writeMM(counts, file=mh)
    R.utils::gzip(mh, overwrite=TRUE)
    
    # Write barcodes and barcode metadata
    if (is.null(barcode_data)) {
      barcode_data = data.frame(barcode=colnames(counts))
    } else {
      assertthat::assert_that(ncol(counts) == nrow(barcode_data),
                              msg=FormatString("The number of rows in barcode_data differs from the number of columns in the counts matrix."))
    }
    
    bh = gzfile(file.path(d, "barcodes.tsv.gz"), open="wb")
    if (ncol(barcode_data) > 1) {
      comment_line = paste(colnames(barcode_data), collapse = " ")
      comment_line = paste("#", comment_line)
      writeLines(comment_line, con=bh)
    }
    write.table(barcode_data, file=bh, sep="\t", col.names=FALSE, row.names=FALSE, quote=FALSE)
    close(bh)
    
    if (is.null(feature_data)) {
      feature_data = data.frame(id=rownames(counts), name=rownames(counts))
    } else{
      assertthat::assert_that(nrow(counts) == nrow(feature_data),
                              msg=FormatString("The number of rows in feature_data differs from the number of rows in the counts matrix."))
    }
    
    fh = gzfile(file.path(d, "features.tsv.gz"), open="wb")
    if (ncol(feature_data) > 1) {
      comment_line = paste(colnames(feature_data), collapse = " ")
      comment_line = paste("#", comment_line)
      writeLines(comment_line, con=fh)
    }
    write.table(feature_data, file=fh, sep="\t", col.names=FALSE, row.names=FALSE, quote=FALSE)
    close(fh)
  }
}

#' Reads image data produced by 10x Visium or 10x VisiumHD.
#' 
#' @param path Path to the 'spatial' directory produced by 10x Visium or 10x VisiumHD.
#' @param barcodes If not NULL, named vector with barcodes to keep, order and rename. Names are original barcodes and values are barcodes after renaming. Barcodes will be re-ordered.
#' @param coordinate_type For Visium HD segmented output, load cell "centroids", cell "segmentation" or both (default).
#' @return A Seurat VisiumV1/V2 object.
ReadImage_10xVisium = function(image_dir, barcodes=NULL, coordinate_type=c("centroids", "segmentation")) {
  # Checks
  assertthat::is.readable(image_dir)
  for (f in c("tissue_hires_image.png", "tissue_lowres_image.png", "scalefactors_json.json")) {
    assertthat::assert_that(file.exists(file.path(image_dir, f)),
                            msg=FormatString("10x Visium image directory {image_dir} misses the file {f}."))
  }
  
  # Decide whether this is a bin/spot-based or a segmented dataset
  cell_segmentations_file = file.path(dirname(image_dir), "cell_segmentations.geojson")
  nucleus_segmentations_file = file.path(dirname(image_dir), "nucleus_segmentations.geojson")
  
  if (file.exists(file.path(image_dir, "tissue_positions.parquet"))) {
      tissue_positions_file = file.path(image_dir, "tissue_positions.parquet")
  } else if (file.exists(file.path(image_dir, "tissue_positions_list.csv"))) {
      tissue_positions_file = file.path(image_dir, "tissue_positions_list.csv")
  } else {
      tissue_positions_file = file.path(image_dir, "tissue_positions.csv")
  }
  
  is_segmented = !file.exists(tissue_positions_file)
  
  if (is_segmented) {
      assertthat::assert_that(file.exists(cell_segmentations_file) & file.exists(nucleus_segmentations_file),
                              msg=FormatString("10x Visium image directory {image_dir} seems to contain segmented data but is missing either the cell segmentations file {cell_segmentations_file} or the nucleus segmentations file {nucleus_segmentations_file}."))
  } else {
      assertthat::assert_that(file.exists(tissue_positions_file),
                              msg=FormatString("10x Visium image directory {image_dir} seems to contain bin/spot-based data but is missing the tissue positions file {tissue_positions_file}."))
  }
  
  # Read image
  if (!is_segmented) {
      # Bin/spot-based dataset
      image = Seurat::Read10X_Image(image_dir, 
                                    filter.matrix=TRUE,
                                    image.name="tissue_lowres_image.png")
  } else {
      # Segmentation-based dataset
      
      # Read the Visium (V2) object with cell segmentations loaded
      image = Read10X_Segmentations(
          image.dir=image_dir,
          data.dir=dirname(dirname(image_dir)),
          image.name="tissue_lowres_image.png",
          segmentation.type="cell",
          cell.names=names(barcodes)
      )
      
      # Holds the segmentation object
      segmentations_obj = image@boundaries$segmentation
      
      # Get sf data (these are the polygons)
      segmentations_sf_data = segmentations_obj@sf.data
      
      # Set the attribute-geometry relationship to constant
      # See https://r-spatial.github.io/sf/reference/sf.html#details
      sf::st_agr(segmentations_sf_data) = "constant"
      
      # Create a dataframe from sf data to hold centroids
      centroid_coords = sf::st_coordinates(sf::st_centroid(segmentations_sf_data))
      centroids_df = data.frame(
          x = centroid_coords[, "X"],
          y = centroid_coords[, "Y"],
          row.names = segmentations_sf_data$barcodes
      )
      
      # Create centroids object
      centroids_obj = SeuratObject::CreateCentroids(centroids_df,
                                   nsides = Inf,
                                   radius = NULL,
                                   theta = 0)
      
      # Add centroids to the Visium object
      image@boundaries$centroids = centroids_obj
      
      # Now decide whether to keep centroids, segmentations or both and set default for plotting
      if (!"centroids" %in% coordinate_type) {
          image@boundaries$centroids = NULL
      }
      if (!"segmentation" %in% coordinate_type) {
          image@boundaries$segmentation = NULL
      }
      SeuratObject::DefaultBoundary(image) = coordinate_type[1]
  }
  
  if (!is.null(barcodes)) {
    image_bcs = SeuratObject::Cells(image)
    
    # Keep only barcodes that are requested
    i = which(image_bcs %in% names(barcodes))
    image_bcs = image_bcs[i]
    
    # Re-order and subset
    i = match(names(barcodes), image_bcs)
    i = i[!is.na(i)]
    image_bcs = image_bcs[i]
    image = image[image_bcs]
    
    # Re-name
    image_bcs = SeuratObject::Cells(image)
    new_image_bcs = unname(barcodes[SeuratObject::Cells(image)])
    image = SeuratObject::RenameCells(image, new.names=new_image_bcs)
  }
  
  return(image)
}

#' Improved version of SeuratObject::CreateSegmentation.
#' 
#' Further improvements might be possible by parallelisation of purrr.
#' #' 
#' @param coords A coordinate table with columns 'cell', 'x' and 'y'.
#' @return A Seurat Segmentation object.
CreateSegmentationImproved = function(coords) {
  library(sp)
  library(SeuratObject)
  
  assertthat::assert_that(all(colnames(coords) == c("cell", "x", "y")),
                          msg="Function 'CreateSegmentationImproved' requires a table with columns 'cell', 'x' and 'y'.")
  
  coords_cell_names = coords[[1]]
  coords_cell_names = factor(coords_cell_names, levels=unique(coords_cell_names))
  
  coords = as.matrix(coords[, 2:3])
  coords = split(x=coords, f=coords_cell_names)
  coords = purrr::map(coords, .f=matrix, ncol=2, dimnames=list(NULL, c("x", "y")))
  
  polygons_names = names(coords)
  polygons = purrr::map(seq_along(coords), function(i) {
    return(Polygons(
      srl=list(Polygon(coords=coords[[i]])),
      ID=polygons_names[i])
    )
  })
  polygons = SpatialPolygons(Srl=polygons)
  polygons = as(polygons, "Segmentation")
  CheckGC()
  return(polygons)
}

#' Reads "image data" produced by 10x Xenium.
#' 
#' Note that this is not an actual image but rather a set of pixel coordinates (field of vision = FOV)
#' 
#' @param path Path to a 10x Xenium data directory.
#' @param barcodes If not NULL, named vector with barcodes to keep, order and rename. Names are original barcodes and values are barcodes after renaming. Barcodes will be re-ordered.
#' @param coordinate_type Load cell "centroids", cell "segmentations" or both (default).
#' @return A Seurat FOV object.
ReadImage_10xXenium = function(image_dir, barcodes=NULL, coordinate_type=c("centroids", "segmentation")) {
  # Checks
  assertthat::is.readable(image_dir)
  assertthat::assert_that(file.exists(file.path(image_dir, "cells.parquet")),
                          msg=FormatString("10x Xenium image directory {image_dir} misses the file 'cells.parquet'."))
  assertthat::assert_that(file.exists(file.path(image_dir, "cell_boundaries.parquet")),
                          msg=FormatString("10x Xenium image directory {image_dir} misses the file 'cell_boundaries.parquet'."))
  assertthat::assert_that(file.exists(file.path(image_dir, "transcripts.parquet")),
                          msg=FormatString("10x Xenium image directory {image_dir} misses the file 'transcripts.parquet'."))
  
  mols.qv.threshold = 20
  options(stringsAsFactors=FALSE)
  coords = list()
  
  # Read cell centroids and cell area
  cell_centroids = arrow::read_parquet(file.path(image_dir, "cells.parquet"),
                                       col_select=c("cell_id", "x_centroid", "y_centroid", "cell_area", "nucleus_area"),
                                       as_data_frame=TRUE)
  
  cell_centroids = arrow::open_dataset(file.path(image_dir, "cells.parquet"),
                                        schema=arrow::schema(cell_id=arrow::string(), 
                                                             x_centroid=arrow::float(), 
                                                             y_centroid=arrow::float(),
                                                             cell_area=arrow::float(),
                                                             nucleus_area=arrow::float()))
  cell_centroids = as.data.frame(cell_centroids)
  names(cell_centroids) = c("cell", "x", "y", "cell_area", "nucleus_area")
  
  if (!is.null(barcodes)) {
    # Keep only barcodes that are requested
    i = which(cell_centroids$cell %in% names(barcodes))
    cell_centroids = cell_centroids[i, ]
    
    # Reorder barcodes
    i = match(cell_centroids$cell, names(barcodes))
    cell_centroids = cell_centroids[i, ]
    
    assertthat::assert_that(all(cell_centroids$cell == names(barcodes)),
                            msg="Barcodes for cell centroids do not match the requested barcodes.")
    
    # Rename
    cell_centroids$cell = barcodes
  }
  
  if ("centroids" %in% coordinate_type) {
    coords = c(coords, centroids = SeuratObject::CreateCentroids(cell_centroids[, c("cell", "x", "y")]))
  }
  
  # Read segmentations (area of cells)
  if ("segmentation" %in% coordinate_type) {
    cell_boundaries = arrow::open_dataset(file.path(image_dir, "cell_boundaries.parquet"),
                                          schema=arrow::schema(cell_id=arrow::string(), 
                                                               vertex_x=arrow::float(), 
                                                               vertex_y=arrow::float()))
    cell_boundaries = as.data.frame(cell_boundaries)
    names(cell_boundaries) = c("cell", "x", "y")
    
    if (!is.null(barcodes)) {
      # Keep only barcodes that are requested
      i = which(cell_boundaries$cell %in% names(barcodes))
      cell_boundaries = cell_boundaries[i, ]
      
      # Reorder barcodes
      i = match(cell_boundaries$cell, names(barcodes))
      cell_boundaries = cell_boundaries[i, ]
      
      # Rename
      new = barcodes[cell_boundaries$cell]
      assertthat::assert_that(all(!is.na(new)),
                              msg="Barcodes for cell centroids do not match the requested barcodes.")
      
      cell_boundaries$cell = new
    }
    coords = c(coords, segmentation = CreateSegmentationImproved(cell_boundaries))
  }
  
  # Load microns (molecule coordinates)
  transcripts = arrow::open_dataset(file.path(image_dir, "transcripts.parquet"),
                                    schema=arrow::schema(feature_name=arrow::string(), 
                                                         x_location=arrow::float(), 
                                                         y_location=arrow::float(), 
                                                         qv=arrow::float()))
  transcripts = transcripts %>% dplyr::filter(qv >= mols.qv.threshold) %>% dplyr::select(-qv)
  transcripts = as.data.frame(transcripts)
  colnames(transcripts) = c("gene", "x", "y")
  molecules = SeuratObject::CreateMolecules(transcripts, key='mols_')
  
  # Create FOV (coordinates plus transcript info)
  image = SeuratObject::CreateFOV(coords=coords,
                                  molecules=molecules,
                                  assay = 'Spatial',
                                  key = 'fov_')
  
  # Add information about cell_area and nucleus_area as barcode_metadata
  barcode_metadata = as.data.frame(cell_centroids[, c("cell", "cell_area", "nucleus_area")]) %>%
    tidyr::replace_na(list(cell_area=0, nucleus_area=0))
  rownames(barcode_metadata) = as.character(barcode_metadata$cell)
  barcode_metadata$cell = NULL
  attr(image, "barcode_metadata") = barcode_metadata
  
  # Set default boundary
  SeuratObject::DefaultBoundary(image) = coordinate_type[1]
  
  return(image)
}

#' Reads image data produced by 10x Visium, 10x VisiumHD or 10x Xenium.
#' 
#' @param image_dir Path to a spatial directory.
#' @param technology Technology. Can be: '10x_visium', '10x_visiumhd', '10x_xenium'.
#' @param assay Default assay for this image.
#' @param barcodes Named vector with barcodes to keep, order and rename. Names are original barcodes and values are barcodes after renaming. Barcodes will be re-ordered.
#' @param coordinate_type For 10x Xenium only: Load cell "centroids", cell "segmentations" or both (default).
#' @return A Seurat VisiumV1 object.
ReadImage = function(image_dir, technology, assay, barcodes, coordinate_type=c("centroids", "segmentations")) {
  library(magrittr)
  
  # Checks
  valid_technologies = c("10x_visium", "10x_visiumhd", "10x_xenium")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatString("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read image
  if(technology %in% c("10x_visium", "10x_visiumhd")) {
    # Visium
    image = ReadImage_10xVisium(image_dir=image_dir, barcodes=barcodes, coordinate_type=coordinate_type)
  } else if(technology == "10x_xenium") {
    # Xenium
    image = ReadImage_10xXenium(image_dir=image_dir, barcodes=barcodes, coordinate_type=coordinate_type)
  }
  
  # Set default assay for image
  Seurat::DefaultAssay(image) = assay
  
  return(image)
}

#' Reads summary metrics files produced for SmartSeq data. 
#' 
#' Note: Since there is no generally accepted pipeline for SmartSeq data, a metrics file can contain all kinds of information and 
#' can have any format. Therefore, this function just reads and returns a character-separated table. First column must be the cell name 
#' and all other columns can contain metrics.
#' 
#' @param metrics_file Path to a character-separated metrics file.
#' @return A summary metrics table.
ReadMetrics_Smartseq = function(metrics_file) {
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)
  
  return(metrics_table)
}

#' Reads summary metrics files produced for 10x, 10x Visium, 10x Xenium data.
#' 
#' @param metrics_file Path to a "metrics_summary.csv" file produced by the 10x pipelines cellranger, spaceranger and xenium ranger.
#' @return One or more tables with summary metrics per library type (only cellranger multi) or a table with summary metrics for the entire 10x experiment (all other 10x pipelines).
ReadMetrics_10x = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/konstantint/bfx2278/cellranger_multi/Pwl3_CMO311_A1_CMO312_alt/outs/per_sample_outs/A1/metrics_summary.csv"
  #metrics_file = "/projects/seq-work/analysis/yuliiah/bfx2322/cellranger_arc/m194T/outs/summary.csv"
  library(magrittr)
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)
  metrics_table_colnms = colnames(metrics_table)
  metrics_table = as.data.frame(metrics_table)
  
  if (metrics_table_colnms[1] == "Category") {
    # Produced by cellranger multi
    # https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/metrics-summary-csv
    
    # Subset per-sample metrics
    # Also only keep overall metrics but not the ones calculated for various groups
    metrics_table = metrics_table %>% 
      dplyr::filter(Category=="Cells", is.na(`Grouped By`)) %>%
      dplyr::select(library_type=`Library Type`, Metric=`Metric Name`, Value=`Metric Value`)
    
    # Split by library type and convert into wide table
    metrics_table = split(metrics_table, metrics_table$library_type)
    metrics_table = purrr::map(metrics_table, function(tbl) {
      tbl = tbl %>% dplyr::select(-library_type) %>% tidyr::pivot_wider(names_from="Metric", values_from="Value") %>% as.data.frame()
      return(tbl)
    })
  } else {
    # Produced by other cellranger pipelines (standard, multiome, atac, visium, xenium)
    metrics_table = list(Overall=metrics_table)
  }
  
  return(metrics_table)
}

#' Reads summary metrics files produced for Parse Biosciences data. 
#' 
#' @param metrics_file Path to an "analysis_summary.csv" file produced by the splitpipe pipeline.
#' @return A summary metrics table.
ReadMetrics_ParseBio = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/L132590_Sl_1_62500cells/BC001/report/analysis_summary.csv"
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=c("Metric", "Value"), col_select=1:2, show_col_types=FALSE, skip=1)
  metrics_table = metrics_table %>% 
    tidyr::pivot_wider(names_from=1, values_from=2) %>%
    as.data.frame()
  metrics_table = list(metrics_table)
  
  return(metrics_table)
}

#' Reads summary metrics files produced for Scale Bio data. 
#'
#' @param metrics_file Path to a "reportStatistics.csv" file produced by the ScaleRna pipeline.
#' @return A summary metrics table.
ReadMetrics_ScaleBio = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/SCdev/bfx2279/scalerna/output/reports/csv/BC238.reportStatistics.csv"
  library(magrittr)
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)

  # Only keep sections "Reads" and "Cells"
  metrics_table = metrics_table %>% dplyr::filter(Sample %in% c("Reads", "Cells"))
  metrics_table = metrics_table %>% 
    dplyr::select(-Sample) %>%
    tidyr::pivot_wider(names_from=1, values_from=2) %>%
    as.data.frame()
  
  metrics_table = list(Overall=metrics_table)
  return(metrics_table)
}

#' Reads summary metrics files produced by Smartseq, 10x, 10x Visium, 10x Xenium, Parse Biosciences, Scale Bio.
#' 
#' @param metrics_file Path to a metrics file.
#' @param technology Technology. Can be: 'smartseq2', 'smartseq3', '10x', '10x_visium', '10x_xenium', 'parse' or 'scale'.
#' @return One or more tables with summary metrics.
ReadMetrics = function(metrics_file, technology) {
  #metrics_file = datasets$metrics_file[1]
  #technology = "10x"
  
  # Checks
  valid_technologies = c("smartseq2", "smartseq3", "10x", "10x_visium", "10x_xenium", "parse", "scale")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatString("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read metrics file
  if (technology %in% c("smartseq2", "smartseq3")) {
    metrics_table = ReadMetrics_SmartSeq(metrics_file=metrics_file)
  } else if(technology %in% c("10x", "10x_visium", "10x_xenium")) {
    metrics_table = ReadMetrics_10x(metrics_file=metrics_file)
  } else if(technology == "parse") {
    metrics_table = ReadMetrics_ParseBio(metrics_file=metrics_file)
  } else if(technology == "scale") {
    metrics_table = ReadMetrics_ScaleBio(metrics_file=metrics_file)
  }
  
  return(metrics_table)
}

#' Parses plate information from the cell names. Mainly used for Smartseq2 datasets where this information is often included in the cell name.
#' 
#' @param cell_names A vector with cell names.
#' @param pattern A regular expression pattern with capture groups for plate number, row or column. Default is '_(\\d+)_([A-Z])(\\d+)$'. If the pattern does not match, all information will be set to NA.
#' @return A data frame with plate information.
ParsePlateInformation = function(cell_names, pattern='_(\\d+)_([A-Z])(\\d+)$') {
  library(magrittr)
  
  # Split cell name into plate information and rest
  rest = gsub(pattern=pattern, replacement="", x=cell_names)
  plate_information = as.data.frame(stringr::str_match(string=cell_names, pattern=pattern), stringsAsFactors=FALSE)
  plate_information[, 1] = NULL
  
  if (ncol(plate_information) == 2) {
    colnames(plate_information) = c("PlateRow", "PlateCol")
    plate_information$PlateNumber = NA 
  } else if (ncol(plate_information) == 3) {
    colnames(plate_information) = c("PlateNumber", "PlateRow", "PlateCol")
  }
  plate_information = plate_information[, c("PlateNumber", "PlateRow", "PlateCol")]
  plate_information$Rest = rest
  
  plate_information$PlateNumber = as.integer(plate_information$PlateNumber)
  plate_information$PlateRow = as.character(plate_information$PlateRow)
  plate_information$PlateCol = as.integer(plate_information$PlateCol)
  
  # Decide on plate layout
  if ("Q" %in% plate_information$PlateRow | max(c(-Inf,plate_information$PlateCol), na.rm=T) > 24) {
    # super plate?
    plate_information$PlateRow = factor(plate_information$PlateRow, 
                                        levels=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z"), 
                                        ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, levels=1:max(c(-Inf,plate_information$PlateCol), na.rm=T), ordered=TRUE)
  } else if ("I" %in% plate_information$PlateRow | max(c(-Inf,plate_information$PlateCol), na.rm=T) > 12) {
    # 384 plate
    plate_information$PlateRow = factor(plate_information$PlateRow, 
                                        levels=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P"), 
                                        ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, levels=1:24, ordered=TRUE)
  } else {
    plate_information$PlateRow = factor(plate_information$PlateRow, ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, ordered=TRUE)
  }
  
  return(plate_information)
}

#' Saves Seurat object and - if available and requested - associated on-disk layers. Improved version of Seurat's SaveSeuratRds.
#' 
#' The function was rewritten mainly for the usage of on-disk matrices for big datasets:
#' - When layers of multiple samples are joined (e.g. counts.sample1 and counts.sample2 into counts), the 
#' original function cannot save them. This was fixed.
#' - The original function will write the on-disk matrices every time the Seurat object is saved. This is not necessary 
#' since the matrices do not change after a certain point. This function includes an option to use the existing directories.
#' - This function is much more documented.
#' 
#' @param sc A Seurat sc object.
#' @param outdir Output directory for saved Seurat object (sc.rds) and associated on-disk layers. If it does not exist, it will be created.
#' @param write_disk_data If TRUE also copy existing on-disk layers into this directory.
#' @param relative_paths Make paths to on-disk layers relative.
#' @param commpress Whether to compress the Seurat sc.rds file
SaveSeuratRdsWrapper = function(sc, outdir, write_disk_data=TRUE, relative_paths=FALSE, compress=FALSE) {
  # If output directory does not exist, create it
  if (!dir.exists(outdir)) dir.create(outdir, recursive=TRUE)
  
  # Make sure output directory is empty
  files = list.files(outdir)
  assertthat::assert_that(length(files) == 0,
                          msg=FormatString("Target directory for Seurat object and associated matrix directories at {outdir} must be empty but is not. Please delete all files and directories in this directory."))
  
  # Name of seurat object
  file = file.path(outdir, "sc.rds")
  file = normalizePath(path=file, winslash="/", mustWork=FALSE)
  
  # Check that the fs package is installed for file moving (in case there are on-disk matrices)
  assertthat::assert_that(require("fs"), msg="The package 'fs' is required to move on-disk matrices. Please install it.")
  
  # Assays
  assays = SeuratObject::.FilterObjects(sc, classes.keep="StdAssay")
  
  # Progressor
  p = progressr::progressor(along=assays, auto_finish=TRUE)
  on.exit(expr=p(type="finish"), add=TRUE)
  p(message=FormatString("Looking for on-disk matrices in {length(assays)} assays", quote=FALSE), class="sticky", amount=0)
  
  # Set up table with on-disk matrix information
  cache = vector(mode="list", length=length(assays))
  names(cache) = assays
  destdir = dirname(file)
  
  # This table contains information about already existing on-disk directories
  save_seurat_rds = SeuratObject::Tool(sc, "SaveSeuratRds")
  
  # Loop over assays
  for (assay in assays) {
    p(message = FormatString("Searching through assay {assay}"), class="sticky", amount=0)
    
    # Loop over layers and collect information required for on-disk matrices
    layer_disk_info = purrr::map_dfr(SeuratObject::Layers(sc[[assay]]), function(layer) {
      # Get layer data
      layer_data = SeuratObject::LayerData(sc[[assay]], layer=layer)
      
      # Get path(s) to on-disk matrix
      layer_disk_paths = SeuratObject::.FilePath(layer_data)
      
      # Empty means no on-disk data and can be skipped
      layer_disk_paths = layer_disk_paths[nzchar(layer_disk_paths) > 0]
      if (length(layer_disk_paths)==0) layer_disk_paths = NULL
      if (is.null(layer_disk_paths)) return(NULL)
      
      # If a layer consists of multiple on-disk directories, split them
      layer_disk_paths = unlist(strsplit(layer_disk_paths, ","))
      layer_disk_paths = trimws(layer_disk_paths)
      
      # We want to keep the paths relative to the current directory
      layer_disk_paths = fs::path_rel(layer_disk_paths)
      
      # If write_disk_data is FALSE and the on-disk directories already exist 
      # (from previous run, stored in tool SaveSeuratRds) change paths so that 
      # these are used
      if (!write_disk_data & !is.null(save_seurat_rds)) {
        known_disk_paths = unlist(strsplit(save_seurat_rds$path, ","))
        known_disk_paths = trimws(known_disk_paths)
        layer_disk_paths = purrr::map_chr(layer_disk_paths, function(p) {
          i = which(basename(known_disk_paths) %in% basename(p))
          if (length(i) == 1) {
            return(known_disk_paths[i])
          } else if (length(i) == 0) {
            return(p)
          } else {
            stop("Multiple on-disk directories found for the same layer. This should not happen.")
          }
        })
      }
      
      # Return table with information
      layer_disk_fxn = SeuratObject::.DiskLoad(layer_data)
      if (is.null(layer_disk_fxn)) layer_disk_fxn = identity
      return(data.frame(
        # Layer name
        layer=layer,
        # Path(s) to on-disk matrix
        path=paste(layer_disk_paths, collapse=","),
        # Class of on-disk matrix
        class=paste(class(layer_data), collapse=","),
        # Package of on-disk matrix
        pkg=SeuratObject::.ClassPkg(layer_data),
        # Function to load on-disk matrix
        fxn=layer_disk_fxn
      ))
    })
    
    # No on-disk layers found, skip (everything stored in Seurat object)
    if (is.null(layer_disk_info) || !nrow(layer_disk_info)) {
      p(message="No on-disk layers found", class="sticky", amount=0)
      next
    }
    
    # Write on-disk matrices (if needed)
    if (write_disk_data) {
      for (i in seq_len(length.out=nrow(layer_disk_info))) {
        layer = layer_disk_info$layer[i]
        
        # A on-disk matrix can consist of multiple data directories
        layer_disk_paths = unlist(strsplit(layer_disk_info$path[i], ","))
        layer_disk_paths = trimws(layer_disk_paths)
        
        # Iterate over paths of data directories, copy them (if needed) and return new paths
        new_layer_disk_paths = purrr::map(layer_disk_paths, function(p) {
          np = file.path(destdir, basename(p))
          
          # If p already exists and we do not want write the data, do not write, 
          # just return
          if (file.exists(p) & !write_disk_data) return(p)
          
          # If np already exists, do not write, just return
          if (file.exists(np)) return(np)
          
          # Else write
          p(message=FormatString("Writing on-disk directory {basename(p)}, layer {layer}, to {destdir}"), class="sticky", amount = 0)
          np = as.character(.FileMove(path=p, new_path=destdir))
          return(np)
        })

        new_layer_disk_paths = paste(unlist(new_layer_disk_paths), collapse=",")
        layer_disk_info[i, "path"] = new_layer_disk_paths
      }
    }
    
    # If requested, store paths to on-disk matrices relative to Seurat object
    if (relative_paths) {
      p(message=FormatString("Adjusting paths to be relative to {dirname(file)}"), class="sticky", amount=0)
      
      for (i in seq_len(length.out=nrow(layer_disk_info))) {
        layer_disk_paths = unlist(strsplit(layer_disk_info$path[i], ","))
        new_layer_disk_paths = purrr::map(layer_disk_paths, function(p) {
          return(fs::path_rel(path=p, start=dirname(path=file)))
        })
      }
    }
    
    # Add to cache information
    layer_disk_info$assay = assay
    cache[[assay]] = layer_disk_info
    
    if (nrow(layer_disk_info) == length(SeuratObject::Layers(sc[[assay]]))) {
      p(message = FormatString("Clearing layers from {assay}"), class="sticky", amount=0)
      adata = SeuratObject::S4ToList(sc[[assay]])
      adata$layers = list()
      adata$default = 0L
      adata$cells = SeuratObject::LogMap(colnames(sc[[assay]]))
      adata$features = SeuratObject::LogMap(rownames(sc[[assay]]))
      sc[[assay]] = SeuratObject::ListToS4(adata)
    } else {
      p(message = FormatString("Clearing layers from {assay}"), class="sticky", amount=0)
      for (layer in layer_disk_info$layer) {
        SeuratObject::LayerData(sc[[assay]], layer=layer) = NULL
      }
    }
    p()
  }
  
  # Update table with on-disk matrix information in Seurat object
  cache = do.call("rbind", cache)
  if (!is.null(cache) && nrow(cache) > 0) {
    p(message="Saving on-disk cache to object", class="sticky", amount=0)
    row.names(cache) = NULL
    #SeuratObject::Tool(sc) = cache
    sc@tools$SaveSeuratRds = cache
  }
  
  saveRDS(sc, file=file, compress=compress)
}

#' Copies on-disk layers of a Seurat object to a new directory.
#' 
#' Note: This will copy the data and adjust the paths for the IterableMatrix objects in an existing Seurat object. To make changes permanent, you will also need to update the Tool entry SaveSeuratRds.
#' 
#' @param sc A Seurat sc object.
#' @param dir New directory for on-disk layers.
#' @param assays For which assays should on-disk layers be copied. If NULL, copy all.
#' @param layer For which layers should on-disk layers be copied. If NULL, copy all. Can also be a pattern.
#' @return Seurat object with updated on-disk layer paths.
UpdateMatrixDirs = function (sc, dir, assays=NULL, layer=NULL, update_tool_saveseuratrds=FALSE) {
  # New directory for on-disk matrices
  dir = normalizePath(path=dir, winslash="/", mustWork=FALSE)
  if (is.null(assays)) assays = SeuratObject::Assays(sc)
  
  # Stores information about on-disk matrices
  cache = SeuratObject::Tool(sc, slot="SaveSeuratRds")
  
  # Iterate over assays
  progr = progressr::progressor(along=assays)
  progr(message=paste("Copying on-disk matrices to directory", dir), class="sticky", amount=0)
  
  for (a in assays) {
    # Skip if assay has no information about on-disk matrices
    if (!a %in% cache$assay) next
    
    progr(message = paste("Searching through assay", a), class="sticky", amount=0)
    
    if (!is.null(layer)) {
      layers = SeuratObject::Layers(sc, assay=a, layers=layer)
    } else {
      layers = SeuratObject::Layers(sc, assay=a)
    }
    
    # Iterate over layers
    for(i in seq_along(layers)) {
      # Get IterableMatrix for layer
      data = SeuratObject::LayerData(sc, assay=a, layer=layers[i])
      
      # Get old path
      path = SeuratObject::.FilePath(x=data)
      path = Filter(f=nzchar, x=path)
      if (is.null(path)) next
      
      # Move on-disk matrix directory to new path
      progr(message = paste("Moving layer", layers[i], "to", dir), class="sticky", amount=0)
      
      path = unlist(strsplit(path, ","))
      new_path = lapply(path, function(p) {
        np = file.path(dir, basename(p))
        assertthat::assert_that(!file.exists(np) & !dir.exists(np),
                                msg=FormatString("Cannot copy matrix directory to already existing path {np}. Please delete this path first."))
        return(as.character(.FileMove(path=p, new_path=dir)))
      })
      new_path = as.character(paste(unlist(new_path), collapse=","))
      
      # Reload matrix directory with new path into Seurat object
      fnx = SeuratObject::.DiskLoad(data)
      fnx = eval(expr = str2lang(fnx))
      SeuratObject::LayerData(sc, assay=a, layer=layers[i]) = fnx(new_path)
      
      # Update path for on-disk matrices
      if (!is.null(cache)) {
        idx = which(cache$assay == a & cache$layer == layers[i])
        cache$path[idx] = new_path
      }
    }
  }
  progr(type='finish')
  
  # Update information about on-disk matrices in Seurat object if requested
  if (update_tool_saveseuratrds) {
    if (!is.null(cache) && nrow(cache) > 0) {
      row.names(cache) = NULL
      sc@tools$SaveSeuratRds = cache
    }
  }
  
  return(sc)
}


#' Exports a Seurat object to a Loupe file.
#'
#' @param sc A Seurat object.
#' @param assay Assay to include in the Loupe file. If NULL, the default assay is included.
#' @param categories Cell metadata columns to include in the Loupe file. If NULL, all non-numeric are included. Numeric columns are always discarded.
#' @param embeddings Embeddings to include in the Loupe file. If NULL, all embeddings are included.
#' @param barcodes Barcodes to include in the Loupe file. If NULL, all barcodes of the selected assay are included.
#' @param output_dir Directory where the Loupe file will be saved.
#' @param output_name Name of the Loupe file (cloupe.cloupe).
ExportLoupe = function(sc, assay=NULL, categories=NULL, embeddings=NULL, barcodes=NULL, output_dir=".", output_name="cloupe.cloupe") {
  # Download executable for loupeR
  loupeR::setup()
  
  # It it does not work, it has to be done manually
  louper_status = loupeR:::needs_setup()
  if (!louper_status$success) stop(louper_status$msg)
  
  # Check requested assays
  if (is.null(assay)) assay = SeuratObject::DefaultAssay(sc)
  assertthat::assert_that(all(assay %in% SeuratObject::Assays(sc)),
                          msg=FormatString("Requested assay {assay} part of the Seurat object."))
  
  # Barcoded and barcode metadata
  if (is.null(barcodes)) barcodes = SeuratObject::Cells(sc[[assay]])
  barcode_metadata = sc[[]][barcodes, ]
  
  # Check and discard numeric columns (Loupe cannot handle them)
  if (is.null(categories)) categories = colnames(barcode_metadata)
  assertthat::assert_that(all(categories %in% colnames(barcode_metadata)),
                          msg="Not all requested categories are part of the cell metadata.")
  categories = purrr::discard(categories, function(i) return(is.numeric(barcode_metadata[, i])))
  
  # Get counts of assay and convert to numeric matrix
  counts = SeuratObject::GetAssayData(sc, assay=assay, layer="counts")
  counts = as(counts[, barcodes], "dgCMatrix")
  
  # Replace NA with "NA" in barcode metadata
  # Convert character columns to factors
  categorial_data = purrr::map(categories, function(x) {
    v = barcode_metadata[, x]
    if (!is.factor(v)) {
      v = factor(as.character(v))
    }
    if (any(is.na(v))) v = forcats::fct_na_value_to_level(v, level="NA")
    return(v)
  })
  names(categorial_data) = categories
  categorial_data[["active_cluster"]] = categorial_data[["seurat_clusters"]]
  
  # Get embeddings data
  if (is.null(embeddings)) embeddings = SeuratObject::Reductions(sc)
  embedding_names = embeddings
  embeddings = purrr::map(embedding_names, function(r) {
    return(SeuratObject::Embeddings(sc, r)[,1:2])
  })
  names(embeddings) = embedding_names
  
  # Seurat object version
  seurat_obj_version = NULL
  if (!is.null(sc@version)) seurat_obj_version = as.character(sc@version)
  
  # Create Loupe file
  success = loupeR::create_loupe(counts, 
                                 clusters=categorial_data,
                                 projections=embeddings,
                                 output_dir=output_dir,
                                 output_name=gsub("\\.cloupe", "", output_name),
                                 force=TRUE,
                                 seurat_obj_version=seurat_obj_version)
}

#' Exports the cell categorial metadata of a Seurat object to a Xenium Explorer analysis.zarr.zip file.
#' 
#' Note: By default, Xenium Explorer allows to import cell categorial metadata via csv files. However, only one category at a time and it 
#' cannot be saved. Therefore, we save the complete cell categorial metadata in the native Xenium Explorer format for analysis results.
#' 
#' @param sc A Seurat object.
#' @param assay Assay to include in the Loupe file. If NULL, the default assay is included.
#' @param categories Cell metadata columns to include in the Xenium Explorer file. If NULL, all non-numeric are included. Numeric columns are always discarded.
#' @param barcodes Barcodes to include in the Loupe file. If NULL, all barcodes of the selected assay are included.
#' @param output_dir Directory where the Xenium Explorer file will be saved.
#' @param output_name Name of the Xenium Explorer file (analysis.zarr.zip).
ExportXeniumExplorer = function(sc, assay=NULL, categories=NULL, barcodes=NULL, output_dir=".", output_name="analysis.zar.zip") {
  # For this function, we need a datasets table in the misc slot (to get all barcodes present in the dataset)
  assertthat::assert_that("datasets" %in% names(sc@misc),
                          msg="This function requires the Seurat object to have a 'datasets' table in the misc slot with columns 'experiment' for orig.ident and 'path' for the path to the dataset.")
  datasets = sc@misc$datasets
  
  # Check requested assays
  if (is.null(assay)) assay = SeuratObject::DefaultAssay(sc)
  assertthat::assert_that(all(assay %in% SeuratObject::Assays(sc)),
                          msg=FormatString("Requested assay {assay} part of the Seurat object."))
  
  # Barcoded and barcode metadata
  if (is.null(barcodes)) barcodes = SeuratObject::Cells(sc[[assay]])
  barcode_metadata = sc[[]][barcodes, ]
  
  # Moreover, there needs to be a column 'orig_barcode' in the cell metadata
  assertthat::assert_that("orig_barcode" %in% colnames(barcode_metadata),
                          msg="This function requires the Seurat object to have a column 'orig_barcode' in the cell metadata.")
  
  # Check and discard numeric columns (Xenium Explorer cannot handle them)
  if (is.null(categories)) categories = colnames(barcode_metadata)
  assertthat::assert_that(all(categories %in% colnames(barcode_metadata)),
                          msg="Not all requested categories are part of the cell metadata.")
  categories = purrr::discard(categories, function(i) return(is.numeric(barcode_metadata[, i])))
  
  # Per dataset
  for(smp in levels(barcode_metadata$orig.ident)) {
    dir.create(file.path(output_dir, smp), showWarnings=FALSE, recursive=TRUE)
    
    # Get path to dataset and get a list of all barcodes present in the dataset
    # This is needed because the Xenium Explorer requires all barcodes to be present in the metadata (even if they are not part of the analysis)
    dataset_path = datasets %>% 
      dplyr::filter(experiment == smp) %>% 
      dplyr::pull(path)
    
    # Get a list of all barcodes present in the dataset
    if (dir.exists(dataset_path)) {
      # 10x market exchange format
      barcodes_file = dplyr::case_when(
        file.exists(file.path(dataset_path, "barcodes.tsv.gz")) ~ "barcodes.tsv.gz",
        file.exists(file.path(dataset_path, "barcodes.tsv")) ~ "barcodes.tsv"
      )
      
      unfiltered_barcodes = readLines(file.path(dataset_path, barcodes_file))
    } else {
      # h5 file
      hdf5_fh = hdf5r::H5File$new(dataset_path, mode = "r+")
      unfiltered_barcodes = hdf5_fh[["/matrix/barcodes"]][]
      hdf5_fh$close()
    }
    
    # Get dataset barcodes and metadata
    bcs = barcode_metadata %>% 
      dplyr::filter(orig.ident == smp) %>% 
      rownames()
    categories = categories[categories != "orig_barcode"]
    categorial_data = barcode_metadata[bcs, c("orig_barcode", categories)]
    rownames(categorial_data) = NULL
    
    # Add filtered (removed) barcodes to metadata table
    if (length(unfiltered_barcodes) > length(barcodes)) {
      categorial_data = categorial_data %>% 
        dplyr::bind_rows(
          data.frame(orig_barcode=setdiff(unfiltered_barcodes, barcodes))
        )
    }
    i = match(unfiltered_barcodes, categorial_data$orig_barcode)
    categorial_data = categorial_data[i, ]
    
    # Convert character columns to factors
    categorial_data = purrr::map(categories, function(x) {
      v = categorial_data[, x]
      if (!is.factor(v)) {
        v = factor(as.character(v))
      }
      return(v)
    })
    names(categorial_data) = categories
    
    # Attributes for zarr store
    zarr_attr = list("major_version" = 1,
                     "minor_version" = 0,
                     "number_groupings" = length(categorial_data),
                     "grouping_names" = names(categorial_data),
                     "group_names" = unname(purrr::map(categorial_data, levels)))
    
    # Convert categorial data to zarr-compatible format (lots of indices packed)
    zarr_categorial_data = purrr::map(categorial_data, function(values) {

      # For each categories, get the cell indices (note: we switch now to 0-based indices)
      values_indices = purrr::map(levels(values), function(cat) return(which(values == cat) - 1))
      
      # For each category, get the cumulative length of cell indices
      values_cum_len = purrr::map(values_indices, length) %>% 
        unlist() %>% 
        cumsum() %>%
        as.integer()
      
      # indices: array of the cell indices assigned to one of the categories
      indices = values_indices %>% unlist()
      
      # indptr: indicates the cell index value (row) where a new category begins
      if (length(values_cum_len) == 1) {
        indptr = c(0)
      } else {
        indptr = c(0, values_cum_len[1:length(values_cum_len)-1])
      }
      
      if (length(indices) == 0){
        indptr = as.integer(c())
      }
      
      return(list("indices" = as.integer(indices), "indptr" = as.integer(indptr)))
    })
    names(zarr_categorial_data) = names(categorial_data)
    
    # Now switch to python via reticulate
    # zarr_module is used to access the python module zarr, numpy_module ...
    zarr_module = import("zarr")
    numpy_module = import("numpy")
    
    # Create a zarr store file
    zarr_store = zarr_module$ZipStore(file.path(output_dir, smp, "analysis.zarr.zip"), mode="w")
    
    # Create a hierarchy with root and group "cell_groups"
    root = zarr_module$group(store=zarr_store)
    cell_groups = root$create_group("cell_groups")
    
    # Add zarr groups
    for(i in seq_along(zarr_categorial_data)) {
      indices = zarr_categorial_data[[i]]$indices
      indptr = zarr_categorial_data[[i]]$indptr
      chunk_size = max(length(indices), 1)
      
      group = cell_groups$create_group(as.character(i-1))
      group$array("indices", 
                  numpy_module$array(indices, dtype="uint32"), 
                  dtype="uint32", 
                  chunks=reticulate::tuple(chunk_size))
      group$array("indptr", 
                  numpy_module$array(indptr, dtype="uint32"), 
                  dtype="uint32", 
                  chunks=reticulate::tuple(chunk_size))
    }
    
    cell_groups$attrs$put(zarr_attr)
    zarr_store$close()
  }
}

################################################################################
# IMPORTANT: The following functions are part of the pull request that implements
# reading of spaceranger segmentations for Seurat: https://github.com/satijalab/seurat/pull/10028
################################################################################

# Once they are part of an official Seurat release, this code needs to be removed.
assertthat::assert_that(!exists("Read10X_Segmentations", where="package:Seurat", mode="function"),
                        msg="Read10X_Segmentations has already been defined in the Seurat package. Please remove the function definition in R/functions_io.R!")
Read10X_Segmentations <- function (image.dir,
                                   data.dir,
                                   image.name = "tissue_lowres_image.png",
                                   assay = "Spatial.Polygons",
                                   slice = "slice1.polygons",
                                   segmentation.type = "cell",
                                   cell.names
)
{
    
    image <- png::readPNG(source = file.path(image.dir, image.name))
    scale.factors <- Read10X_ScaleFactors(filename = file.path(image.dir,
                                                               "scalefactors_json.json"))
    key <- Key(slice, quiet = TRUE)
    
    sf.data <- Read10X_HD_GeoJson(data.dir = data.dir, image.dir = image.dir, scale.factor = "lowres")
    
    # Create a Segmentation object based on sf, populate sf.data and polygons
    segmentation <- CreateSegmentation(sf.data)
    
    # Named list with segmentation
    boundaries <- list(segmentation = segmentation)
    
    # Build VisiumV2 object
    visium.v2 <- new(
        Class = "VisiumV2",
        boundaries = boundaries,
        assay = assay,
        key = key,
        image = image,
        scale.factors = scale.factors
    )
    
    return(visium.v2)
}

assertthat::assert_that(!exists("Read10X_HD_GeoJson", where="package:Seurat", mode="function"),
                        msg="Read10X_HD_GeoJson has already been defined in the Seurat package. Please remove the function definition in R/functions_io.R!")
Read10X_HD_GeoJson <- function(data.dir, image.dir, segmentation.type = "cell", scale.factor = NULL) {
    segmentation_polygons <- sf::read_sf(file.path(data.dir,"segmented_outputs", paste0(segmentation.type, "_segmentations.geojson")))
    if (!is.null(scale.factor)) {
        scale.factors <- Read10X_ScaleFactors(
            filename = file.path(image.dir, "scalefactors_json.json")
        )
        segmentation_polygons$geometry <- segmentation_polygons$geometry*scale.factors[[scale.factor]]
    }
    
    segmentation_polygons$barcodes <- Format10X_GeoJson_CellID(segmentation_polygons$cell_id)
    segmentation_polygons
}

assertthat::assert_that(!exists("Format10X_GeoJson_CellID", where="package:Seurat", mode="function"),
                        msg="Format10X_GeoJson_CellID has already been defined in the Seurat package. Please remove the function definition in R/functions_io.R!")
Format10X_GeoJson_CellID <- function(ids, prefix = "cellid_", suffix = "-1", digits = 9) {
    format_string <- paste0("%0", as.integer(digits), "d")
    
    formatted_ids <- sapply(ids, function(id) {
        numeric_part <- sprintf(format_string, as.integer(id))
        paste0(prefix, numeric_part, suffix)
    })
    
    return(formatted_ids)
}

################################################################################

