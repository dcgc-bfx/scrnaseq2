#' Mapping of 10x Feature Types to Assay Names
#'
#' A named list mapping 10x Genomics feature types (as they appear in the
#' features.tsv file) to standardized assay names used in Seurat objects.
#'
#' @format A named list where:
#'   \itemize{
#'     \item Names are the feature type strings from 10x data
#'     \item Values are the corresponding Seurat assay names
#'   }
#'
#' @examples
#' # Get the assay name for gene expression data
#' Assays_10x[["Gene Expression"]]  # Returns "RNA"
#'
#' @export
Assays_10x = list("Gene Expression" = "RNA",
                  "Multiplexing Capture" = "MPLX",
                  "VDJ T" = "VDJT",
                  "VDJ B" = "VDJB",
                  "Antibody Capture" = "ADT",
                  "Peaks" = "ATAC",
                  "Motifs" = "TFMotifs",
                  "CRISPR Guide Capture" = "CRISPR",
                  "Antigen Capture" = "BEAM",
                  "Custom" = "CUSTOM",
                  "Negative Control Codeword" = "ControlCodeword",
                  "Negative Control Probe" = "ControlProbe",
                  "Unassigned Codeword" = "BlankCodeword",
                  "Blank Codeword" = "BlankCodeword")

#' Mapping of Scale Bio Feature Types to Assay Names
#'
#' A named list mapping Scale Bio feature types to standardized assay names
#' used in Seurat objects.
#'
#' @format A named list where:
#'   \itemize{
#'     \item Names are the feature type strings from Scale Bio data
#'     \item Values are the corresponding Seurat assay names
#'   }
#'
#' @export
Assays_Scale = list("Gene Expression" = "RNA",
                  "Antibody Capture" = "ADT",
                  "Chromatin Accessibility" = "ATAC",
                  "CRISPR Guide Capture" = "CRISPR")

#' Mapping of Parse Biosciences Feature Types to Assay Names
#'
#' A named list mapping Parse Biosciences feature types to standardized assay
#' names used in Seurat objects.
#'
#' @format A named list where:
#'   \itemize{
#'     \item Names are the feature type strings from Parse Bio data
#'     \item Values are the corresponding Seurat assay names
#'   }
#'
#' @export
Assays_Parse = list("Gene Expression" = "RNA",
                    "Antibody Capture" = "ADT",
                    "Chromatin Accessibility" = "ATAC",
                    "CRISPR Guide Capture" = "CRISPR")

#' Mapping of SmartSeq Feature Types to Assay Names
#'
#' A named list mapping SmartSeq feature types to standardized assay names
#' used in Seurat objects.
#'
#' @format A named list where:
#'   \itemize{
#'     \item Names are the feature type strings from SmartSeq data
#'     \item Values are the corresponding Seurat assay names
#'   }
#'
#' @export
Assays_Smartseq = list("Gene Expression" = "RNA",
                    "Antibody Capture" = "ADT",
                    "Chromatin Accessibility" = "ATAC")

#' Read Metadata from an AnnData HDF5 File
#'
#' Reads barcode or feature metadata from an AnnData object stored in HDF5
#' format (typically generated by scanpy). This function is largely adapted
#' from \code{Azimuth::LoadH5ADobs}.
#'
#' @param h5ad_file Character. Path to an AnnData object in HDF5 format.
#' @param type Character. Type of metadata to read. Either \code{"obs"} for
#'   barcode/cell metadata or \code{"var"} for feature/gene metadata.
#'
#' @return A \code{data.frame} containing the metadata with row names set to
#'   the index column from the HDF5 file.
#'
#' @importFrom hdf5r H5File h5attr
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun
#' # Read barcode metadata
#' barcode_meta <- ReadMetadata_h5ad("data.h5ad", type = "obs")
#'
#' # Read feature metadata
#' feature_meta <- ReadMetadata_h5ad("data.h5ad", type = "var")
#' }
ReadMetadata_h5ad = function(h5ad_file, type) {
  # Checks
  assertthat::is.readable(h5ad_file)
  
  # Open hdf5 file with anndata object
  hdf5_fh = hdf5r::H5File$new(h5ad_file, mode = "r+")
  hd5_data = hdf5_fh[[paste0("/", type)]]
  
  # Create metadata matrix
  index.var = hdf5r::h5attr(hd5_data, "_index")
  index = hd5_data[[index.var]][]
  groups = names(hd5_data)
  matrix = as.data.frame(x = matrix(
    data = NA,
    nrow = length(index),
    ncol = length(groups)
  ))
  colnames(matrix) = groups
  rownames(matrix) = index
  
  # Get columns and values
  if ("__categories" %in% names(x = hd5_data)) {
    hd5_data_cate = hd5_data[["__categories"]]
    for (i in seq_along(groups)) {
      g.i = groups[i]
      value_i = hd5_data[[g.i]][]
      if (g.i %in% names(x = hd5_data_cate)) {
        value_i = factor(x = value_i, labels = hd5_data[[g.i]][])
      }
      matrix[, i] = value_i
    }
  } else {
    for (i in seq_along(groups)) {
      g.i = groups[i]
      if (all(names(hd5_data[[g.i]]) == c("categories", "codes"))) {
        if (length(unique(hd5_data[[g.i]][["codes"]][])) == length(hd5_data[[g.i]][["categories"]][])) {
          value_i = factor(x = hd5_data[[g.i]][["codes"]][], labels = hd5_data[[g.i]][["categories"]][])
        }
        else {
          value_i = hd5_data[[g.i]][["codes"]][]
        }
      }
      else {
        value_i = tryCatch(expr = hd5_data[[g.i]][],
                           error=function(e) return("unknown")
        )
      }
      matrix[, i] = value_i
    }
  }
  
  hdf5_fh$close_all()
  return(as.data.frame(matrix))
}

#' Read Metadata from a Character-Separated File
#'
#' Reads barcode or feature metadata from a delimited text file (CSV, TSV, etc.).
#' The first column must contain unique identifiers (barcode or feature IDs)
#' which will be used as row names.
#'
#' @param csv_file Character. Path to a character-separated file. The first column
#'   must contain the respective barcode or feature ID.
#'
#' @return A \code{data.frame} containing the metadata with row names set to the
#'   values in the first column.
#'
#' @importFrom readr read_delim cols
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read metadata from a TSV file
#' metadata <- ReadMetadata_csv("cell_metadata.tsv")
#' }
ReadMetadata_csv = function(csv_file) {
  # Checks
  assertthat::is.readable(csv_file)
  
  # Read table
  meta_data = readr::read_delim(csv_file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types=readr::cols())
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatString("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Read Metadata from an Excel File
#'
#' Reads barcode or feature metadata from an Excel file (.xls or .xlsx).
#' The first column must contain unique identifiers (barcode or feature IDs)
#' which will be used as row names.
#'
#' @param excel_file Character. Path to an Excel file (.xls or .xlsx).
#' @param sheet Integer or character. Sheet number or name to read from.
#'   Default is \code{1} (first sheet).
#'
#' @return A \code{data.frame} containing the metadata with row names set to the
#'   values in the first column.
#'
#' @importFrom readxl read_excel
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read metadata from the first sheet
#' metadata <- ReadMetadata_excel("cell_metadata.xlsx")
#'
#' # Read metadata from a specific sheet
#' metadata <- ReadMetadata_excel("cell_metadata.xlsx", sheet = 2)
#' }
ReadMetadata_excel = function(excel_file, sheet=1) {
  # Checks
  assertthat::is.readable(excel_file)
  
  # Read table
  meta_data = readxl::read_excel(excel_file, sheet=sheet, col_names=TRUE)
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatString("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Read Metadata from an RDS File
#'
#' Reads barcode or feature metadata from an R RDS file. This method preserves
#' factor levels and other R-specific data structures that would be lost in
#' text-based formats.
#'
#' @param rds_file Character. Path to an R RDS file containing a data frame.
#'   The first column must contain the respective barcode or feature ID.
#'
#' @return A \code{data.frame} containing the metadata with row names set to the
#'   values in the first column. Factor levels are preserved.
#'
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read metadata from an RDS file (preserves factor levels)
#' metadata <- ReadMetadata_rds("cell_metadata.rds")
#' }
ReadMetadata_rds = function(rds_file) {
  # Checks
  assertthat::is.readable(rds_file)
  
  # Read table
  meta_data = readRDS(rds_file)
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatString("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Read Metadata from Various File Formats
#'
#' A unified function to read barcode or feature metadata from various file
#' formats including CSV, TSV, Excel, or RDS files. Automatically detects the
#' file type from the extension and uses the appropriate reader.
#'
#' @param file Character. Path to a metadata file. Supported formats:
#'   \itemize{
#'     \item \code{.csv}, \code{.tsv} – character-separated files (can be gzipped)
#'     \item \code{.xls}, \code{.xlsx} – Excel files (sheet can be specified with \code{:})
#'     \item \code{.rds} – R RDS files (preserves factor levels)
#'   }
#'   For Excel files, append \code{:<sheet_number>} to specify a sheet
#'   (e.g., \code{"metadata.xlsx:2"}).
#'
#' @return A \code{data.frame} containing the metadata with row names set to the
#'   values in the first column.
#'
#' @importFrom tools file_ext
#' @importFrom assertthat assert_that not_empty
#' @export
#'
#' @examples
#' \dontrun{
#' # Read from CSV
#' metadata <- ReadMetadata("cell_metadata.csv")
#'
#' # Read from gzipped TSV
#' metadata <- ReadMetadata("cell_metadata.tsv.gz")
#'
#' # Read from Excel, second sheet
#' metadata <- ReadMetadata("cell_metadata.xlsx:2")
#'
#' # Read from RDS (preserves factor levels)
#' metadata <- ReadMetadata("cell_metadata.rds")
#' }
ReadMetadata = function(file) {
  # Sheet number appended?
  sheet = 1
  if (grepl(":\\d+$", file)) {
    sheet = gsub(pattern=".+:(\\d+)$", replacement="\\1", x=file)
    file = gsub(pattern=":\\d+$", replacement="", x=file)
  }
  
  # Checks
  extension = tools::file_ext(gsub(pattern="\\.gz$", replacement="", x=file))
  valid_extensions = c("csv", "tsv", "xls", "xlsx", "rds")
  assertthat::assert_that(extension %in% valid_extensions,
                          msg=FormatString("Metadata file type must be: {valid_extensions*} (file can be gzipped)."))
  
  # Read metadata
  if (extension %in% c("csv", "tsv")) {
    meta_data = ReadMetadata_csv(file)
  } else if(extension %in% c("xls", "xlsx")) {
    meta_data = ReadMetadata_excel(file, sheet=sheet)
  } else if(extension %in% c("rds")) {
    meta_data = ReadMetadata_rds(file)
  }
  
  # Assert that it is not empty
  assertthat::assert_that(assertthat::not_empty(meta_data),
                          msg=FormatString("Metadata file {file} is empty."))

  return(meta_data)
}

#' Read Datasets Table
#'
#' Reads a datasets configuration table from a file. This table typically
#' contains information about multiple datasets to be processed together,
#' including paths, sample names, and other metadata.
#'
#' @param file Character. Path to a datasets table file. Supported formats:
#'   \itemize{
#'     \item \code{.csv}, \code{.tsv} – character-separated files (can be gzipped)
#'     \item \code{.xls}, \code{.xlsx} – Excel files (sheet can be specified with \code{:})
#'     \item \code{.rds} – R RDS files
#'   }
#'   For Excel files, append \code{:<sheet_number>} to specify a sheet.
#'
#' @return A \code{data.frame} containing the datasets configuration.
#'
#' @details
#' This function is similar to \code{\link{ReadMetadata}} but is specifically

#' designed for reading datasets configuration tables. It performs additional
#' validation checks specific to dataset tables.
#'
#' @importFrom tools file_ext
#' @importFrom readr read_delim
#' @importFrom readxl read_excel
#' @importFrom assertthat assert_that not_empty
#' @export
#'
#' @examples
#' \dontrun{
#' # Read datasets configuration
#' datasets <- ReadDatasetsTable("datasets.xlsx")
#' }
ReadDatasetsTable = function(file) {
  # Sheet number appended?
  sheet = 1
  if (grepl(":\\d+$", file)) {
    sheet = gsub(pattern=".+:(\\d+)$", replacement="\\1", x=file)
    file = gsub(pattern=":\\d+$", replacement="", x=file)
  }
  
  # Checks
  extension = tools::file_ext(gsub(pattern="\\.gz$", replacement="", x=file))
  valid_extensions = c("csv", "tsv", "xls", "xlsx", "rds")
  assertthat::assert_that(extension %in% valid_extensions,
                          msg=FormatString("Datasets file type must be: {valid_extensions*} (file can be gzipped)."))
  
  # Read datasets table
  if (extension %in% c("csv", "tsv")) {
    datasets_table = readr::read_delim(file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types="text")
  } else if(extension %in% c("xls", "xlsx")) {
    datasets_table = readxl::read_excel(file, sheet=sheet, col_names=TRUE)
  }
  assertthat::assert_that(assertthat::not_empty(datasets_table),
                          msg=FormatString("Datasets file {file} is empty."))
  
  # Check that all columns are present
  
  
  return(datasets_table)
}

#' Read Counts from a Character-Separated File
#'
#' Reads count data from a delimited text file (CSV, TSV, etc.) and converts
#' it to a sparse matrix format. The first column should contain feature IDs
#' (or barcode IDs if \code{transpose = TRUE}).
#'
#' @param csv_file Character. Path to a character-separated counts file.
#' @param transpose Logical. If \code{TRUE}, rows are barcodes and columns are
#'   features (transposed orientation). Default is \code{FALSE}.
#' @param strip_suffix Character or \code{NULL}. String to be removed from the
#'   end of barcode names. Default is \code{NULL}.
#'
#' @return A list containing one sparse matrix (\code{dgCMatrix}) with element
#'   name "All". The matrix has features as rows and barcodes as columns.
#'   The dataset path is attached as an attribute.
#'
#' @importFrom Matrix Matrix
#' @importFrom readr read_delim cols
#' @importFrom assertthat is.readable assert_that
#' @importFrom magrittr %>%
#' @export
#'
#' @examples
#' \dontrun{
#' # Read counts from a CSV file
#' counts_list <- ReadCounts_csv("counts.csv")
#' counts_matrix <- counts_list[["All"]]
#'
#' # Read transposed counts (cells as rows)
#' counts_list <- ReadCounts_csv("counts.csv", transpose = TRUE)
#' }
ReadCounts_csv = function(csv_file, transpose=FALSE, strip_suffix=NULL) {
  library(magrittr)
  
  # Checks
  assertthat::is.readable(csv_file)
  
  # Read character-separated counts table with gene id in the first column and counts in the other columns 
  counts_data = readr::read_delim(csv_file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types=readr::cols())
  row_ids = counts_data[, 1, drop=TRUE]
  col_ids = colnames(counts_data)
  col_ids = col_ids[-1]
  
  # Check that barcodes and features are unique
  assertthat::assert_that(sum(duplicated(col_ids)) == 0,
                          msg=FormatString("Dataset {csv_file} contains at least two barcodes with the same name."))
  
  assertthat::assert_that(sum(duplicated(row_ids)) == 0,
                          msg=FormatString("Dataset {csv_file} contains at least two features with the same name.")) 
  
  # Strip suffix from barcodes if requested
  if (!is.null(strip_suffix)) {
    col_ids = trimws(col_ids, which="right", whitespace=strip_suffix)
  }
  
  assertthat::assert_that(sum(duplicated(col_ids)) == 0,
                          msg=FormatString("Dataset {csv_file} contains at least two barcodes with the same name after removing the suffix {strip_suffix}."))
  
  # Check that all columns are numeric
  is_numeric = sapply(counts_data, is.numeric)
  assertthat::assert_that(all(is_numeric[-1]),
                          msg=FormatString("There are non-numeric columns in dataset {csv_file}! Only the first column may be non-numeric."))
  
  
  # Create sparse matrix
  if (transpose) {
    counts_data = Matrix::Matrix(data=counts_data[, -1, drop=FALSE] %>% as.matrix() %>% t(), 
                                 sparse=TRUE,
                                 dimnames = list(col_ids, row_ids))
  } else {
    counts_data = Matrix::Matrix(data=counts_data[, -1, drop=FALSE] %>% as.matrix(), 
                                 sparse=TRUE,
                                 dimnames = list(row_ids, col_ids))
  }
  
  # Attach path
  attr(counts_data, "path") = csv_file
  
  return(list(All=counts_data))
}
  
#' Read Counts from Market Exchange Format
#'
#' Reads count data from a directory in Market Exchange (MTX) format.
#' This is a generic reader that can be customized for different platforms
#' through its parameters.
#'
#' @param mtx_directory Character. Path to the counts directory containing
#'   matrix, barcodes, and features files.
#' @param mtx_file_name Character. Name of the matrix MTX file.
#'   Default is \code{"matrix.mtx.gz"}.
#' @param transpose Logical. If \code{TRUE}, rows are cells and columns are
#'   genes in the input matrix. Default is \code{FALSE}.
#' @param barcodes_file_name Character. Name of the barcodes file.
#'   Default is \code{"barcodes.tsv.gz"}.
#' @param barcodes_column_names Logical or character vector. How to name columns
#'   in the barcodes file. \code{TRUE} uses the first line as header, \code{FALSE}
#'   uses generic names. Alternatively, provide a character vector with column names.
#'   Default is \code{FALSE}.
#' @param features_file_name Character. Name of the features file.
#'   Default is \code{"features.tsv.gz"}.
#' @param features_column_names Logical or character vector. How to name columns
#'   in the features file. Same options as \code{barcodes_column_names}.
#'   Default is \code{FALSE}.
#' @param feature_type_column Integer or \code{NULL}. Column number in the features
#'   file that identifies feature type. If \code{NULL}, all features are treated
#'   as "Gene Expression". Default is \code{NULL}.
#' @param delim Character. Delimiter used in barcodes and features files.
#'   Default is \code{"\\t"} (tab).
#' @param strip_suffix Character or \code{NULL}. String to remove from the end
#'   of barcode names. Default is \code{NULL}.
#'
#' @return A named list with one sparse matrix (\code{dgCMatrix}) per feature type.
#'   Each matrix has features as rows and barcodes as columns. Additional metadata
#'   is attached as attributes:
#'   \itemize{
#'     \item \code{barcode_metadata} – data frame with barcode information
#'     \item \code{feature_metadata} – data frame with feature information
#'     \item \code{path} – the input directory path
#'   }
#'
#' @importFrom Matrix readMM t
#' @importFrom readr read_delim
#' @importFrom purrr map map_lgl
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' # Read 10x-style MTX format
#' counts_list <- ReadCounts_mtx(
#'   mtx_directory = "filtered_feature_bc_matrix",
#'   feature_type_column = 3
#' )
#' }
ReadCounts_mtx = function(mtx_directory, mtx_file_name="matrix.mtx.gz", transpose=FALSE, barcodes_file_name="barcodes.tsv.gz", barcodes_column_names=FALSE, features_file_name="features.tsv.gz", features_column_names=FALSE, feature_type_column=NULL, delim="\t", strip_suffix=NULL) {
  # Checks
  for(f in file.path(mtx_directory, c(mtx_file_name, barcodes_file_name, features_file_name))) assertthat::is.readable(f)
  
  # Read market exchange format file
  counts_data = Matrix::readMM(file=file.path(mtx_directory, mtx_file_name))
  if (transpose) {
    counts_data = Matrix::t(counts_data)
  }
  counts_data = as(counts_data, "dgCMatrix")
  
  # Read barcodes file
  barcodes_data = readr::read_delim(file=file.path(mtx_directory, barcodes_file_name), col_names=barcodes_column_names, delim=delim, progress=FALSE, show_col_types=FALSE)
  barcodes_col_nms = colnames(barcodes_data)
  if (is.logical(barcodes_column_names) && barcodes_column_names==FALSE) {
    barcodes_col_nms = rep("barcode_info", length(barcodes_col_nms)) %>% make.unique(sep="_")

  }
  barcodes_col_nms[1] = "orig_barcode"
  colnames(barcodes_data) = barcodes_col_nms
  barcodes_data = as.data.frame(barcodes_data)
  
  if (!is.null(strip_suffix)) {
    rownames(barcodes_data) = trimws(barcodes_data[, 1, drop=TRUE], which="right", whitespace=strip_suffix)
  } else {
    rownames(barcodes_data) = barcodes_data[, 1, drop=TRUE]
  }
  
  # Read features file
  features_data = readr::read_delim(file=file.path(mtx_directory, features_file_name), col_names=features_column_names, delim=delim, progress=FALSE, show_col_types=FALSE)
  feature_col_nms = colnames(features_data)
  if (is.logical(features_column_names) && features_column_names==FALSE)  {
    feature_col_nms = make.unique(rep("feature_info", length(feature_col_nms)), sep="_")
    
  }
  feature_col_nms[1] = "feature_id"
  colnames(features_data) = feature_col_nms
  features_data = as.data.frame(features_data)
  rownames(features_data) = features_data[, 1, drop=TRUE]
  
  # Split by feature type column
  feature_sets = list("All" = rep(TRUE, nrow(features_data)))
  if (!is.null(feature_type_column)) {
    feature_types = unique(features_data[, feature_type_column, drop=TRUE])
    
    feature_sets = purrr::map(feature_types, function(f) {
      return(features_data[, feature_type_column, drop=TRUE] == f)
    })
    names(feature_sets) = feature_types
  }
  
  # Generate list of matrix (matrices)
  counts_lst = purrr::map(feature_sets, function(f) {
    # Add counts
    cts = counts_data[f, ]
    col_ids = rownames(barcodes_data[, 1, drop=FALSE])
    row_ids = rownames(features_data[f, 1, drop=FALSE])
    dimnames(cts) = list(row_ids, col_ids)
    
    # Add barcode metadata
    bc_meta = barcodes_data[colnames(cts), , drop=FALSE]
    attr(cts, "barcode_metadata") = bc_meta
    
    # Add feature metadata
    fc_meta = features_data[f, , drop=FALSE]
    keep = purrr::map_lgl(colnames(fc_meta), function(n) {
      return(all(!is.na(fc_meta[, n, drop=TRUE]) & nchar(fc_meta[, n, drop=TRUE]) > 0))
    })
    attr(cts, "feature_metadata") = fc_meta[, keep, drop=FALSE]
    
    # Attach path
    attr(cts, "path") = mtx_directory
    
    return(cts)
  })
  names(counts_lst) = names(counts_lst)
  
  return(counts_lst)
}

#' Read Counts from an AnnData HDF5 File
#'
#' Reads count data from an AnnData object stored in HDF5 format (typically
#' generated by scanpy). This function returns an iterator object that reads
#' data directly from file, minimizing memory usage for large datasets.
#'
#' @param h5ad_file Character. Path to an AnnData object in HDF5 format.
#' @param strip_suffix Character or \code{NULL}. String to remove from the end
#'   of barcode names. Default is \code{NULL}.
#'
#' @return A list containing one \code{IterableMatrix} (BPCells format) with
#'   element name "All". The matrix reads data on-demand from the HDF5 file.
#'   Additional metadata is attached as attributes:
#'   \itemize{
#'     \item \code{barcode_metadata} – data frame with barcode information
#'     \item \code{feature_metadata} – data frame with feature information
#'     \item \code{path} – the input file path
#'   }
#'
#' @details
#' This function does not read the entire counts data into memory. Instead,
#' it returns an iterator object that retrieves values directly from the file
#' (random access). It is recommended to convert this object to a BPCells
#' on-disk storage format for better performance.
#'
#' Note: This function does not discriminate between feature types.
#'
#' @importFrom BPCells open_matrix_anndata_hdf5
#' @importFrom assertthat is.readable
#' @importFrom magrittr %>%
#' @importFrom dplyr select
#' @export
#'
#' @examples
#' \dontrun{
#' # Read counts from an h5ad file
#' counts_list <- ReadCounts_h5ad("data.h5ad")
#' counts_matrix <- counts_list[["All"]]
#' }
ReadCounts_h5ad = function(h5ad_file, strip_suffix=NULL) {
  library(magrittr)
  
  # Checks
  assertthat::is.readable(h5ad_file)
  
  # Read barcodes and features data separately
  barcodes_data = ReadMetadata_h5ad(h5ad_file=h5ad_file, type='obs')
  barcodes_col_nms = colnames(barcodes_data)
  barcodes_col_nms[1] = "orig_barcode"
  colnames(barcodes_data) = barcodes_col_nms
  if (!is.null(strip_suffix)) {
    rownames(barcodes_data) = trimws(barcodes_data[, 1, drop=TRUE], which="right", whitespace=strip_suffix)
  } else {
    rownames(barcodes_data) = barcodes_data[, 1, drop=TRUE]
  }
  
  
  features_data = ReadMetadata_h5ad(h5ad_file=h5ad_file, type='var')
  features_data = features_data %>% dplyr::select(feature_id=gene_id,
                                  feature_name=gene_name,
                                  feature_type=gene_id,
                                  setdiff(colnames(features_data), c("gene_id", "gene_name")))
  features_data$feature_type = NA
  rownames(features_data) = features_data$feature_id
  
  # Read counts and attach barcodes/features data
  counts_data=BPCells::open_matrix_anndata_hdf5(h5ad_file)
  rownames(counts_data) = rownames(features_data)
  colnames(counts_data) = rownames(barcodes_data)
  attr(counts_data, "barcode_metadata") = barcodes_data
  attr(counts_data, "feature_metadata") = features_data
  
  # Attach path
  attr(counts_data, "path") = h5ad_file
  
  return(list(All=counts_data))
}


#' Read Counts from SmartSeq Data
#'
#' Reads count data produced by plate-based methods like SmartSeq2 or SmartSeq3.
#' Supports both character-separated files and Market Exchange format directories.
#'
#' @param path Character. Path to counts data. Can be:
#'   \itemize{
#'     \item A character-separated file (CSV, TSV)
#'     \item A matrix exchange format directory containing \code{matrix.mtx.gz},
#'       \code{barcodes.tsv.gz}, and \code{features.tsv.gz}
#'   }
#' @param assays Character. The assay name to assign. SmartSeq technologies
#'   currently do not support multi-assay datasets, so this simply sets the
#'   assay type.
#' @param version Character. SmartSeq version. Either \code{"2"} for SmartSeq2
#'   or \code{"3"} for SmartSeq3.
#' @param transpose Logical. If \code{TRUE}, rows are cells and columns are genes
#'   in the input file. Default is \code{FALSE}.
#'
#' @return A list containing one sparse matrix (\code{dgCMatrix}) per assay.
#'   Additional metadata is attached as attributes:
#'   \itemize{
#'     \item \code{barcode_metadata} – data frame with barcode information
#'     \item \code{feature_metadata} – data frame with feature information
#'     \item \code{technology} – "Smartseq2" or "Smartseq3"
#'     \item \code{assay} – the assay name
#'     \item \code{path} – the input path
#'   }
#'
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read SmartSeq2 counts from a CSV file
#' counts_list <- ReadCounts_SmartSeq(
#'   path = "counts.csv",
#'   assays = "RNA",
#'   version = "2"
#' )
#' }
ReadCounts_SmartSeq = function(path, assays, version, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)
  assertthat::assert_that(version %in% c("2", "3"),
                          msg="Smartseq version must be '2' or '3'.")
  
  
  # Convert to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_Smartseq), Assays_Smartseq)
  valid_assays = names(assay_to_feature_type)
  assertthat::assert_that(assays %in% valid_assays,
                          msg=FormatString("'{assays*} must be: {valid_assays*}."))

  if (dir.exists(path)) {
    # market exchange format
    counts_lst = ReadCounts_mtx(mtx_directory=path,
                   transpose=transpose,
                   mtx_directory="matrix.mtx.gz",
                   barcodes_file_name="barcodes.tsv.gz",
                   barcodes_column_names=FALSE,
                   features_file_name="features.tsv.gz",
                   features_column_names=FALSE,
                   feature_type_column=NULL,
                   delim="\t",
                   strip_suffix=NULL)
    counts_lst = counts_lst[1]
  } else {
    # character-separated file
    counts_lst = ReadCounts_csv(csv_file=path, transpose=transpose)
    barcode_metadata = data.frame(orig_barcode=colnames(counts_lst[[1]]),
                                  row.names=colnames(counts_lst[[1]]))
    feature_metadata = data.frame(feature_id=rownames(counts_lst[[1]]),
                                  row.names=rownames(counts_lst[[1]]))
    attr(counts_lst[[1]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[1]], "feature_metadata") = feature_metadata
  }
  
  # Assay
  names(counts_lst) = assays[1]
  
  # Add attributes technology and assay
  attr(counts_lst[[1]], "technology") = paste0("Smartseq", version)
  attr(counts_lst[[1]], "assay") = assays[1]
  
  return(counts_lst)
}

#' Read 10x Counts from Market Exchange Format
#'
#' Reads count data from a 10x Genomics directory in Market Exchange (MTX) format.
#' Automatically detects file names and column structures based on 10x conventions.
#'
#' @param mtx_directory Character. Path to 10x counts directory containing the
#'   matrix, barcodes, and features files.
#' @param strip_suffix Character or \code{NULL}. String to remove from the end
#'   of barcode names (e.g., "-1" suffix). Default is \code{NULL}.
#'
#' @return A named list with one sparse matrix (\code{dgCMatrix}) per feature type
#'   (e.g., "Gene Expression", "Antibody Capture"). Each matrix has features as
#'   rows and barcodes as columns. Additional metadata is attached as attributes.
#'
#' @importFrom dplyr case_when
#' @importFrom readr read_delim
#' @export
#'
#' @examples
#' \dontrun{
#' # Read 10x counts from filtered_feature_bc_matrix directory
#' counts_list <- ReadCounts_10x_mtx("filtered_feature_bc_matrix")
#' }
ReadCounts_10x_mtx = function(mtx_directory, strip_suffix=NULL) {
  # Determine the name of the matrix file
  mtx_file_name = dplyr::case_when(file.exists(file.path(mtx_directory, "matrix.mtx.gz")) ~ "matrix.mtx.gz",
                                   file.exists(file.path(mtx_directory, "matrix.mtx")) ~ "matrix.mtx")
  
  # Determine the name of the barcodes file
  barcodes_file_name = dplyr::case_when(
    file.exists(file.path(mtx_directory, "barcodes.tsv.gz")) ~ "barcodes.tsv.gz",
    file.exists(file.path(mtx_directory, "barcodes.tsv")) ~ "barcodes.tsv"
  )
  
  # Determine the name of the features file
  features_file_name = dplyr::case_when(
    file.exists(file.path(mtx_directory, "features.tsv.gz")) ~ "features.tsv.gz",
    file.exists(file.path(mtx_directory, "features.tsv")) ~ "features.tsv",
    file.exists(file.path(mtx_directory, "genes.tsv")) ~ "genes.tsv",
    file.exists(file.path(mtx_directory, "peaks.bed")) ~ "peaks.bed",
    file.exists(file.path(mtx_directory, "motifs.tsv")) ~ "motifs.tsv"
  )
  
  # Determine the column name of the features file
  if (features_file_name == "peaks.bed") {
    # 10x atac
    features_column_names = c("chr", "start", "end")
  } else if (features_file_name == "motifs.tsv") {
    # 10x atac
    features_column_names = c("tf_full_name", "tf_name")
  } else {
    feature_metadata = readr::read_delim(file.path(mtx_directory, features_file_name), delim="\t", col_names=FALSE, n_max=3, progress=FALSE, show_col_types=FALSE)
    
    if (ncol(feature_metadata) == 6) {
      # 10x multiome
      features_column_names = c("feature_id",
                                "feature_name",
                                "feature_type",
                                "chr",
                                "start",
                                "end")
    } else {
      # 10x other
      features_column_names = c("feature_id", "feature_name", "feature_type")
    }
  }
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=FALSE,
    features_file_name=features_file_name,
    features_column_names=features_column_names,
    feature_type_column=3,
    delim="\t",
    strip_suffix=strip_suffix
  )
  
  return(counts_lst)
}

#' Read 10x Counts from HDF5 Format
#'
#' Reads count data from a 10x Genomics HDF5 file. This function returns an
#' iterator object that reads data on-demand from the file, minimizing memory
#' usage for large datasets.
#'
#' @param h5_file Character. Path to a 10x HDF5 counts file (e.g.,
#'   \code{filtered_feature_bc_matrix.h5}).
#' @param strip_suffix Character or \code{NULL}. String to remove from the end
#'   of barcode names. Default is \code{NULL}.
#'
#' @return A named list with one \code{IterableMatrix} (BPCells format) per
#'   feature type. Each matrix reads data on-demand from the HDF5 file.
#'   Additional metadata is attached as attributes:
#'   \itemize{
#'     \item \code{barcode_metadata} – data frame with barcode information
#'     \item \code{feature_metadata} – data frame with feature information
#'     \item \code{path} – the input file path
#'   }
#'
#' @details
#' This function does not read the entire counts data into memory. Instead,
#' it returns an iterator object that retrieves values directly from the file.
#' This is recommended for large datasets to minimize memory usage.
#'
#' @importFrom hdf5r H5File
#' @importFrom BPCells open_matrix_10x_hdf5
#' @importFrom purrr map map_lgl
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' # Read 10x counts from an h5 file
#' counts_list <- ReadCounts_10x_h5("filtered_feature_bc_matrix.h5")
#' }
ReadCounts_10x_h5 = function(h5_file, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(h5_file)
  
  # Read barcodes and features data separately
  hdf5_fh = hdf5r::H5File$new(h5_file, mode = "r+")
  
  # No barcodes data, just a vector of the barcodes
  h5_group = names(hdf5_fh)[1]
  h5_group = hdf5_fh[[h5_group]]
  barcodes = h5_group[["barcodes"]][]
  if (!is.null(strip_suffix)) {
    barcodes_data = data.frame(row.names=trimws(barcodes, which="right", whitespace=strip_suffix), orig_barcode=barcodes)
  } else {
    barcodes_data = data.frame(row.names=barcodes, orig_barcode=barcodes)
  }
  
  # Read feature data
  if ("features" %in% names(h5_group)) {
    hdf5_features = h5_group[["features"]]
    feature_id=hdf5_features[["id"]][]
    feature_name=hdf5_features[["name"]][]
    feature_type=hdf5_features[["feature_type"]][]
  } else if ("genes" %in% names(h5_group)) {
    hdf5_features = h5_group[["genes"]]
    feature_id=h5_group[["genes"]][]
    feature_name=h5_group[["gene_names"]][]
    feature_type="Gene Expression"
  }
  
  features_data = data.frame(
    feature_id, feature_name, feature_type
  )
  
  if ("_all_tag_keys" %in% names(hdf5_features)) {
    non_standard_features = hdf5_features[["_all_tag_keys"]][]
  } else {
    non_standard_features = c()
  }

  if (length(non_standard_features) > 0) {
    non_standard_features_data = purrr::map(non_standard_features, function(f) {
      return(hdf5_features[[f]][])
    })
    names(non_standard_features_data) = non_standard_features
    
    if ("interval" %in% names(non_standard_features_data)) {
      interval_data = list(
        chr = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\1",
          non_standard_features_data[["interval"]]
        ),
        start = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\2",
          non_standard_features_data[["interval"]]
        ),
        end = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\3",
          non_standard_features_data[["interval"]]
        )
      )
      
      idx = which(names(non_standard_features_data) == "interval")
      if (idx == 1) {
        pre_idx = c()
      } else {
        pre_idx = 1:(idx - 1)
      }
      if (idx == length(non_standard_features_data)) {
        post_idx = c()
      } else {
        post_idx = (idx + 1):length(non_standard_features_data)
      }
      non_standard_features_data = c(non_standard_features_data[pre_idx],
                                     interval_data,
                                     non_standard_features_data[post_idx])
    }
    features_data = cbind(features_data, non_standard_features_data)
  }
  rownames(features_data) = features_data$feature_id
  hdf5_fh$close_all()
  
  feature_types = unique(features_data$feature_type)
  counts_lst = purrr::map(feature_types, function(f) {
    # Subset counts
    if (length(feature_types) > 1) {
      cts = BPCells::open_matrix_10x_hdf5(h5_file, feature_type=f)
    } else {
      cts = BPCells::open_matrix_10x_hdf5(h5_file)
    }
    
    # Strip barcode suffix
    if (!is.null(strip_suffix)) {
      colnames(cts) = trimws(colnames(cts), which="right", whitespace=strip_suffix)
    }
    
    # Add barcode metadata
    bc_meta = barcodes_data[colnames(cts), , drop=FALSE]
    attr(cts, "barcode_metadata") = bc_meta
    
    # Add feature metadata
    fc_meta = features_data[features_data$feature_type == f, , drop=FALSE]
    keep = purrr::map_lgl(colnames(fc_meta), function(n) {
      return(all(!is.na(fc_meta[, n, drop=TRUE]) & nchar(fc_meta[, n, drop=TRUE]) > 0))
    })
    attr(cts, "feature_metadata") = fc_meta[, keep, drop=FALSE]
    
    # Attach path
    attr(cts, "path") = h5_file
    
    return(cts)
  })
  names(counts_lst) = feature_types
  
  return(counts_lst)
}

#' Read 10x Counts (Non-Spatial Datasets)
#'
#' Reads count data produced by 10x Genomics for non-spatial single-cell
#' experiments. Automatically detects whether input is HDF5 or Market Exchange
#' format.
#'
#' @param path Character. Path to 10x counts data. Can be:
#'   \itemize{
#'     \item A 10x HDF5 file (recommended for large datasets)
#'     \item A 10x matrix exchange format directory
#'   }
#' @param assays Character vector or \code{NULL}. Which assays to read (e.g.,
#'   \code{c("RNA", "ADT")}). If \code{NULL}, reads all available assays.
#'   Default is \code{NULL}.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names (e.g., "-1"). Default is \code{NULL}.
#'
#' @return A named list with one matrix per assay. Format is either:
#'   \itemize{
#'     \item \code{IterableMatrix} when reading from HDF5
#'     \item \code{dgCMatrix} when reading from MTX directory
#'   }
#'   Additional metadata is attached as attributes:
#'   \itemize{
#'     \item \code{barcode_metadata} – data frame with barcode information
#'     \item \code{feature_metadata} – data frame with feature information
#'     \item \code{technology} – "10x"
#'     \item \code{assay} – the assay name
#'     \item \code{path} – the input path
#'   }
#'
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read all assays from an h5 file
#' counts_list <- ReadCounts_10x("filtered_feature_bc_matrix.h5")
#'
#' # Read only RNA and ADT assays
#' counts_list <- ReadCounts_10x(
#'   "filtered_feature_bc_matrix",
#'   assays = c("RNA", "ADT")
#' )
#' }
ReadCounts_10x = function(path, assays=NULL, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # Converts assay to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_10x), Assays_10x)
  feature_type_to_assay = unlist(Assays_10x)
  valid_assays = names(assay_to_feature_type)
  
  # If assays are specified, check that they are valid
  if (!is.null(assays)) {
    assertthat::assert_that(all(assays %in% valid_assays),
                            msg=FormatString("'{assays} must be: {valid_assays*}."))
  }

  # Read counts
  if (dir.exists(path)) {
    # 10x market exchange format
    counts_lst = ReadCounts_10x_mtx(mtx_directory=path, strip_suffix=strip_suffix)
  } else {
    # 10x h5 file
    counts_lst = ReadCounts_10x_h5(h5_file=path, strip_suffix=strip_suffix)
  }
  
  # 10x Xenium hack: the assay BlankCodeword can be feature type "Unassigned Codeword" (old) and "Blank Codeword" (new)
  if ("BlankCodeword" %in% assays) {
    if ("Unassigned Codeword" %in% names(counts_lst)) {
      d = assay_to_feature_type == "Blank Codeword"
    } else if ("Blank Codeword" %in% names(counts_lst)) {
      d = assay_to_feature_type == "Unassigned Codeword"
    }
    assay_to_feature_type = assay_to_feature_type[!d]
  }
  
  # Subset feature types (which correspond to assays)
  if (!is.null(assays)) {
    feature_types = assay_to_feature_type[assays]
    f = feature_types %in% names(counts_lst)
    assertthat::assert_that(all(f),
                            msg=FormatString("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
    counts_lst = counts_lst[feature_types]
  }
  
  # Change names from feature type to assay
  feature_types = names(counts_lst)
  f = feature_types %in% names(feature_type_to_assay)
  assertthat::assert_that(all(f),
                          msg=FormatString("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_10x in functions_io.R."))
  names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "10x"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Read 10x Visium Counts
#'
#' Reads count data produced by the 10x Visium spatial transcriptomics platform.
#' This is a wrapper around \code{\link{ReadCounts_10x}} that sets the technology
#' attribute to "10x_visium".
#'
#' @param path Character. Path to 10x Visium counts data. Can be:
#'   \itemize{
#'     \item A 10x HDF5 file (recommended for large datasets)
#'     \item A 10x matrix exchange format directory
#'   }
#' @param assays Character vector or \code{NULL}. Which assays to read.
#'   If \code{NULL}, reads all assays. Default is \code{NULL}.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A named list with one matrix per assay. See \code{\link{ReadCounts_10x}}
#'   for details on the return format. The \code{technology} attribute is set to
#'   "10x_visium".
#'
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' # Read Visium counts
#' counts_list <- ReadCounts_10xVisium("filtered_feature_bc_matrix.h5")
#' }
ReadCounts_10xVisium = function(path, assays=NULL, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # Read counts
  counts_lst = ReadCounts_10x(path, assays=assays, strip_suffix=strip_suffix)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_visium"
  }
  
  return(counts_lst)
}

#' Read 10x VisiumHD Counts
#'
#' Reads count data produced by the 10x VisiumHD high-definition spatial
#' transcriptomics platform. Can read data from multiple bin sizes.
#'
#' @param path Character. Path to the \code{binned_outputs} directory produced
#'   by VisiumHD. Bin sizes specified by \code{bin_sizes} will be read.
#' @param assays Character vector or \code{NULL}. Which assays to read.
#'   If \code{NULL}, reads all assays. Default is \code{NULL}.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#' @param bin_sizes Integer vector or \code{NULL}. Bin sizes to read (in micrometers).
#'   If \code{NULL}, reads all available bin sizes. Default is \code{c(8, 16)}.
#'
#' @return A named list with one matrix per assay and bin size. When multiple
#'   bin sizes are read, assay names include the bin size suffix (e.g., "RNA008",
#'   "RNA016"). The \code{technology} attribute is set to "10x_visiumhd".
#'
#' @importFrom assertthat is.readable
#' @importFrom purrr map flatten
#' @importFrom magrittr %>%
#' @export
#'
#' @examples
#' \dontrun{
#' # Read VisiumHD counts at 8um and 16um resolution
#' counts_list <- ReadCounts_10xVisiumHD(
#'   "binned_outputs",
#'   bin_sizes = c(8, 16)
#' )
#'
#' # Read all available bin sizes
#' counts_list <- ReadCounts_10xVisiumHD("binned_outputs", bin_sizes = NULL)
#' }
ReadCounts_10xVisiumHD = function(path, assays=NULL, strip_suffix=NULL, bin_sizes=c(8, 16)) {
  # Checks
  assertthat::is.readable(path)
  
  # If set to NULL, collect all available bin sizes
  if (is.null(bin_sizes)) {
    subdirs = list.dirs(path, full.names=FALSE, recursive=FALSE)
    subdirs = subdirs[grepl("square_\\d+um", subdirs)]
    bin_sizes = gsub("square_(\\d+)um", "\\1", subdirs) %>% as.integer()
  }
  
  # Expand paths for bin sizes and check whether they exist
  bin_sizes = sprintf("%03d", bin_sizes)
  paths = file.path(path, paste0("square_", bin_sizes, "um"), "filtered_feature_bc_matrix.h5")
  for(p in paths) {
    assertthat::is.readable(p)
  }

  # Read counts for all bin sizes; the result is a list of bin sizes where each entry is a list of assays read for each bin size
  counts_lst = purrr::map(paths, ReadCounts_10x, assays=assays, strip_suffix=strip_suffix)
  
  # For multiple bin sizes, rename the assays accordingly
  if (length(bin_sizes) > 1) {
    for (i in seq_along(counts_lst)) {
      names(counts_lst[[i]]) = paste0(names(counts_lst[[i]]), bin_sizes[i])
      for (j in seq_along(counts_lst[[i]])) {
        attr(counts_lst[[i]][[j]], "assay") = paste0(attr(counts_lst[[i]][[j]], "assay"), bin_sizes[i])
      }
    }
  }
  
  # Flatten the counts list
  counts_lst = purrr::flatten(counts_lst)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_visiumhd"
  }
  
  return(counts_lst)
}

#' Read 10x Xenium Counts
#'
#' Reads count data produced by the 10x Xenium in situ transcriptomics platform.
#' This is a wrapper around \code{\link{ReadCounts_10x}} that sets the technology
#' attribute to "10x_xenium".
#'
#' @param path Character. Path to 10x Xenium counts data. Can be:
#'   \itemize{
#'     \item A 10x HDF5 file (recommended for large datasets)
#'     \item A 10x matrix exchange format directory
#'   }
#' @param assays Character vector or \code{NULL}. Which assays to read.
#'   If \code{NULL}, reads all assays. Default is \code{NULL}.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A named list with one matrix per assay. See \code{\link{ReadCounts_10x}}
#'   for details on the return format. The \code{technology} attribute is set to
#'   "10x_xenium".
#'
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' # Read Xenium counts
#' counts_list <- ReadCounts_10xXenium("cell_feature_matrix.h5")
#' }
ReadCounts_10xXenium = function(path, assays=NULL, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)

  # Read counts
  counts_lst = ReadCounts_10x(path, assays=assays, strip_suffix=strip_suffix)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_xenium"
  }
  
  return(counts_lst)
}

#' Read Parse Biosciences Counts from MTX Format
#'
#' Reads count data from a Parse Biosciences directory in Market Exchange format.
#' Typically contains \code{count_matrix.mtx}, \code{cell_metadata.csv}, and
#' \code{all_genes.csv}.
#'
#' @param mtx_directory Character. Path to Parse Biosciences counts directory.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A named list with one sparse matrix (\code{dgCMatrix}) per feature type.
#'   Additional metadata is attached as attributes:
#'   \itemize{
#'     \item \code{barcode_metadata} – data frame with barcode information
#'     \item \code{feature_metadata} – data frame with feature information
#'     \item \code{path} – the input directory path
#'   }
#'
#' @export
#'
#' @examples
#' \dontrun{
#' counts_list <- ReadCounts_ParseBio_mtx("all-sample/DGE_filtered")
#' }
ReadCounts_ParseBio_mtx = function(mtx_directory, strip_suffix=NULL) {
  # Determine the name of the matrix file
  mtx_file_name = "count_matrix.mtx"
  
  # Determine the name of the barcodes file
  barcodes_file_name = "cell_metadata.csv"
  
  # Determine the name of the features file
  features_file_name = "all_genes.csv"
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    transpose=TRUE,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=TRUE,
    features_file_name=features_file_name,
    features_column_names=TRUE,
    feature_type_column=NULL,
    delim = ",",
    strip_suffix=strip_suffix
  )
  
  return(counts_lst)
}

#' Read Parse Biosciences Counts from H5AD Format
#'
#' Reads count data from a Parse Biosciences AnnData HDF5 file. This function
#' returns an iterator object that reads data on-demand from the file.
#'
#' @param h5ad_file Character. Path to an AnnData object in HDF5 format.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A list containing one \code{IterableMatrix} (BPCells format).
#'   See \code{\link{ReadCounts_h5ad}} for details.
#'
#' @details
#' This function does not read the entire counts data into memory. It is
#' recommended to convert the result to a BPCells on-disk storage format
#' for better performance.
#'
#' @export
#'
#' @examples
#' \dontrun{
#' counts_list <- ReadCounts_ParseBio_h5ad("data.h5ad")
#' }
ReadCounts_ParseBio_h5ad = function(h5ad_file, strip_suffix=NULL) {
  return(ReadCounts_h5ad(h5ad_file, strip_suffix=strip_suffix))
}

#' Read Parse Biosciences Counts
#'
#' Reads count data produced by Parse Biosciences. Automatically detects whether
#' input is HDF5 or Market Exchange format.
#'
#' @param path Character. Path to Parse Biosciences counts data. Can be:
#'   \itemize{
#'     \item A Parse Biosciences anndata.h5ad file (recommended for large datasets)
#'     \item A Parse Biosciences matrix exchange format directory
#'   }
#' @param assays Character. The assay name to assign. Parse Biosciences currently
#'   does not support multi-assay datasets, so this simply sets the assay type.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A named list with one matrix per assay. Format is either:
#'   \itemize{
#'     \item \code{IterableMatrix} when reading from H5AD
#'     \item \code{dgCMatrix} when reading from MTX directory
#'   }
#'   The \code{technology} attribute is set to "Parse Biosciences".
#'
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read Parse Bio counts from a directory
#' counts_list <- ReadCounts_ParseBio(
#'   "all-sample/DGE_filtered",
#'   assays = "RNA"
#' )
#' }
ReadCounts_ParseBio = function(path, assays, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # If assays are specified, check that they are valid
  assay_to_feature_type = setNames(names(Assays_Parse), Assays_Parse)
  valid_assays = names(assay_to_feature_type)
  assertthat::assert_that(all(assays %in% valid_assays),
                          msg=FormatString("'{assay} must be: {valid_assays*}."))
  
  # Read counts
  if (dir.exists(path)) {
    # Parse Bio market exchange format
    counts_lst = ReadCounts_ParseBio_mtx(mtx_directory=path, strip_suffix=strip_suffix)
  } else {
    # Parse Bio anndata h5 file
    counts_lst = ReadCounts_ParseBio_h5ad(h5ad_file=path, strip_suffix=strip_suffix)
  }
  
  # No multi-assay datasets but keep for now
  #
  # Subset feature types (which correspond to assays)
  #if (!is.null(assays)) {
  #  feature_types = assay_to_feature_type[assays]
  #  f = feature_types %in% names(counts_lst)
  #  assertthat::assert_that(all(f),
  #                          msg=FormatString("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
  #  counts_lst = counts_lst[feature_types]
  #}
  
  # Change names from feature type to assay
  #feature_types = names(counts_lst)
  #f = feature_types %in% names(feature_type_to_assay)
  #assertthat::assert_that(all(f),
  #                        msg=FormatString("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_Parse in functions_io.R."))
  #names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  counts_lst = counts_lst[1]
  names(counts_lst) = assays[1]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "Parse Biosciences"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Read Scale Bio Counts from MTX Format
#'
#' Reads count data from a Scale Bio directory in Market Exchange format.
#' Typically contains \code{matrix.mtx}, \code{barcodes.tsv}, and
#' \code{features.tsv}.
#'
#' @param mtx_directory Character. Path to Scale Bio counts directory.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A named list with one sparse matrix (\code{dgCMatrix}) per feature type.
#'   Additional metadata is attached as attributes.
#'
#' @export
#'
#' @examples
#' \dontrun{
#' counts_list <- ReadCounts_ScaleBio_mtx("filtered_matrix")
#' }
ReadCounts_ScaleBio_mtx = function(mtx_directory, strip_suffix=NULL) {
  # Determine the name of the matrix file
  mtx_file_name = "matrix.mtx"
  
  # Determine the name of the barcodes file
  barcodes_file_name = "barcodes.tsv"
  
  # Determine the name of the features file
  features_file_name = "features.tsv"
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    transpose=FALSE,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=FALSE,
    features_file_name=features_file_name,
    features_column_names=c("feature_id", "feature_name", "feature_type"),
    feature_type_column=3,
    delim = "\t",
    strip_suffix=strip_suffix
  )
  
  return(counts_lst)
}

#' Read Scale Bio Counts
#'
#' Reads count data produced by Scale Bio. Currently only supports Market
#' Exchange format directories.
#'
#' @param path Character. Path to Scale Bio counts data. Must be a Scale Bio
#'   matrix exchange format directory.
#' @param assays Character vector or \code{NULL}. Which assays to read.
#'   If \code{NULL}, reads all assays. Default is \code{NULL}.
#' @param strip_suffix Character or \code{NULL}. String to remove from barcode
#'   names. Default is \code{NULL}.
#'
#' @return A named list with one sparse matrix (\code{dgCMatrix}) per assay.
#'   The \code{technology} attribute is set to "ScaleBio".
#'
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read Scale Bio counts
#' counts_list <- ReadCounts_ScaleBio("filtered_matrix", assays = "RNA")
#' }
ReadCounts_ScaleBio = function(path, assays, strip_suffix=NULL) {
  # Checks
  assertthat::is.readable(path)
  
  # Converts assay to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_Scale), Assays_Scale)
  feature_type_to_assay = unlist(Assays_Scale)
  valid_assays = names(assay_to_feature_type)
  
  # If assays are specified, check that they are valid
  if (!is.null(assays)) {
    assertthat::assert_that(all(assays %in% valid_assays),
                            msg=FormatString("'{assays} must be: {valid_assays*}."))
  }

  # Read counts
  counts_lst = ReadCounts_ScaleBio_mtx(mtx_directory=path, strip_suffix=strip_suffix)
  
  # Subset feature types (which correspond to assays)
  if (!is.null(assays)) {
    feature_types = assay_to_feature_type[assays]
    f = feature_types %in% names(counts_lst)
    assertthat::assert_that(all(f),
                            msg=FormatString("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
    counts_lst = counts_lst[feature_types]
  }
  
  # Change names from feature type to assay
  feature_types = names(counts_lst)
  f = feature_types %in% names(feature_type_to_assay)
  assertthat::assert_that(all(f),
                          msg=FormatString("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_Scale in functions_io.R."))
  names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "ScaleBio"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Read Counts from Multiple Technologies
#'
#' A unified function to read count data from various single-cell technologies
#' including SmartSeq, 10x Genomics, Parse Biosciences, and Scale Bio.
#' Automatically selects the appropriate reader based on the technology parameter.
#'
#' @param path Character. Path to counts data. Format depends on technology:
#'   \itemize{
#'     \item SmartSeq: character-separated file or MTX directory
#'     \item 10x: HDF5 file or MTX directory
#'     \item 10x VisiumHD: \code{binned_outputs} directory or specific bin path
#'     \item Parse: H5AD file or MTX directory
#'     \item Scale: MTX directory
#'   }
#' @param technology Character. Technology used to generate the data. Options:
#'   \code{"smartseq2"}, \code{"smartseq3"}, \code{"10x"}, \code{"10x_visium"},
#'   \code{"10x_visiumhd"}, \code{"10x_xenium"}, \code{"parse"}, \code{"scale"}.
#' @param assays Character vector. Which assays to read. For single-assay
#'   technologies, this sets the assay name.
#' @param barcode_metadata Data frame or list or \code{NULL}. Additional barcode
#'   metadata to merge. If a list, specifies metadata for each assay.
#'   Default is \code{NULL}.
#' @param feature_metadata Data frame or list or \code{NULL}. Additional feature
#'   metadata to merge. If a list, specifies metadata for each assay.
#'   Default is \code{NULL}.
#' @param barcode_suffix Character or \code{NULL}. Suffix to add to barcode names.
#'   Default is \code{NULL}.
#' @param visiumhd_bin_sizes Integer vector. For 10x VisiumHD, which bin sizes
#'   to read. Default is \code{c(8, 16)}.
#'
#' @return A named list with one matrix per assay. Format can be:
#'   \itemize{
#'     \item \code{dgCMatrix} – when reading from MTX or CSV files
#'     \item \code{IterableMatrix} – when reading from HDF5/H5AD files
#'   }
#'   For 10x VisiumHD, different bin sizes are returned as separate assays.
#'
#' @importFrom magrittr %>%
#' @importFrom assertthat assert_that not_empty
#' @importFrom purrr map flatten_chr
#' @importFrom dplyr left_join
#' @export
#'
#' @examples
#' \dontrun{
#' # Read 10x data
#' counts <- ReadCounts(
#'   path = "filtered_feature_bc_matrix.h5",
#'   technology = "10x",
#'   assays = "RNA"
#' )
#'
#' # Read Parse Bio data with additional metadata
#' metadata <- data.frame(barcode = c("cell1", "cell2"), condition = c("A", "B"))
#' counts <- ReadCounts(
#'   path = "DGE_filtered",
#'   technology = "parse",
#'   assays = "RNA",
#'   barcode_metadata = metadata
#' )
#' }
ReadCounts = function(path, technology, assays, barcode_metadata=NULL, feature_metadata=NULL, barcode_suffix=NULL, visiumhd_bin_sizes=c(8, 16)) {
  library(magrittr)

  # Checks
  valid_technologies = c("smartseq2", "smartseq3", "10x", "10x_visium", "10x_visiumhd", "10x_xenium", "parse", "scale")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatString("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read counts
  strip_suffix = NULL
  if (technology == "smartseq2") {
    counts_lst = ReadCounts_SmartSeq(path=path, assays=assays[1], version="2")
  } else if (technology == "smartseq3") {
    counts_lst = ReadCounts_SmartSeq(path=path, assays=assays[1], version="3")
  } else if(technology == "10x") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10x(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "10x_visium") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10xVisium(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "10x_visiumhd") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10xVisiumHD(path=path, assays=assays, strip_suffix=strip_suffix, bin_sizes=visiumhd_bin_sizes)
  } else if(technology == "10x_xenium") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_10xXenium(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "parse") {
    strip_suffix = NULL
    counts_lst = ReadCounts_ParseBio(path=path, assays=assays, strip_suffix=strip_suffix)
  } else if(technology == "scale") {
    if(!is.null(barcode_suffix)) strip_suffix = "-1"
    counts_lst = ReadCounts_ScaleBio(path=path, assays=assays, strip_suffix=strip_suffix)
  }
  
  assertthat::assert_that(assertthat::not_empty(counts_lst),
                          msg=FormatString("Count not read counts for dataset {path}, assay {assay}."))
  
  # Add barcode metadata to counts objects
  if (!is.null(barcode_metadata)) {
    # Merge list of barcode metadata files
    if (inherits(barcode_metadata, "list")) {
      # Collect all barcodes: Get first column of each table
      barcodes = purrr::map(barcode_metadata, 1) %>% unlist() %>% unique()
      # Now merge all tables
      merged_barcode_metadata = data.frame(orig_barcode=barcodes)
      for(i in seq_along(barcode_metadata)) {
        x_id = colnames(merged_barcode_metadata)[1]
        y_id = colnames(barcode_metadata[[i]])[1]
        merged_barcode_metadata = dplyr::left_join(x=merged_barcode_metadata,
                                                   y=barcode_metadata[[i]],
                                                   by=setNames(y_id, x_id),
                                                   suffix=paste0(".", c(i-1, i)))
      }
      barcode_metadata = merged_barcode_metadata
    }
    
    for(i in seq_along(counts_lst)) {
      # Do we have already other barcode metadata
      if ("barcode_metadata" %in% names(attributes(counts_lst[[i]]))) {
        metadata = attr(counts_lst[[i]], "barcode_metadata")
      } else {
        metadata = data.frame(orig_barcode=colnames(counts_lst[[i]]))
        rownames(metadata) = metadata$orig_barcode
      }
      
      x_id = colnames(metadata)[1]
      x_rownames = rownames(metadata)
      y_id = colnames(barcode_metadata)[1]
      metadata = dplyr::left_join(x=metadata,
                                   y=barcode_metadata,
                                   by=setNames(y_id, x_id))
      rownames(metadata) = x_rownames
      attr(counts_lst[[i]], "barcode_metadata") = metadata
    }
  }

  # Add feature metadata to counts objects
  if (!is.null(feature_metadata)) {
    # Merge list of feature metadata files
    if (inherits(feature_metadata, "list")) {
      # Collect all barcodes: Get first column of each table
      features = purrr::map(feature_metadata, 1) %>% unlist() %>% unique()
      # Now merge all tables
      merged_feature_metadata = data.frame(feature_id=features)
      for(i in seq_along(feature_metadata)) {
        x_id = colnames(merged_feature_metadata)[1]
        y_id = colnames(feature_metadata[[i]])[1]
        merged_barcode_metadata = dplyr::left_join(x=merged_feature_metadata,
                                                   y=feature_metadata[[i]],
                                                   by=setNames(y_id, x_id),
                                                   suffix=paste0(".", c(i-1, i)))
      }
      feature_metadata = merged_feature_metadata
    }
    
    for(i in seq_along(counts_lst)) {
      # Do we have already other feature metadata
      if ("feature_metadata" %in% names(attributes(counts_lst[[i]]))) {
        metadata = attr(counts_lst[[i]], "feature_metadata")
      } else {
        metadata = data.frame(feature_id=rownames(counts_lst[[i]]))
        rownames(metadata) = metadata$feature_id
      }
      
      x_id = colnames(metadata)[1]
      x_rownames = rownames(metadata)
      y_id = colnames(feature_metadata)[1]
      metadata = dplyr::left_join(x=metadata,
                                  y=feature_metadata,
                                  by=setNames(y_id, x_id))
      rownames(metadata) = x_rownames
      attr(counts_lst[[i]], "feature_metadata") = metadata
    }
  }
  
  # Make feature names Seurat-compatible (replace '_' with '-') and unique
  for(i in seq_along(counts_lst)) {
    # Get attributes
    barcode_metadata = attr(counts_lst[[i]], "barcode_metadata")
    feature_metadata = attr(counts_lst[[i]], "feature_metadata")
    assay = attr(counts_lst[[i]], "assay")
    technology = attr(counts_lst[[i]], "technology")
    
    
    feature_names = rownames(counts_lst[[i]])
    if (any(grepl(pattern="_", x=feature_names, fixed=TRUE))) {
      warning(FormatString("Feature names contain '_' for dataset {path}, assay {assay}. All occurences will be replaced with '-'."))
      feature_names = gsub(pattern="_", replacement="-", x=feature_names, fixed=TRUE)
    }
    if (any(duplicated(feature_names))) {
      warning(FormatString("Features contains duplicate values for dataset {path}, assay {assay}. Feature names will be made unique."))
      feature_names = make.unique(feature_names)
    }
    
    rownames(counts_lst[[i]]) = feature_names
    rownames(feature_metadata) = feature_names
    
    # Restore attributes (when modifying the object they get erased)
    attr(counts_lst[[i]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]], "assay") = assay
    attr(counts_lst[[i]], "technology") = technology
  }

  # Add barcode suffix
  if (!is.null(barcode_suffix)) {
    for(i in seq_along(counts_lst)) {
      # Get attributes
      barcode_metadata = attr(counts_lst[[i]], "barcode_metadata")
      feature_metadata = attr(counts_lst[[i]], "feature_metadata")
      assay = attr(counts_lst[[i]], "assay")
      technology = attr(counts_lst[[i]], "technology")
      
      # Counts
      colnames(counts_lst[[i]]) = paste0(colnames(counts_lst[[i]]), barcode_suffix)
      
      # Metadata
      rownames(barcode_metadata) = paste0(rownames(barcode_metadata), barcode_suffix)
      
      # Restore attributes (when modifying the object they get erased)
      attr(counts_lst[[i]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]], "assay") = assay
      attr(counts_lst[[i]], "technology") = technology
    }
  }
  
  return(counts_lst)
}

#' Write Counts to BPCells Matrix Directory Format
#'
#' Writes count data to disk in BPCells matrix directory format for efficient
#' on-disk access. This format is optimized for large single-cell datasets.
#'
#' @param counts A counts matrix. Supported formats:
#'   \itemize{
#'     \item Standard matrix
#'     \item Sparse matrix (\code{dgCMatrix})
#'     \item \code{IterableMatrix} (from BPCells)
#'   }
#' @param path Character. Directory path where the matrix will be saved.
#' @param overwrite Logical. If \code{TRUE}, overwrite existing files.
#'   Default is \code{FALSE}.
#'
#' @return An \code{IterableMatrix} pointing to the saved BPCells matrix directory.
#'
#' @details
#' If the matrix contains only non-negative integers, it is automatically
#' converted to unsigned 32-bit integers to save disk space.
#'
#' If \code{overwrite = FALSE} and the path already exists, the function
#' simply opens and returns the existing matrix.
#'
#' @importFrom BPCells write_matrix_dir open_matrix_dir convert_matrix_type
#' @export
#'
#' @examples
#' \dontrun{
#' # Write counts to disk
#' counts_disk <- WriteCounts_MatrixDir(counts, "output/counts_matrix")
#' }
WriteCounts_MatrixDir = function(counts, path, overwrite=FALSE) {
  library(BPCells)
  
  # If path exists and overwrite is FALSE, just open the matrix directory
  if (dir.exists(path) & overwrite==FALSE) {
    counts = BPCells::open_matrix_dir(path)
    return(counts)
  }
  
  # Check that the counts object has the correct format
  if (is(counts, "IterableMatrix")) {
    # Produced by BPCells::open_matrix_10x_hdf5 or BPCells::open_matrix_anndata_hdf5
    # Nothing to do
  } else if (is.matrix(counts)) {
    # Standard matrix - convert to sparse matrix
    counts = as(counts, "dgCMatrix")
  } else if(is(counts, "sparseMatrix")) {
    # Sparse matrix - convert to dgCMatrix
    if (!is(counts, "dgCMatrix")) {
      counts = as(counts, "dgCMatrix")
    }
  }else {
    # No matrix - try to convert to sparse matrix
    counts = as(as.matrix(counts), "dgCMatrix")
  }
  
  # Sparse matrices in dgCMatrix format must be converted to BPCells internal format for better efficiency
  if (is(counts, "dgCMatrix")) {
    counts = as(counts, "IterableMatrix")
  }
  
  # Test if we have non-negative integers, then convert matrix from double to integer to save disk space (default is double)
  vals = as(counts[1:min(1000, nrow(counts)), 1:min(1000, ncol(counts))], "dgCMatrix")
  if (all(vals >= 0) & all(vals == round(vals))) {
    counts = BPCells::convert_matrix_type(counts, type="uint32_t")
  }
  
  # Write to directory
  counts = BPCells::write_matrix_dir(mat=counts, dir=path, overwrite=overwrite)
  
  # Return IterableMatrix pointing to this directory
  return(counts)
}

#' Write Counts to Market Matrix Format
#'
#' Writes count data to disk in Market Matrix (MTX) format, which is a standard
#' format for sparse matrices used by many single-cell tools.
#'
#' @param counts A counts matrix. Supported formats:
#'   \itemize{
#'     \item Standard matrix
#'     \item Sparse matrix (\code{dgCMatrix})
#'     \item \code{IterableMatrix} (from BPCells)
#'   }
#' @param path Character. Directory path where the files will be saved.
#' @param overwrite Logical. If \code{TRUE}, overwrite existing files.
#'   Default is \code{FALSE}.
#' @param barcode_data Data frame or \code{NULL}. If provided, writes this data
#'   to \code{barcodes.tsv.gz} instead of column names from the matrix.
#'   Must have the same number of rows as matrix columns.
#' @param feature_data Data frame or \code{NULL}. If provided, writes this data
#'   to \code{features.tsv.gz} instead of row names from the matrix.
#'   Must have the same number of rows as matrix rows.
#'
#' @return Invisibly returns \code{NULL}. Creates a directory containing:
#'   \itemize{
#'     \item \code{matrix.mtx.gz} – the sparse matrix in MTX format
#'     \item \code{barcodes.tsv.gz} – barcode information
#'     \item \code{features.tsv.gz} – feature information
#'   }
#'
#' @importFrom Matrix writeMM
#' @importFrom R.utils gzip
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Write counts with default barcode and feature names
#' WriteCounts_MatrixMarket(counts, "output/mtx_format")
#'
#' # Write with custom metadata
#' WriteCounts_MatrixMarket(
#'   counts,
#'   "output/mtx_format",
#'   barcode_data = data.frame(barcode = colnames(counts), sample = "Sample1"),
#'   feature_data = data.frame(id = rownames(counts), name = rownames(counts))
#' )
#' }
WriteCounts_MatrixMarket = function(counts, path, overwrite=FALSE, barcode_data=NULL, feature_data=NULL) {
  if (!dir.exists(path) | overwrite==TRUE) {
    if (!is(counts, "dgCMatrix")) {
      counts = as(counts, "dgCMatrix")
    }
    
    # Create directory
    d = file.path(path)
    dir.create(d, showWarnings=FALSE)
    
    # Write matrix.mtx.gz
    mh = file.path(d, "matrix.mtx")
    Matrix::writeMM(counts, file=mh)
    R.utils::gzip(mh, overwrite=TRUE)
    
    # Write barcodes and barcode metadata
    if (is.null(barcode_data)) {
      barcode_data = data.frame(barcode=colnames(counts))
    } else {
      assertthat::assert_that(ncol(counts) == nrow(barcode_data),
                              msg=FormatString("The number of rows in barcode_data differs from the number of columns in the counts matrix."))
    }
    
    bh = gzfile(file.path(d, "barcodes.tsv.gz"), open="wb")
    if (ncol(barcode_data) > 1) {
      comment_line = paste(colnames(barcode_data), collapse = " ")
      comment_line = paste("#", comment_line)
      writeLines(comment_line, con=bh)
    }
    write.table(barcode_data, file=bh, sep="\t", col.names=FALSE, row.names=FALSE, quote=FALSE)
    close(bh)
    
    if (is.null(feature_data)) {
      feature_data = data.frame(id=rownames(counts), name=rownames(counts))
    } else{
      assertthat::assert_that(nrow(counts) == nrow(feature_data),
                              msg=FormatString("The number of rows in feature_data differs from the number of rows in the counts matrix."))
    }
    
    fh = gzfile(file.path(d, "features.tsv.gz"), open="wb")
    if (ncol(feature_data) > 1) {
      comment_line = paste(colnames(feature_data), collapse = " ")
      comment_line = paste("#", comment_line)
      writeLines(comment_line, con=fh)
    }
    write.table(feature_data, file=fh, sep="\t", col.names=FALSE, row.names=FALSE, quote=FALSE)
    close(fh)
  }
}

#' Read 10x Visium Image Data
#'
#' Reads spatial image data from a 10x Visium or VisiumHD \code{spatial} directory.
#' This includes the tissue image, spot coordinates, and scale factors.
#'
#' @param image_dir Character. Path to the \code{spatial} directory produced by
#'   10x Visium or VisiumHD.
#' @param barcodes Named character vector or \code{NULL}. If provided, keeps only
#'   specified barcodes, reorders them, and optionally renames them. Names are
#'   original barcodes, values are new barcode names. Default is \code{NULL}.
#'
#' @return A Seurat \code{VisiumV1} or \code{VisiumV2} image object.
#'
#' @details
#' The spatial directory must contain:
#' \itemize{
#'   \item \code{tissue_lowres_image.png} – low resolution tissue image
#'   \item \code{scalefactors_json.json} – scale factors for coordinates
#' }
#'
#' @importFrom Seurat Read10X_Image
#' @importFrom SeuratObject Cells RenameCells
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read spatial image data
#' image <- ReadImage_10xVisium("spatial")
#'
#' # Read and rename barcodes
#' barcodes <- setNames(paste0("Sample1_", c("A", "B", "C")), c("barcode1", "barcode2", "barcode3"))
#' image <- ReadImage_10xVisium("spatial", barcodes = barcodes)
#' }
ReadImage_10xVisium = function(image_dir, barcodes=NULL) {
  # Checks
  assertthat::is.readable(image_dir)
  for (f in c("tissue_lowres_image.png", "scalefactors_json.json")) {
    assertthat::assert_that(file.exists(file.path(image_dir, f)),
                            msg=FormatString("10x Visium image directory {image_dir} misses the file {f}."))
  }
  
  # Read image
  image = Seurat::Read10X_Image(image_dir, filter.matrix=TRUE)
  
  if (!is.null(barcodes)) {
    image_bcs = SeuratObject::Cells(image)
    
    # Keep only barcodes that are requested
    i = which(image_bcs %in% names(barcodes))
    image_bcs = image_bcs[i]
    
    # Re-order and subset
    i = match(names(barcodes), image_bcs)
    i = i[!is.na(i)]
    image_bcs = image_bcs[i]
    image = image[image_bcs]
    
    # Re-name
    image_bcs = SeuratObject::Cells(image)
    new_image_bcs = unname(barcodes[SeuratObject::Cells(image)])
    image = SeuratObject::RenameCells(image, new.names=new_image_bcs)
  }
  
  return(image)
}

#' Create Segmentation Object (Improved)
#'
#' An improved version of \code{SeuratObject::CreateSegmentation} that handles
#' large datasets more efficiently.
#'
#' @param coords A data frame with three columns: \code{cell}, \code{x}, \code{y}.
#'   Contains polygon vertices for cell segmentation boundaries.
#'
#' @return A Seurat \code{Segmentation} object.
#'
#' @details
#' This function converts polygon coordinates to Seurat Segmentation format.
#' Further improvements might be possible through parallelization.
#'
#' @importFrom sp Polygons Polygon SpatialPolygons
#' @importFrom SeuratObject CreateSegmentation
#' @importFrom purrr map
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' coords <- data.frame(
#'   cell = c("cell1", "cell1", "cell1", "cell2", "cell2", "cell2"),
#'   x = c(0, 1, 0.5, 2, 3, 2.5),
#'   y = c(0, 0, 1, 0, 0, 1)
#' )
#' segmentation <- CreateSegmentationImproved(coords)
#' }
CreateSegmentationImproved = function(coords) {
  library(sp)
  library(SeuratObject)
  
  assertthat::assert_that(all(colnames(coords) == c("cell", "x", "y")),
                          msg="Function 'CreateSegmentationImproved' requires a table with columns 'cell', 'x' and 'y'.")
  
  coords_cell_names = coords[[1]]
  coords_cell_names = factor(coords_cell_names, levels=unique(coords_cell_names))
  
  coords = as.matrix(coords[, 2:3])
  coords = split(x=coords, f=coords_cell_names)
  coords = purrr::map(coords, .f=matrix, ncol=2, dimnames=list(NULL, c("x", "y")))
  
  polygons_names = names(coords)
  polygons = purrr::map(seq_along(coords), function(i) {
    return(Polygons(
      srl=list(Polygon(coords=coords[[i]])),
      ID=polygons_names[i])
    )
  })
  polygons = SpatialPolygons(Srl=polygons)
  polygons = as(polygons, "Segmentation")
  CheckGC()
  return(polygons)
}

#' Read 10x Xenium Image Data
#'
#' Reads spatial coordinate data from a 10x Xenium directory. Note that Xenium
#' does not use traditional images but rather cell coordinates (centroids and/or
#' segmentation boundaries) and molecule locations.
#'
#' @param image_dir Character. Path to a 10x Xenium data directory.
#' @param barcodes Named character vector or \code{NULL}. If provided, keeps only
#'   specified barcodes, reorders them, and optionally renames them.
#'   Default is \code{NULL}.
#' @param coordinate_type Character vector. Which coordinate types to load.
#'   Options are \code{"centroids"} (cell center points) and/or
#'   \code{"segmentation"} (cell boundaries). Default is both.
#'
#' @return A Seurat \code{FOV} (Field of View) object containing:
#'   \itemize{
#'     \item Cell centroids and/or segmentation boundaries
#'     \item Molecule locations (transcript coordinates)
#'     \item Cell area and nucleus area as barcode metadata attributes
#'   }
#'
#' @details
#' The Xenium directory must contain:
#' \itemize{
#'   \item \code{cells.parquet} – cell centroid coordinates and metadata
#'   \item \code{cell_boundaries.parquet} – cell segmentation polygons
#'   \item \code{transcripts.parquet} – molecule/transcript locations
#' }
#'
#' @importFrom arrow read_parquet open_dataset
#' @importFrom SeuratObject CreateCentroids CreateMolecules CreateFOV DefaultBoundary
#' @importFrom dplyr filter select
#' @importFrom tidyr replace_na
#' @importFrom assertthat is.readable assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read Xenium data with centroids and segmentation
#' image <- ReadImage_10xXenium("xenium_output")
#'
#' # Read only centroids
#' image <- ReadImage_10xXenium("xenium_output", coordinate_type = "centroids")
#' }
ReadImage_10xXenium = function(image_dir, barcodes=NULL, coordinate_type=c("centroids", "segmentation")) {
  # Checks
  assertthat::is.readable(image_dir)
  assertthat::assert_that(file.exists(file.path(image_dir, "cells.parquet")),
                          msg=FormatString("10x Xenium image directory {image_dir} misses the file 'cells.parquet'."))
  assertthat::assert_that(file.exists(file.path(image_dir, "cell_boundaries.parquet")),
                          msg=FormatString("10x Xenium image directory {image_dir} misses the file 'cell_boundaries.parquet'."))
  assertthat::assert_that(file.exists(file.path(image_dir, "transcripts.parquet")),
                          msg=FormatString("10x Xenium image directory {image_dir} misses the file 'transcripts.parquet'."))
  
  mols.qv.threshold = 20
  options(stringsAsFactors=FALSE)
  coords = list()
  
  # Read cell centroids and cell area
  cell_centroids = arrow::read_parquet(file.path(image_dir, "cells.parquet"),
                                       col_select=c("cell_id", "x_centroid", "y_centroid", "cell_area", "nucleus_area"),
                                       as_data_frame=TRUE)
  
  cell_centroids = arrow::open_dataset(file.path(image_dir, "cells.parquet"),
                                        schema=arrow::schema(cell_id=arrow::string(), 
                                                             x_centroid=arrow::float(), 
                                                             y_centroid=arrow::float(),
                                                             cell_area=arrow::float(),
                                                             nucleus_area=arrow::float()))
  cell_centroids = as.data.frame(cell_centroids)
  names(cell_centroids) = c("cell", "x", "y", "cell_area", "nucleus_area")
  
  if (!is.null(barcodes)) {
    # Keep only barcodes that are requested
    i = which(cell_centroids$cell %in% names(barcodes))
    cell_centroids = cell_centroids[i, ]
    
    # Reorder barcodes
    i = match(cell_centroids$cell, names(barcodes))
    cell_centroids = cell_centroids[i, ]
    
    assertthat::assert_that(all(cell_centroids$cell == names(barcodes)),
                            msg="Barcodes for cell centroids do not match the requested barcodes.")
    
    # Rename
    cell_centroids$cell = barcodes
  }
  
  if ("centroids" %in% coordinate_type) {
    coords = c(coords, centroids = SeuratObject::CreateCentroids(cell_centroids[, c("cell", "x", "y")]))
  }
  
  # Read segmentations (area of cells)
  if ("segmentation" %in% coordinate_type) {
    cell_boundaries = arrow::open_dataset(file.path(image_dir, "cell_boundaries.parquet"),
                                          schema=arrow::schema(cell_id=arrow::string(), 
                                                               vertex_x=arrow::float(), 
                                                               vertex_y=arrow::float()))
    cell_boundaries = as.data.frame(cell_boundaries)
    names(cell_boundaries) = c("cell", "x", "y")
    
    if (!is.null(barcodes)) {
      # Keep only barcodes that are requested
      i = which(cell_boundaries$cell %in% names(barcodes))
      cell_boundaries = cell_boundaries[i, ]
      
      # Reorder barcodes
      i = match(cell_boundaries$cell, names(barcodes))
      cell_boundaries = cell_boundaries[i, ]
      
      # Rename
      new = barcodes[cell_boundaries$cell]
      assertthat::assert_that(all(!is.na(new)),
                              msg="Barcodes for cell centroids do not match the requested barcodes.")
      
      cell_boundaries$cell = new
    }
    coords = c(coords, segmentation = CreateSegmentationImproved(cell_boundaries))
  }
  
  # Load microns (molecule coordinates)
  transcripts = arrow::open_dataset(file.path(image_dir, "transcripts.parquet"),
                                    schema=arrow::schema(feature_name=arrow::string(), 
                                                         x_location=arrow::float(), 
                                                         y_location=arrow::float(), 
                                                         qv=arrow::float()))
  transcripts = transcripts %>% dplyr::filter(qv >= mols.qv.threshold) %>% dplyr::select(-qv)
  transcripts = as.data.frame(transcripts)
  colnames(transcripts) = c("gene", "x", "y")
  molecules = SeuratObject::CreateMolecules(transcripts, key='mols_')
  
  # Create FOV (coordinates plus transcript info)
  image = SeuratObject::CreateFOV(coords=coords,
                                  molecules=molecules,
                                  assay = 'Spatial',
                                  key = 'fov_')
  
  # Add information about cell_area and nucleus_area as barcode_metadata
  barcode_metadata = as.data.frame(cell_centroids[, c("cell", "cell_area", "nucleus_area")]) %>%
    tidyr::replace_na(list(cell_area=0, nucleus_area=0))
  rownames(barcode_metadata) = as.character(barcode_metadata$cell)
  barcode_metadata$cell = NULL
  attr(image, "barcode_metadata") = barcode_metadata
  
  # Set default boundary
  SeuratObject::DefaultBoundary(image) = coordinate_type[1]
  
  return(image)
}

#' Read Spatial Image Data
#'
#' A unified function to read spatial image data from various 10x platforms
#' including Visium, VisiumHD, and Xenium. Automatically selects the appropriate
#' reader based on the technology parameter.
#'
#' @param image_dir Character. Path to the spatial data directory.
#' @param technology Character. Technology used. Options: \code{"10x_visium"},
#'   \code{"10x_visiumhd"}, \code{"10x_xenium"}.
#' @param assay Character. Default assay to associate with this image.
#' @param barcodes Named character vector. Barcodes to keep, reorder, and rename.
#'   Names are original barcodes, values are new names.
#' @param coordinate_type Character vector. For 10x Xenium only: which coordinate
#'   types to load (\code{"centroids"} and/or \code{"segmentations"}).
#'
#' @return A Seurat image object:
#'   \itemize{
#'     \item \code{VisiumV1/V2} for Visium/VisiumHD data
#'     \item \code{FOV} for Xenium data
#'   }
#'
#' @importFrom magrittr %>%
#' @importFrom assertthat assert_that
#' @importFrom Seurat DefaultAssay
#' @export
#'
#' @examples
#' \dontrun{
#' # Read Visium spatial data
#' image <- ReadImage("spatial", technology = "10x_visium", assay = "Spatial")
#'
#' # Read Xenium spatial data
#' image <- ReadImage("xenium_output", technology = "10x_xenium", assay = "Xenium")
#' }
ReadImage = function(image_dir, technology, assay, barcodes, coordinate_type=c("centroids", "segmentations")) {
  library(magrittr)
  
  # Checks
  valid_technologies = c("10x_visium", "10x_visiumhd", "10x_xenium")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatString("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read image
  if(technology %in% c("10x_visium", "10x_visiumhd")) {
    # Visium
    image = ReadImage_10xVisium(image_dir=image_dir, barcodes=barcodes)
  } else if(technology == "10x_xenium") {
    # Xenium
    image = ReadImage_10xXenium(image_dir=image_dir, barcodes=barcodes, coordinate_type=coordinate_type)
  }
  
  # Set default assay for image
  Seurat::DefaultAssay(image) = assay
  
  return(image)
}

#' Read SmartSeq Metrics File
#'
#' Reads summary metrics from a SmartSeq metrics file. Since there is no standard
#' format for SmartSeq metrics, this function simply reads a character-separated
#' table.
#'
#' @param metrics_file Character. Path to a character-separated metrics file.
#'   First column must be the cell name; other columns can contain any metrics.
#'
#' @return A data frame containing the metrics.
#'
#' @importFrom readr read_delim
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' metrics <- ReadMetrics_Smartseq("metrics.tsv")
#' }
ReadMetrics_Smartseq = function(metrics_file) {
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)
  
  return(metrics_table)
}

#' Read 10x Metrics File
#'
#' Reads summary metrics from a 10x Genomics metrics file produced by Cell Ranger,
#' Space Ranger, or Xenium Ranger pipelines.
#'
#' @param metrics_file Character. Path to a \code{metrics_summary.csv} file
#'   produced by 10x pipelines.
#'
#' @return A named list of data frames. For Cell Ranger multi, returns one
#'   data frame per library type. For other pipelines, returns a single
#'   "Overall" data frame.
#'
#' @importFrom readr read_delim
#' @importFrom magrittr %>%
#' @importFrom dplyr filter select
#' @importFrom tidyr pivot_wider
#' @importFrom purrr map
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' # Read metrics from Cell Ranger count
#' metrics <- ReadMetrics_10x("metrics_summary.csv")
#'
#' # Read metrics from Cell Ranger multi (returns per-library metrics)
#' metrics <- ReadMetrics_10x("per_sample_outs/Sample1/metrics_summary.csv")
#' }
ReadMetrics_10x = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/konstantint/bfx2278/cellranger_multi/Pwl3_CMO311_A1_CMO312_alt/outs/per_sample_outs/A1/metrics_summary.csv"
  #metrics_file = "/projects/seq-work/analysis/yuliiah/bfx2322/cellranger_arc/m194T/outs/summary.csv"
  library(magrittr)
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)
  metrics_table_colnms = colnames(metrics_table)
  metrics_table = as.data.frame(metrics_table)
  
  if (metrics_table_colnms[1] == "Category") {
    # Produced by cellranger multi
    # https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/metrics-summary-csv
    
    # Subset per-sample metrics
    # Also only keep overall metrics but not the ones calculated for various groups
    metrics_table = metrics_table %>% 
      dplyr::filter(Category=="Cells", is.na(`Grouped By`)) %>%
      dplyr::select(library_type=`Library Type`, Metric=`Metric Name`, Value=`Metric Value`)
    
    # Split by library type and convert into wide table
    metrics_table = split(metrics_table, metrics_table$library_type)
    metrics_table = purrr::map(metrics_table, function(tbl) {
      tbl = tbl %>% dplyr::select(-library_type) %>% tidyr::pivot_wider(names_from="Metric", values_from="Value") %>% as.data.frame()
      return(tbl)
    })
  } else {
    # Produced by other cellranger pipelines (standard, multiome, atac, visium, xenium)
    metrics_table = list(Overall=metrics_table)
  }
  
  return(metrics_table)
}

#' Read Parse Biosciences Metrics File
#'
#' Reads summary metrics from a Parse Biosciences \code{analysis_summary.csv}
#' file produced by the splitpipe pipeline.
#'
#' @param metrics_file Character. Path to an \code{analysis_summary.csv} file.
#'
#' @return A list containing one data frame with summary metrics.
#'
#' @importFrom readr read_delim
#' @importFrom tidyr pivot_wider
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' metrics <- ReadMetrics_ParseBio("analysis_summary.csv")
#' }
ReadMetrics_ParseBio = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/L132590_Sl_1_62500cells/BC001/report/analysis_summary.csv"
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=c("Metric", "Value"), col_select=1:2, show_col_types=FALSE, skip=1)
  metrics_table = metrics_table %>% 
    tidyr::pivot_wider(names_from=1, values_from=2) %>%
    as.data.frame()
  metrics_table = list(metrics_table)
  
  return(metrics_table)
}

#' Read Scale Bio Metrics File
#'
#' Reads summary metrics from a Scale Bio \code{reportStatistics.csv} file
#' produced by the ScaleRna pipeline.
#'
#' @param metrics_file Character. Path to a \code{reportStatistics.csv} file.
#'
#' @return A list containing one "Overall" data frame with summary metrics.
#'
#' @importFrom readr read_delim
#' @importFrom magrittr %>%
#' @importFrom dplyr filter select
#' @importFrom tidyr pivot_wider
#' @importFrom assertthat is.readable
#' @export
#'
#' @examples
#' \dontrun{
#' metrics <- ReadMetrics_ScaleBio("reportStatistics.csv")
#' }
ReadMetrics_ScaleBio = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/SCdev/bfx2279/scalerna/output/reports/csv/BC238.reportStatistics.csv"
  library(magrittr)
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)

  # Only keep sections "Reads" and "Cells"
  metrics_table = metrics_table %>% dplyr::filter(Sample %in% c("Reads", "Cells"))
  metrics_table = metrics_table %>% 
    dplyr::select(-Sample) %>%
    tidyr::pivot_wider(names_from=1, values_from=2) %>%
    as.data.frame()
  
  metrics_table = list(Overall=metrics_table)
  return(metrics_table)
}

#' Read Metrics File from Multiple Technologies
#'
#' A unified function to read summary metrics files from various single-cell
#' technologies. Automatically selects the appropriate reader based on the
#' technology parameter.
#'
#' @param metrics_file Character. Path to a metrics file.
#' @param technology Character. Technology used. Options: \code{"smartseq2"},
#'   \code{"smartseq3"}, \code{"10x"}, \code{"10x_visium"}, \code{"10x_xenium"},
#'   \code{"parse"}, \code{"scale"}.
#'
#' @return One or more data frames with summary metrics. Format depends on
#'   the technology and pipeline used.
#'
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Read 10x metrics
#' metrics <- ReadMetrics("metrics_summary.csv", technology = "10x")
#'
#' # Read Parse Bio metrics
#' metrics <- ReadMetrics("analysis_summary.csv", technology = "parse")
#' }
ReadMetrics = function(metrics_file, technology) {
  #metrics_file = datasets$metrics_file[1]
  #technology = "10x"
  
  # Checks
  valid_technologies = c("smartseq2", "smartseq3", "10x", "10x_visium", "10x_xenium", "parse", "scale")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatString("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read metrics file
  if (technology %in% c("smartseq2", "smartseq3")) {
    metrics_table = ReadMetrics_SmartSeq(metrics_file=metrics_file)
  } else if(technology %in% c("10x", "10x_visium", "10x_xenium")) {
    metrics_table = ReadMetrics_10x(metrics_file=metrics_file)
  } else if(technology == "parse") {
    metrics_table = ReadMetrics_ParseBio(metrics_file=metrics_file)
  } else if(technology == "scale") {
    metrics_table = ReadMetrics_ScaleBio(metrics_file=metrics_file)
  }
  
  return(metrics_table)
}

#' Parse Plate Information from Cell Names
#'
#' Extracts plate layout information (plate number, row, column) from cell names.
#' Commonly used for SmartSeq datasets where this information is encoded in
#' cell identifiers.
#'
#' @param cell_names Character vector. Cell names containing plate information.
#' @param pattern Character. Regular expression with capture groups for plate
#'   number, row, and column. Default is \code{"_(\\d+)_([A-Z])(\\d+)$"} which
#'   matches patterns like "_1_A1" at the end of cell names.
#'
#' @return A data frame with columns:
#'   \itemize{
#'     \item \code{PlateNumber} – numeric plate identifier (NA if not matched)
#'     \item \code{PlateRow} – ordered factor (A-H for 96-well, A-P for 384-well)
#'     \item \code{PlateCol} – ordered factor (1-12 for 96-well, 1-24 for 384-well)
#'     \item \code{Rest} – the cell name with plate information removed
#'   }
#'
#' @details
#' The function automatically detects plate format (96-well, 384-well, or larger)
#' based on the row letters and column numbers found in the data.
#'
#' @importFrom magrittr %>%
#' @importFrom stringr str_match
#' @export
#'
#' @examples
#' \dontrun{
#' cell_names <- c("Sample1_1_A1", "Sample1_1_A2", "Sample1_1_B1")
#' plate_info <- ParsePlateInformation(cell_names)
#' }
ParsePlateInformation = function(cell_names, pattern='_(\\d+)_([A-Z])(\\d+)$') {
  library(magrittr)
  
  # Split cell name into plate information and rest
  rest = gsub(pattern=pattern, replacement="", x=cell_names)
  plate_information = as.data.frame(stringr::str_match(string=cell_names, pattern=pattern), stringsAsFactors=FALSE)
  plate_information[, 1] = NULL
  
  if (ncol(plate_information) == 2) {
    colnames(plate_information) = c("PlateRow", "PlateCol")
    plate_information$PlateNumber = NA 
  } else if (ncol(plate_information) == 3) {
    colnames(plate_information) = c("PlateNumber", "PlateRow", "PlateCol")
  }
  plate_information = plate_information[, c("PlateNumber", "PlateRow", "PlateCol")]
  plate_information$Rest = rest
  
  plate_information$PlateNumber = as.integer(plate_information$PlateNumber)
  plate_information$PlateRow = as.character(plate_information$PlateRow)
  plate_information$PlateCol = as.integer(plate_information$PlateCol)
  
  # Decide on plate layout
  if ("Q" %in% plate_information$PlateRow | max(c(-Inf,plate_information$PlateCol), na.rm=T) > 24) {
    # super plate?
    plate_information$PlateRow = factor(plate_information$PlateRow, 
                                        levels=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z"), 
                                        ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, levels=1:max(c(-Inf,plate_information$PlateCol), na.rm=T), ordered=TRUE)
  } else if ("I" %in% plate_information$PlateRow | max(c(-Inf,plate_information$PlateCol), na.rm=T) > 12) {
    # 384 plate
    plate_information$PlateRow = factor(plate_information$PlateRow, 
                                        levels=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P"), 
                                        ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, levels=1:24, ordered=TRUE)
  } else {
    plate_information$PlateRow = factor(plate_information$PlateRow, ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, ordered=TRUE)
  }
  
  return(plate_information)
}

#' Save Seurat Object with On-Disk Layers
#'
#' An improved version of \code{SeuratObject::SaveSeuratRds} that properly
#' handles on-disk matrix layers. Designed for use with large datasets stored
#' in BPCells format.
#'
#' @param sc A Seurat object.
#' @param outdir Character. Output directory for the saved Seurat object
#'   (\code{sc.rds}) and associated on-disk layers. Created if it doesn't exist.
#' @param write_disk_data Logical. If \code{TRUE}, copy existing on-disk layers
#'   to the output directory. Default is \code{TRUE}.
#' @param relative_paths Logical. If \code{TRUE}, store paths to on-disk layers
#'   as relative paths. Default is \code{FALSE}.
#' @param compress Logical. Whether to compress the RDS file. Default is \code{FALSE}.
#'
#' @return Invisibly returns \code{NULL}. Saves:
#'   \itemize{
#'     \item \code{sc.rds} – the Seurat object
#'     \item On-disk matrix directories (if present)
#'   }
#'
#' @details
#' Key improvements over the original function:
#' \itemize{
#'   \item Handles joined layers (e.g., \code{counts.sample1} + \code{counts.sample2}
#'     merged into \code{counts})
#'   \item Option to reuse existing on-disk directories instead of rewriting
#'   \item Better documentation
#' }
#'
#' @importFrom SeuratObject Layers LayerData .FilterObjects
#' @importFrom Seurat Misc
#' @importFrom progressr progressor
#' @importFrom purrr map_dfr
#' @importFrom fs path_rel
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Save Seurat object with on-disk matrices
#' SaveSeuratRdsWrapper(seurat_obj, "output/seurat_saved")
#' }
SaveSeuratRdsWrapper = function(sc, outdir, write_disk_data=TRUE, relative_paths=FALSE, compress=FALSE) {
  # If output directory does not exist, create it
  if (!dir.exists(outdir)) dir.create(outdir, recursive=TRUE)
  
  # Make sure output directory is empty
  files = list.files(outdir)
  assertthat::assert_that(length(files) == 0,
                          msg=FormatString("Target directory for Seurat object and associated matrix directories at {outdir} must be empty but is not. Please delete all files and directories in this directory."))
  
  # Name of seurat object
  file = file.path(outdir, "sc.rds")
  file = normalizePath(path=file, winslash="/", mustWork=FALSE)
  
  # Check that the fs package is installed for file moving (in case there are on-disk matrices)
  assertthat::assert_that(require("fs"), msg="The package 'fs' is required to move on-disk matrices. Please install it.")
  
  # Assays
  assays = SeuratObject::.FilterObjects(sc, classes.keep="StdAssay")
  
  # Progressor
  p = progressr::progressor(along=assays, auto_finish=TRUE)
  on.exit(expr=p(type="finish"), add=TRUE)
  p(message=FormatString("Looking for on-disk matrices in {length(assays)} assays", quote=FALSE), class="sticky", amount=0)
  
  # Set up table with on-disk matrix information
  cache = vector(mode="list", length=length(assays))
  names(cache) = assays
  destdir = dirname(file)
  
  # This table contains information about already existing on-disk directories
  save_seurat_rds = SeuratObject::Tool(sc, "SaveSeuratRds")
  
  # Loop over assays
  for (assay in assays) {
    p(message = FormatString("Searching through assay {assay}"), class="sticky", amount=0)
    
    # Loop over layers and collect information required for on-disk matrices
    layer_disk_info = purrr::map_dfr(SeuratObject::Layers(sc[[assay]]), function(layer) {
      # Get layer data
      layer_data = SeuratObject::LayerData(sc[[assay]], layer=layer)
      
      # Get path(s) to on-disk matrix
      layer_disk_paths = SeuratObject::.FilePath(layer_data)
      
      # Empty means no on-disk data and can be skipped
      layer_disk_paths = layer_disk_paths[nzchar(layer_disk_paths) > 0]
      if (length(layer_disk_paths)==0) layer_disk_paths = NULL
      if (is.null(layer_disk_paths)) return(NULL)
      
      # If a layer consists of multiple on-disk directories, split them
      layer_disk_paths = unlist(strsplit(layer_disk_paths, ","))
      layer_disk_paths = trimws(layer_disk_paths)
      
      # We want to keep the paths relative to the current directory
      layer_disk_paths = fs::path_rel(layer_disk_paths)
      
      # If write_disk_data is FALSE and the on-disk directories already exist 
      # (from previous run, stored in tool SaveSeuratRds) change paths so that 
      # these are used
      if (!write_disk_data & !is.null(save_seurat_rds)) {
        known_disk_paths = unlist(strsplit(save_seurat_rds$path, ","))
        known_disk_paths = trimws(known_disk_paths)
        layer_disk_paths = purrr::map_chr(layer_disk_paths, function(p) {
          i = which(basename(known_disk_paths) %in% basename(p))
          if (length(i) == 1) {
            return(known_disk_paths[i])
          } else if (length(i) == 0) {
            return(p)
          } else {
            stop("Multiple on-disk directories found for the same layer. This should not happen.")
          }
        })
      }
      
      # Return table with information
      layer_disk_fxn = SeuratObject::.DiskLoad(layer_data)
      if (is.null(layer_disk_fxn)) layer_disk_fxn = identity
      return(data.frame(
        # Layer name
        layer=layer,
        # Path(s) to on-disk matrix
        path=paste(layer_disk_paths, collapse=","),
        # Class of on-disk matrix
        class=paste(class(layer_data), collapse=","),
        # Package of on-disk matrix
        pkg=SeuratObject::.ClassPkg(layer_data),
        # Function to load on-disk matrix
        fxn=layer_disk_fxn
      ))
    })
    
    # No on-disk layers found, skip (everything stored in Seurat object)
    if (is.null(layer_disk_info) || !nrow(layer_disk_info)) {
      p(message="No on-disk layers found", class="sticky", amount=0)
      next
    }
    
    # Write on-disk matrices (if needed)
    if (write_disk_data) {
      for (i in seq_len(length.out=nrow(layer_disk_info))) {
        layer = layer_disk_info$layer[i]
        
        # A on-disk matrix can consist of multiple data directories
        layer_disk_paths = unlist(strsplit(layer_disk_info$path[i], ","))
        layer_disk_paths = trimws(layer_disk_paths)
        
        # Iterate over paths of data directories, copy them (if needed) and return new paths
        new_layer_disk_paths = purrr::map(layer_disk_paths, function(p) {
          np = file.path(destdir, basename(p))
          
          # If p already exists and we do not want write the data, do not write, 
          # just return
          if (file.exists(p) & !write_disk_data) return(p)
          
          # If np already exists, do not write, just return
          if (file.exists(np)) return(np)
          
          # Else write
          p(message=FormatString("Writing on-disk directory {basename(p)}, layer {layer}, to {destdir}"), class="sticky", amount = 0)
          np = as.character(.FileMove(path=p, new_path=destdir))
          return(np)
        })

        new_layer_disk_paths = paste(unlist(new_layer_disk_paths), collapse=",")
        layer_disk_info[i, "path"] = new_layer_disk_paths
      }
    }
    
    # If requested, store paths to on-disk matrices relative to Seurat object
    if (relative_paths) {
      p(message=FormatString("Adjusting paths to be relative to {dirname(file)}"), class="sticky", amount=0)
      
      for (i in seq_len(length.out=nrow(layer_disk_info))) {
        layer_disk_paths = unlist(strsplit(layer_disk_info$path[i], ","))
        new_layer_disk_paths = purrr::map(layer_disk_paths, function(p) {
          return(fs::path_rel(path=p, start=dirname(path=file)))
        })
      }
    }
    
    # Add to cache information
    layer_disk_info$assay = assay
    cache[[assay]] = layer_disk_info
    
    if (nrow(layer_disk_info) == length(SeuratObject::Layers(sc[[assay]]))) {
      p(message = FormatString("Clearing layers from {assay}"), class="sticky", amount=0)
      adata = SeuratObject::S4ToList(sc[[assay]])
      adata$layers = list()
      adata$default = 0L
      adata$cells = SeuratObject::LogMap(colnames(sc[[assay]]))
      adata$features = SeuratObject::LogMap(rownames(sc[[assay]]))
      sc[[assay]] = SeuratObject::ListToS4(adata)
    } else {
      p(message = FormatString("Clearing layers from {assay}"), class="sticky", amount=0)
      for (layer in layer_disk_info$layer) {
        SeuratObject::LayerData(sc[[assay]], layer=layer) = NULL
      }
    }
    p()
  }
  
  # Update table with on-disk matrix information in Seurat object
  cache = do.call("rbind", cache)
  if (!is.null(cache) && nrow(cache) > 0) {
    p(message="Saving on-disk cache to object", class="sticky", amount=0)
    row.names(cache) = NULL
    #SeuratObject::Tool(sc) = cache
    sc@tools$SaveSeuratRds = cache
  }
  
  saveRDS(sc, file=file, compress=compress)
}

#' Update On-Disk Matrix Directories
#'
#' Copies on-disk layers of a Seurat object to a new directory and updates
#' the internal references. Useful when relocating or reorganizing saved data.
#'
#' @param sc A Seurat object with on-disk layers.
#' @param dir Character. New directory path for on-disk layers.
#' @param assays Character vector or \code{NULL}. Which assays to process.
#'   If \code{NULL}, processes all assays. Default is \code{NULL}.
#' @param layer Character or \code{NULL}. Which layers to copy. Can be a
#'   specific layer name or a pattern. If \code{NULL}, copies all layers.
#'   Default is \code{NULL}.
#' @param update_tool_saveseuratrds Logical. If \code{TRUE}, update the
#'   \code{SaveSeuratRds} tool entry with new paths. Default is \code{FALSE}.
#'
#' @return The Seurat object with updated on-disk layer paths.
#'
#' @note
#' This function copies data and adjusts paths for IterableMatrix objects.
#' To make changes permanent, the Seurat object must be saved again.
#'
#' @importFrom SeuratObject Assays Layers LayerData .FilePath .DiskLoad
#' @importFrom Seurat Tool Misc
#' @importFrom progressr progressor
#' @importFrom fs path_rel
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun
#' # Move on-disk layers to a new directory
#' seurat_obj <- UpdateMatrixDirs(seurat_obj, "new_location/matrices")
#' }
UpdateMatrixDirs = function (sc, dir, assays=NULL, layer=NULL, update_tool_saveseuratrds=FALSE) {
  # New directory for on-disk matrices
  dir = normalizePath(path=dir, winslash="/", mustWork=FALSE)
  if (is.null(assays)) assays = SeuratObject::Assays(sc)
  
  # Stores information about on-disk matrices
  cache = SeuratObject::Tool(sc, slot="SaveSeuratRds")
  
  # Iterate over assays
  progr = progressr::progressor(along=assays)
  progr(message=paste("Copying on-disk matrices to directory", dir), class="sticky", amount=0)
  
  for (a in assays) {
    # Skip if assay has no information about on-disk matrices
    if (!a %in% cache$assay) next
    
    progr(message = paste("Searching through assay", a), class="sticky", amount=0)
    
    if (!is.null(layer)) {
      layers = SeuratObject::Layers(sc, assay=a, layers=layer)
    } else {
      layers = SeuratObject::Layers(sc, assay=a)
    }
    
    # Iterate over layers
    for(i in seq_along(layers)) {
      # Get IterableMatrix for layer
      data = SeuratObject::LayerData(sc, assay=a, layer=layers[i])
      
      # Get old path
      path = SeuratObject::.FilePath(x=data)
      path = Filter(f=nzchar, x=path)
      if (is.null(path)) next
      
      # Move on-disk matrix directory to new path
      progr(message = paste("Moving layer", layers[i], "to", dir), class="sticky", amount=0)
      
      path = unlist(strsplit(path, ","))
      new_path = lapply(path, function(p) {
        np = file.path(dir, basename(p))
        assertthat::assert_that(!file.exists(np) & !dir.exists(np),
                                msg=FormatString("Cannot copy matrix directory to already existing path {np}. Please delete this path first."))
        return(as.character(.FileMove(path=p, new_path=dir)))
      })
      new_path = as.character(paste(unlist(new_path), collapse=","))
      
      # Reload matrix directory with new path into Seurat object
      fnx = SeuratObject::.DiskLoad(data)
      fnx = eval(expr = str2lang(fnx))
      SeuratObject::LayerData(sc, assay=a, layer=layers[i]) = fnx(new_path)
      
      # Update path for on-disk matrices
      if (!is.null(cache)) {
        idx = which(cache$assay == a & cache$layer == layers[i])
        cache$path[idx] = new_path
      }
    }
  }
  progr(type='finish')
  
  # Update information about on-disk matrices in Seurat object if requested
  if (update_tool_saveseuratrds) {
    if (!is.null(cache) && nrow(cache) > 0) {
      row.names(cache) = NULL
      sc@tools$SaveSeuratRds = cache
    }
  }
  
  return(sc)
}


#' Export Seurat Object to Loupe File
#'
#' Exports a Seurat object to a 10x Genomics Loupe Browser file format
#' (\code{.cloupe}). Enables visualization and exploration of the data in the
#' Loupe Browser application.
#'
#' @param sc A Seurat object.
#' @param assay Character or \code{NULL}. Assay to include. If \code{NULL},
#'   uses the default assay.
#' @param categories Character vector or \code{NULL}. Cell metadata columns
#'   to include. If \code{NULL}, includes all non-numeric columns.
#'   Numeric columns are always discarded.
#' @param embeddings Character vector or \code{NULL}. Embeddings to include.
#'   If \code{NULL}, includes all embeddings.
#' @param barcodes Character vector or \code{NULL}. Barcodes to include.
#'   If \code{NULL}, includes all barcodes from the selected assay.
#' @param output_dir Character. Directory where the Loupe file will be saved.
#'   Default is \code{"."}.
#' @param output_name Character. Name of the output file. Default is
#'   \code{"cloupe.cloupe"}.
#'
#' @return Invisibly returns \code{TRUE} on success.
#'
#' @details
#' Requires the \code{loupeR} package to be installed. The function will
#' automatically set up the loupeR executable if needed.
#'
#' @importFrom SeuratObject DefaultAssay Cells Embeddings
#' @importFrom purrr map discard
#' @importFrom forcats fct_na_value_to_level
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Export to Loupe format
#' ExportLoupe(seurat_obj, output_dir = "output", output_name = "my_data")
#' }
ExportLoupe = function(sc, assay=NULL, categories=NULL, embeddings=NULL, barcodes=NULL, output_dir=".", output_name="cloupe.cloupe") {
  # Download executable for loupeR
  loupeR::setup()
  
  # It it does not work, it has to be done manually
  louper_status = loupeR:::needs_setup()
  if (!louper_status$success) stop(louper_status$msg)
  
  # Check requested assays
  if (is.null(assay)) assay = SeuratObject::DefaultAssay(sc)
  assertthat::assert_that(all(assay %in% SeuratObject::Assays(sc)),
                          msg=FormatString("Requested assay {assay} part of the Seurat object."))
  
  # Barcoded and barcode metadata
  if (is.null(barcodes)) barcodes = SeuratObject::Cells(sc[[assay]])
  barcode_metadata = sc[[]][barcodes, ]
  
  # Check and discard numeric columns (Loupe cannot handle them)
  if (is.null(categories)) categories = colnames(barcode_metadata)
  assertthat::assert_that(all(categories %in% colnames(barcode_metadata)),
                          msg="Not all requested categories are part of the cell metadata.")
  categories = purrr::discard(categories, function(i) return(is.numeric(barcode_metadata[, i])))
  
  # Get counts of assay and convert to numeric matrix
  counts = SeuratObject::GetAssayData(sc, assay=assay, layer="counts")
  counts = as(counts[, barcodes], "dgCMatrix")
  
  # Replace NA with "NA" in barcode metadata
  # Convert character columns to factors
  categorial_data = purrr::map(categories, function(x) {
    v = barcode_metadata[, x]
    if (!is.factor(v)) {
      v = factor(as.character(v))
    }
    if (any(is.na(v))) v = forcats::fct_na_value_to_level(v, level="NA")
    return(v)
  })
  names(categorial_data) = categories
  categorial_data[["active_cluster"]] = categorial_data[["seurat_clusters"]]
  
  # Get embeddings data
  if (is.null(embeddings)) embeddings = SeuratObject::Reductions(sc)
  embedding_names = embeddings
  embeddings = purrr::map(embedding_names, function(r) {
    return(SeuratObject::Embeddings(sc, r)[,1:2])
  })
  names(embeddings) = embedding_names
  
  # Seurat object version
  seurat_obj_version = NULL
  if (!is.null(sc@version)) seurat_obj_version = as.character(sc@version)
  
  # Create Loupe file
  success = loupeR::create_loupe(counts, 
                                 clusters=categorial_data,
                                 projections=embeddings,
                                 output_dir=output_dir,
                                 output_name=gsub("\\.cloupe", "", output_name),
                                 force=TRUE,
                                 seurat_obj_version=seurat_obj_version)
}

#' Export Seurat Object to Xenium Explorer Format
#'
#' Exports cell categorical metadata from a Seurat object to the Xenium Explorer
#' \code{analysis.zarr.zip} file format. This native format allows persistent
#' storage of cell annotations that can be loaded directly into Xenium Explorer.
#'
#' @param sc A Seurat object.
#' @param assay Character or \code{NULL}. Assay to include. If \code{NULL},
#'   uses the default assay.
#' @param categories Character vector or \code{NULL}. Cell metadata columns
#'   to include. If \code{NULL}, includes all non-numeric columns.
#'   Numeric columns are automatically discarded.
#' @param barcodes Character vector or \code{NULL}. Barcodes to include.
#'   If \code{NULL}, includes all barcodes from the selected assay.
#' @param output_dir Character. Directory where files will be saved.
#'   Default is \code{"."}.
#' @param output_name Character. Name of the output file. Default is
#'   \code{"analysis.zarr.zip"}.
#'
#' @return Invisibly returns \code{NULL}. Creates one \code{analysis.zarr.zip}
#'   file per sample in subdirectories named by sample.
#'
#' @details
#' By default, Xenium Explorer allows importing cell metadata via CSV files,
#' but only one category at a time and it cannot be saved persistently.
#' This function exports complete categorical metadata in the native Xenium
#' Explorer format.
#'
#' The Seurat object must have:
#' \itemize{
#'   \item A \code{datasets} table in the misc slot with \code{experiment} and
#'     \code{path} columns
#'   \item An \code{orig_barcode} column in the cell metadata
#' }
#'
#' Requires Python with the \code{zarr} and \code{numpy} modules.
#'
#' @importFrom SeuratObject DefaultAssay Cells
#' @importFrom reticulate import
#' @importFrom purrr map discard
#' @importFrom dplyr filter pull bind_rows
#' @importFrom hdf5r H5File
#' @importFrom assertthat assert_that
#' @export
#'
#' @examples
#' \dontrun{
#' # Export cell annotations for Xenium Explorer
#' ExportXeniumExplorer(
#'   seurat_obj,
#'   categories = c("celltype", "cluster"),
#'   output_dir = "xenium_annotations"
#' )
#' }
ExportXeniumExplorer = function(sc, assay=NULL, categories=NULL, barcodes=NULL, output_dir=".", output_name="analysis.zar.zip") {
  # For this function, we need a datasets table in the misc slot (to get all barcodes present in the dataset)
  assertthat::assert_that("datasets" %in% names(sc@misc),
                          msg="This function requires the Seurat object to have a 'datasets' table in the misc slot with columns 'experiment' for orig.ident and 'path' for the path to the dataset.")
  datasets = sc@misc$datasets
  
  # Check requested assays
  if (is.null(assay)) assay = SeuratObject::DefaultAssay(sc)
  assertthat::assert_that(all(assay %in% SeuratObject::Assays(sc)),
                          msg=FormatString("Requested assay {assay} part of the Seurat object."))
  
  # Barcoded and barcode metadata
  if (is.null(barcodes)) barcodes = SeuratObject::Cells(sc[[assay]])
  barcode_metadata = sc[[]][barcodes, ]
  
  # Moreover, there needs to be a column 'orig_barcode' in the cell metadata
  assertthat::assert_that("orig_barcode" %in% colnames(barcode_metadata),
                          msg="This function requires the Seurat object to have a column 'orig_barcode' in the cell metadata.")
  
  # Check and discard numeric columns (Xenium Explorer cannot handle them)
  if (is.null(categories)) categories = colnames(barcode_metadata)
  assertthat::assert_that(all(categories %in% colnames(barcode_metadata)),
                          msg="Not all requested categories are part of the cell metadata.")
  categories = purrr::discard(categories, function(i) return(is.numeric(barcode_metadata[, i])))
  
  # Per dataset
  for(smp in levels(barcode_metadata$orig.ident)) {
    dir.create(file.path(output_dir, smp), showWarnings=FALSE, recursive=TRUE)
    
    # Get path to dataset and get a list of all barcodes present in the dataset
    # This is needed because the Xenium Explorer requires all barcodes to be present in the metadata (even if they are not part of the analysis)
    dataset_path = datasets %>% 
      dplyr::filter(experiment == smp) %>% 
      dplyr::pull(path)
    
    # Get a list of all barcodes present in the dataset
    if (dir.exists(dataset_path)) {
      # 10x market exchange format
      barcodes_file = dplyr::case_when(
        file.exists(file.path(dataset_path, "barcodes.tsv.gz")) ~ "barcodes.tsv.gz",
        file.exists(file.path(dataset_path, "barcodes.tsv")) ~ "barcodes.tsv"
      )
      
      unfiltered_barcodes = readLines(file.path(dataset_path, barcodes_file))
    } else {
      # h5 file
      hdf5_fh = hdf5r::H5File$new(dataset_path, mode = "r+")
      unfiltered_barcodes = hdf5_fh[["/matrix/barcodes"]][]
      hdf5_fh$close()
    }
    
    # Get dataset barcodes and metadata
    bcs = barcode_metadata %>% 
      dplyr::filter(orig.ident == smp) %>% 
      rownames()
    categories = categories[categories != "orig_barcode"]
    categorial_data = barcode_metadata[bcs, c("orig_barcode", categories)]
    rownames(categorial_data) = NULL
    
    # Add filtered (removed) barcodes to metadata table
    if (length(unfiltered_barcodes) > length(barcodes)) {
      categorial_data = categorial_data %>% 
        dplyr::bind_rows(
          data.frame(orig_barcode=setdiff(unfiltered_barcodes, barcodes))
        )
    }
    i = match(unfiltered_barcodes, categorial_data$orig_barcode)
    categorial_data = categorial_data[i, ]
    
    # Convert character columns to factors
    categorial_data = purrr::map(categories, function(x) {
      v = categorial_data[, x]
      if (!is.factor(v)) {
        v = factor(as.character(v))
      }
      return(v)
    })
    names(categorial_data) = categories
    
    # Attributes for zarr store
    zarr_attr = list("major_version" = 1,
                     "minor_version" = 0,
                     "number_groupings" = length(categorial_data),
                     "grouping_names" = names(categorial_data),
                     "group_names" = unname(purrr::map(categorial_data, levels)))
    
    # Convert categorial data to zarr-compatible format (lots of indices packed)
    zarr_categorial_data = purrr::map(categorial_data, function(values) {

      # For each categories, get the cell indices (note: we switch now to 0-based indices)
      values_indices = purrr::map(levels(values), function(cat) return(which(values == cat) - 1))
      
      # For each category, get the cumulative length of cell indices
      values_cum_len = purrr::map(values_indices, length) %>% 
        unlist() %>% 
        cumsum() %>%
        as.integer()
      
      # indices: array of the cell indices assigned to one of the categories
      indices = values_indices %>% unlist()
      
      # indptr: indicates the cell index value (row) where a new category begins
      if (length(values_cum_len) == 1) {
        indptr = c(0)
      } else {
        indptr = c(0, values_cum_len[1:length(values_cum_len)-1])
      }
      
      if (length(indices) == 0){
        indptr = as.integer(c())
      }
      
      return(list("indices" = as.integer(indices), "indptr" = as.integer(indptr)))
    })
    names(zarr_categorial_data) = names(categorial_data)
    
    # Now switch to python via reticulate
    # zarr_module is used to access the python module zarr, numpy_module ...
    zarr_module = import("zarr")
    numpy_module = import("numpy")
    
    # Create a zarr store file
    zarr_store = zarr_module$ZipStore(file.path(output_dir, smp, "analysis.zarr.zip"), mode="w")
    
    # Create a hierarchy with root and group "cell_groups"
    root = zarr_module$group(store=zarr_store)
    cell_groups = root$create_group("cell_groups")
    
    # Add zarr groups
    for(i in seq_along(zarr_categorial_data)) {
      indices = zarr_categorial_data[[i]]$indices
      indptr = zarr_categorial_data[[i]]$indptr
      chunk_size = max(length(indices), 1)
      
      group = cell_groups$create_group(as.character(i-1))
      group$array("indices", 
                  numpy_module$array(indices, dtype="uint32"), 
                  dtype="uint32", 
                  chunks=reticulate::tuple(chunk_size))
      group$array("indptr", 
                  numpy_module$array(indptr, dtype="uint32"), 
                  dtype="uint32", 
                  chunks=reticulate::tuple(chunk_size))
    }
    
    cell_groups$attrs$put(zarr_attr)
    zarr_store$close()
  }
}
