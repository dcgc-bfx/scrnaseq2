---
# Module-specific parameters (that are not already in the profile yaml)
params:
  # Name of the module used in configurations
  module: "read_data"
    
  # Relative path to the module directory (which contains the qmd file)
  module_dir: "modules/1_read_data"
  
  # Path to a csv or Excel file containing a table with single-cell datasets. For Excel files, a sheet can be specified by appending ':<sheet_number>'.
  # The table must contain the following columns:
  # - sample (required): Name of the (physical) sample. A sample can have multiple single-cell experiments (e.g. multiple 10x experiments).
  # - experiment (required): Name of the single-cell experiment. A single-cell experiment can have multiple single-cell datasets (e.g. two datasets derived from the same 10x experiment).
  # - technology (required): Single-cell technology used. Can be: "smartseq2" - Smartseq2, "smartseq3" - Smartseq3, "10x" - 10x, "10x_visium" - 10x Visium, 
  #                          "10x_xenium" - 10x Xenium, "parse" - Parse Biosciences, "scale" - ScaleBio.
  # - assays (required): Assay(s) of single-cell data to read. Currently supported are: RNA (Gene Expression), ADT (Antibody Protein Capture) and 
  #                      ATAC (Chromatin Accessibility), CUSTOM (Custom). Multiple assays can be specified comma-separated.
  # - path (required): Path to single-cell dataset. Can be: csv file (Smartseq2/3), mtx matrix market directory (10x, 10x Visium, 10x Xenium, Parse Biosciences,
  #                    ScaleBio), hdf5 file (10x, 10x Visium, 10x Xenium), h5ad file (Parse Biosciences).
  # - metrics_file (can be empty): Path to a file containing a metrics summary table. For Smartseq2/3, this can be any table. For 10x, 10x Visium or 10x Xenium, this 
  #                                must be a "metrics_summary.csv" file produced by the Cellranger pipelines. For Parse Biosciences, this must be a "analysis_summary.csv"
  #                            file produced by the splitpipe pipeline. For ScaleBio, this must be a "*.reportStatistics.csv" file produced by the scalerna pipeline.
  # - barcode_metadata_file (can be empty): Path to a file containing a table of additional barcode metadata. Can be: a csv, Excel or an R table saved as rds file. 
  #                                         For Excel files, a sheet can be specified by appending ':<sheet_number>'. First column must contain the barcode. Table
  #                                         does not need to contain all barcodes. If a dataset contains multiple assays, multiple files can be provided (comma-separated).
  # - feature_metadata_file (can be empty): Path to a file containing a table of additional feature metadata. Can be: a csv, Excel or an R table saved as rds file. 
  #                                         For Excel files, a sheet can be specified by appending ':<sheet_number>'. First column must contain the feature id. Table
  #                                         does not need to contain all features. If a dataset contains multiple assays, multiple files can be provided (comma-separated).
  # - rename_features (can be empty): When reading counts, ids are used to name features. Features can be renamed by: a) Provide a feature metadata column index or name to rename features, 
  #                                   b) Specify 'ENSEMBL' to fetch gene information from Ensembl (gene features only), c) Leave empty or specify 'NO' to not rename features.
  #                                   If a dataset contains multiple assays, this option can be specified multiple times (comma-separated). 
  # - barcode_suffix (can be empty): Suffix to add to the barcodes. If left empty, numeric suffixes will be added ('-1', '-2', etc). If set to 'NO', no suffixes are added.
  datasets_file: "datasets/10x_xenium_mouse_brain.xlsx"
  
  # Default assay. All cells must have this assay.
  default_assay: RNA
  
  # For large datasets: Do not keep counts in memory but store on disk in matrix directories. Computations will access only the relevant parts of the data. Once done, matrix directories will be saved together with the Seurat object in the module directory.
  on_disk_counts: false
  
  # For large datasets: Copy matrix directories to a temporary directory for computations. This will improve performance if the temporary directory  has a better performance than normal disks (e.g. SSD). Once done, matrix directories will be copied back to the module directory. The temporary directory will be deleted once the R session exists.
  on_disk_use_tmp: false
  
  # Downsample data to at most n barcodes per sample
  downsample_barcodes_n: null
  
  # Downsample all samples equally according to the smallest sample. Overwritten by downsample_cells_n.
  downsample_barcodes_equally: false
  
  # Downsample data to at most n features per assay
  #   null to deactivate
  downsample_features_n: null
  
  # Parse plate information and add to barcode metadata. 
  parse_plate_information: false
  
  # For parsing plate information, regular expression that captures the plate information part of the name. Plate, row and col will be recorded and the plate information will be removed from the name.
  plate_information_regex: "_(\\d+)_([A-Z])(\\d+)$"
  
  # When parsing plate information, set the name of the single-cell experiment (orig.ident) for each cell to the cell name after having removed the plate information
  plate_information_update_identity: false

  # For 10x Xenium in situ data: Load cell "centroids", cell "segmentation" or both (default).
  spatial_coordinate_type:
    - "centroids"
    - "segmentation"
  
  # Number of cores to use for computations
  cores: 4
  
# Module execution
execute:
  # Should this module be frozen and never re-rendered?
  # - auto: if the code has not changed (default)
  # - true/false: freeze/re-render
  # Does not apply in interactive mode or when explicitly rendering this document via in rstudio
  freeze: auto
---

# Read data

This workflow supports pre-processed data from a variety of sources, including single-cell data from platforms and protocols such as 10x Genomics, Smart-seq2 and Parse Biosciences. Additionally, it supports spatial data from the 10x Visium and 10x Xenium platforms.

```{r}
#| label: setup

# If running code interactively in rstudio, set profile here
# When rendering with quarto, profile is already set and should not be overwritten
if (nchar(Sys.getenv("QUARTO_PROFILE")) == 0) {Sys.setenv("QUARTO_PROFILE" = "scrnaseq")}

# Load libraries
library(knitr)
library(magrittr)
library(gt)
library(Seurat)
library(BPCells)
library(ggplot2)
library(future)

# Get module directory (needed to access files within the module directory)
module_name = params$module_name
module_dir = params$module_dir
```

```{r}
#| label: read_data_preparation

# CODE
# Working directory is automatically back to scrnaseq2 at this point
# Source general configurations (always)
source("R/general_configuration.R")

# Source required R functions
source("R/functions_util.R")
source("R/functions_io.R")
source("R/functions_plotting.R")
source("R/functions_analysis.R")

# Be verbose or not
verbose = param("verbose")

# Parallelisation plan for all functions that support future
plan(multisession, workers=param("cores"))

# DIRECTORIES
# Module directory 'results' contains all output that should be provided together with the final report of the project
dir.create(file.path(module_dir, "results"), showWarnings=FALSE)
files = list.files(path=file.path(module_dir, "results"), full.names=TRUE)
if (length(files) > 0) unlink(files, recursive=TRUE)

# Module directory 'sc' contains the final Seurat object
dir.create(file.path(module_dir, "sc"), showWarnings=FALSE)
files = list.files(path=file.path(module_dir, "sc"), full.names=TRUE)
if (length(files) > 0) unlink(files, recursive=TRUE)

# Module directory 'tmp' is needed only in this module and only if parameters on_disk_counts=true and on_disk_use_tmp=false
# In this case, we need to write the created on-disk matrices somewhere
dir.create(file.path(module_dir, "tmp"), showWarnings=FALSE)
files = list.files(path=file.path(module_dir, "sc"), full.names=TRUE)
if (length(files) > 0) unlink(files, recursive=TRUE)
```

## Datasets

The following datasets are defined:

```{r}
#| label: tbl-read_data_datasets
#| tbl-cap: "Datasets to be read"

# Read table with datasets

datasets_file = param("datasets_file")
datasets = ReadDatasetsTable(datasets_file)

for (i in 1:nrow(datasets)) {
  # Required columns
  assertthat::assert_that(!is.na(datasets$sample[i]),
                            msg=FormatString("Column 'sample' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$experiment[i]),
                            msg=FormatString("Column 'experiment' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$technology[i]),
                            msg=FormatString("Column 'technology' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$assays[i]),
                            msg=FormatString("Column 'assays' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$path[i]),
                            msg=FormatString("Column 'path' cannot be empty or NA (row {i})."))
  
  # Default assay must be present in all datasets - potential fix: Add an empty default assay table for each dataset without default assay - but is it neccessary (?)
  default_assay = param("default_assay")
  assertthat::assert_that(default_assay %in% (strsplit(datasets$assays[i], ",") %>% unlist() %>% trimws()),
                            msg=FormatString("Each dataset must contain the default assay {default_assay} (dataset {path})."))
}

# If barcode_suffix is NA, generates suffixes per experiment (-1, -2, ...) 
if (any(is.na(datasets$barcode_suffix))) {
  assertthat::assert_that(sum(is.na(datasets$barcode_suffix)) == nrow(datasets),
                            msg=FormatString("If one barcode suffix is empty/NA, all barcode suffixes need to be empty/NA."))
  datasets$barcode_suffix = factor(datasets$experiment, level=unique(datasets$experiment)) %>% as.integer() %>% paste0("-", .)
}

# Print datasets
tmp = datasets %>% t() %>% as.data.frame()
colnames(tmp) = paste("Dataset", 1:ncol(tmp))
gt(tmp, rownames_to_stub=TRUE)
```

## Read and print general metrics

Prior to this workflow, raw sequencing reads were mapped to the reference. The resulting mapping statistics are printed below for a first estimation of sample quality.

::: {.callout-note title="Barcodes and features"}

Remember, dependent on the data we work with, we will use the wording **barcode** to refer to cells, nuclei, or spots, and the wording **feature** to refer to genes, peaks or proteins.

:::

::: {.callout-tip title="Pre-processing 10x scRNA-seq data with Cell Ranger" collapse="true"}

We use the 10x Genomics Cell Ranger software to map 10x sequencing reads. The result is a count matrix that contains the number of unique transcripts per gene (rows) and cell (columns). To save storage space, the matrix is converted into a condensed format and described by the following 3 files:

- “features.tsv.gz”: Tabular file with gene annotation (Ensembl gene ID and the gene symbol) to identify genes
- “barcodes.tsv.gz”: File with barcodes to identify cells
- “matrix.mtx.gz”: A condensed version of the count matrix

:::

::: {.callout-tip title="Pre-processing SmartSeq-2 scRNA-seq data" collapse="true"}

Sequencing reads from SmartSeq-2 (and other) experiments can be mapped with any suitable mapping software as long as a count matrix is generated that contains the number of mapped reads per gene (rows) and cell (columns). The first row of the count matrix should contain cell barcodes or names, the first column of the matrix should contain Ensembl gene IDs.

:::

```{r}
#| label: read_data_metrics
#| results: asis

# Show general metrics for datasets

# Read metrics tables into list
metrics_tables = purrr::map(1:nrow(datasets), function(i) {
  experiment = datasets$experiment[i]
  technology = datasets$technology[i]
  metrics_file = datasets$metrics_file[i]
  
  if (!is.na(metrics_file)) {
    metrics_table = ReadMetrics(metrics_file=metrics_file,  technology=technology) %>% 
      dplyr::bind_rows() %>%
      dplyr::mutate(Dataset=experiment, .before=1)
  } else {
    metrics_table = data.frame(Dataset=experiment)
  }
  
  return(metrics_table)
})

# Then group and bind rows by technology
dataset_rows_by_tech = split(1:nrow(datasets), datasets$technology)
metrics_tables_by_tech = purrr::map(dataset_rows_by_tech, function(rows) {
  metrics_table = dplyr::bind_rows(metrics_tables[rows])
  return(metrics_table)
})

# How to dynamically generate tables: https://stackoverflow.com/questions/73585417/iterating-to-create-tabs-with-gt-in-quarto
# String to create a chunk for each assay ({{assay}} is placeholder)
chunk_template = "
##### {{technology}}

\`\`\`{r}
#| label: tbl-read_data_metric_{{technology}}
#| tbl-cap: 'Metrics for technology {{technology}}'

tmp = metrics_tables_by_tech[['{{technology}}']] %>% t() %>% as.data.frame()
colnames(tmp) = paste('Experiment', 1:ncol(tmp))
gt(tmp, rownames_to_stub=TRUE)
\`\`\`
"

cat("::: panel-tabset\n")
for(n in names(metrics_tables_by_tech)) {
  chunk_filled = knitr::knit_expand(text=chunk_template, technology=n)
  
  if(interactive()) {
    print(EvalKnitrChunk(chunk_filled))
  } else {
    chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
    cat(chunk_filled, sep='\n')
  }
}
cat(":::\n")
```

## Read counts and create Seurat object

We next read the counts for each dataset. Here is the summary of barcodes and features for each dataset:

```{r}
#| label: read_data_read_counts
#| results: asis

# Read counts of barcodes (cells, nuclei or spots) and features (genes, peaks or proteins) for each dataset

############################
# Read counts for datasets #
############################

counts_lst = list()

for (i in 1:nrow(datasets)) {
  ###############################################################################
  # Get experiment, sample, dataset path, technology, assays and barcode suffix #
  ###############################################################################
  sample = datasets$sample[i]
  experiment = datasets$experiment[i]
  path = datasets$path[i]
  technology = datasets$technology[i]
  assays = strsplit(datasets$assays[i], split=",") %>% unlist() %>% trimws()
  barcode_suffix = datasets$barcode_suffix[i]
  if (is.na(barcode_suffix)) {
    barcode_suffix = paste0("-", i)
  } else if (barcode_suffix=='NO') {
    barcode_suffix = NULL
  }

  #########################
  # Read barcode metadata #
  #########################
  barcode_metadata_files = datasets$barcode_metadata_files[i]
  if (is.na(barcode_metadata_files)) {
    barcode_metadata = NULL
  } else {
    barcode_metadata_files = strsplit(barcode_metadata_files, split=",")  %>% unlist() %>% trimws()
    barcode_metadata = purrr::map(barcode_metadata_files, ReadMetadata)
    if (length(barcode_metadata) == 1) {
      barcode_metadata = barcode_metadata[[1]]
    }
  }
  
  #########################
  # Read feature metadata #
  #########################
  feature_metadata_files = datasets$feature_metadata_files[i]
  if (is.na(feature_metadata_files)) {
    feature_metadata = NULL
  } else {
    feature_metadata_files = strsplit(feature_metadata_files, split=",")  %>% unlist() %>% trimws()
    feature_metadata = purrr::map(feature_metadata_files, ReadMetadata)
    if (length(feature_metadata) == 1) {
      feature_metadata = feature_metadata[[1]]
    }
  }
  
  ###############
  # Read counts #
  ###############
  
  # Read counts
  counts_lst[[i]] = ReadCounts(path=path,
                                    technology=technology,
                                    assays=assays,
                                    barcode_metadata=barcode_metadata,
                                    feature_metadata=feature_metadata,
                                    barcode_suffix=barcode_suffix)
  
  #################################
  # If requested, rename features #
  #################################

  rename_features = datasets$rename_features[i]
  if (is.na(rename_features)) {rename_features = 'NO'}
  
  # Rename using a feature metadata column or using Ensembl (keyword: "ENSEMBL")
  if (rename_features != 'NO' ) {
    rename_features = strsplit(rename_features, split=",")  %>% unlist() %>% trimws()
    assertthat::assert_that(length(rename_features) == 1 | length(rename_features) == length(counts_lst[[i]]),
                            msg=FormatString("When renaming features, either specify one feature metadata column/ENSEMBL to be used for all assays or specify a feature metadata column/ENSEMBL for each assay (dataset {path})."))
  }
  
  # Make sure there an option for each assay
  if (length(rename_features) == 1) {
      rename_features = rep(rename_features, length(counts_lst[[i]]))
  }
  
  # Now loop through assays and process
  for (j in seq_along(counts_lst[[i]])) {
    assay = attr(counts_lst[[i]][[j]], "assay")
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    
    
    if (rename_features[j] == 'NO') {
      # 'NO': no renaming - just keep the current row names
      new_feature_names = rownames(feature_metadata)
    } else {
  
      # If 'ENSEMBL', get ensembl information for features
      if (rename_features[j] == "ENSEMBL") {
        # Query Ensembl
        ensembl_annotation = EnsemblFetchGeneInfo(ids=rownames(feature_metadata), 
                                                  species=param("species"),
                                                  ensembl_version=param("ensembl"),
                                                  mart_attributes = c(ensembl_id="ensembl_gene_id",ensembl_symbol="external_gene_name",
                                                                      ensembl_biotype="gene_biotype",ensembl_description="description",
                                                                      ensembl_chr="chromosome_name",ensembl_start_position="start_position", 
                                                                      ensembl_end_position="end_position", ensembl_strand="strand"),
                                                  useCache=TRUE)
        feature_metadata = dplyr::bind_cols(feature_metadata, ensembl_annotation, .name_repair="minimal")
        attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
        
        # Use metadata column 'ensembl_symbol'
        rename_features[j] = "ensembl_symbol"
      }
    
      # Check that metadata column exists and get values
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      feature_name_column = rename_features[j]
    
      is_column_index = suppressWarnings(feature_name_column %>% as.numeric() %>% is.na() %>% not())
      if (is_column_index) {feature_name_column = as.numeric(feature_name_column)}
    
      if (is.numeric(feature_name_column)) {
          assertthat::assert_that(feature_name_column <= ncol(feature_metadata),
                                  msg=FormatString("Column number {feature_name_column} exceeds the number of columns in the feature metadata for dataset {path}, assay {assay}."))
      } else {
          assertthat::assert_that(feature_name_column %in% colnames(feature_metadata),
                                  msg=FormatString("Column {feature_name_column} cannot be found in the feature metadata available for dataset {path}, assay {assay}."))
      }
      new_feature_names = feature_metadata[, feature_name_column, drop=TRUE]

      # Use id if there is no symbol available
      new_feature_names = ifelse(is.na(new_feature_names) | nchar(new_feature_names)==0, rownames(feature_metadata), new_feature_names)
    }

    
    # Now make new feature names Seurat-compatible
    if (any(grepl(pattern="_", x=new_feature_names, fixed=TRUE))) {
      CalloutBox(x="New feature names contain '_' after renaming for dataset {path}, assay {assay}. All occurences will be replaced with '-'.", type="warning")
      new_feature_names = gsub(pattern="_", replacement="-", x=new_feature_names, fixed=TRUE)
    }
    
    # Keep new names in feature metadata column 'feature_name'
    feature_metadata[["feature_name"]] = new_feature_names
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
  }

  ##################
  # Other metadata #
  ##################
  for (j in seq_along(counts_lst[[i]])) {
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
}

##################################################
# Now rename features in each assay              #
# but make sure it is consistent across datasets #
##################################################

# Get rownames (old names) and feature metadata column feature_name (new names)
features_renaming = purrr::map_depth(counts_lst, 2, function(cts) {
  metadata = attr(cts, "feature_metadata") %>% 
    dplyr::select(new_name=feature_name) %>%
    tibble::rownames_to_column("old_name")
  metadata$assay = attr(cts, "assay")
  return(metadata)
}) %>% purrr::flatten() %>% dplyr::bind_rows() %>% unique()

features_renaming = split(features_renaming, features_renaming$assay)
features_renaming = purrr::map(features_renaming, function(df) {
  df$assay = NULL
  df = unique(df) %>% dplyr::arrange(old_name)
  df$new_name = make.unique(df$new_name)
  df = setNames(df$new_name, df$old_name)
  return(df)
})

# Rename
for (i in seq(counts_lst)) {
  for (j in seq(counts_lst[[i]])) {
    # Metadata gets lost when renaming
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
    technology = attr(counts_lst[[i]][[j]], "technology")
    assay = attr(counts_lst[[i]][[j]], "assay")
    sample = attr(counts_lst[[i]][[j]], "sample")
    experiment = attr(counts_lst[[i]][[j]], "orig.ident")
    path = attr(counts_lst[[i]][[j]], "path")
    
    # Rename
    old_names = rownames(counts_lst[[i]][[j]]) %>% as.character()
    new_names = features_renaming[[assay]][old_names]
    rownames(counts_lst[[i]][[j]]) = new_names
    rownames(feature_metadata) = new_names
    
    # Restore metadata
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]][[j]], "technology") = technology
    attr(counts_lst[[i]][[j]], "assay") = assay
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
}
```

```{r}
#| label: tbl-read_data_counts_summary
#| tbl-cap: "Counts for each dataset"

# Print summary of barcodes (cells, nuclei or spots) and features (genes, peaks or proteins)
counts_summary = data.frame(
  sample = purrr::map_depth(counts_lst, 2, attr, "sample") %>% purrr::flatten() %>% unlist(),
  dataset = purrr::map_depth(counts_lst, 2, attr, "orig.ident") %>% purrr::flatten() %>% unlist(),
  assay = purrr::map_depth(counts_lst, 2, attr, "assay") %>% purrr::flatten() %>% unlist(),
  barcodes = purrr::map_depth(counts_lst, 2, ncol) %>% purrr::flatten() %>% unlist(),
  features = purrr::map_depth(counts_lst, 2, nrow) %>% purrr::flatten() %>% unlist())

gt(counts_summary)
```

In case, the count datasets are very big, it is possible to (randomly) sample barcodes to generate smaller count subsets which may be faster to analyse but still give meaningful results. The number of barcodes to sample can either be a fixed number or the number of barcodes in the smallest sample. Similarly, it is also possible to sample down features.

```{r}
#| label: read_data_downsample
#| results: asis

# Downsample barcodes (cells, nuclei or spots)

# Determine number of barcodes to downsample
downsample_barcodes_n = param("downsample_barcodes_n")
downsample_barcodes_equally = param("downsample_barcodes_equally")

if (!is.null(downsample_barcodes_n) | downsample_barcodes_equally) {
  # Get barcodes per dataset
  barcodes_per_dataset = purrr::map(counts_lst, function(cts) {
    purrr::map(cts, colnames) %>% 
      unlist %>%
      unique() %>%
      return()
  })
  
  # If downsample equally, update sample number to number of barcodes of the smallest sample
  if (downsample_barcodes_equally) {
    downsample_barcodes_n = purrr::map(barcodes_per_dataset, length) %>% 
      unlist() %>% 
      min()
  }
  
  # Downsample barcodes
  barcodes_per_dataset = purrr::map(barcodes_per_dataset, function(b) {
    set.seed(getOption("random_seed"))
    b = sample(b, downsample_barcodes_n)
    return(b)
  })
  
  # Subset counts and metadata
  for (i in seq(counts_lst)) {
    for(j in seq_along(counts_lst[[i]])) {
      # Metadata gets lost when subsetting
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
      technology = attr(counts_lst[[i]][[j]], "technology")
      assay = attr(counts_lst[[i]][[j]], "assay")
      sample = attr(counts_lst[[i]][[j]], "sample")
      experiment = attr(counts_lst[[i]][[j]], "orig.ident")
      path = attr(counts_lst[[i]][[j]], "path")
      
      # Subset counts
      keep = colnames(counts_lst[[i]][[j]]) %in% barcodes_per_dataset[[i]]
      counts_lst[[i]][[j]] = counts_lst[[i]][[j]][, keep]
      
      # Subset barcode metadata
      keep = rownames(barcode_metadata) %in% barcodes_per_dataset[[i]]
      barcode_metadata = barcode_metadata[keep,]

      # Restore metadata
      attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]][[j]], "technology") = technology
      attr(counts_lst[[i]][[j]], "assay") = assay
      attr(counts_lst[[i]][[j]], "sample") = sample
      attr(counts_lst[[i]][[j]], "orig.ident") = experiment
      attr(counts_lst[[i]][[j]], "path") = path
    }
  }
}

# Downsample features  (genes, peaks or proteins)
downsample_features_n = param("downsample_features_n")

if (!is.null(downsample_features_n)) {
  
    # Subset counts and metadata
  for (i in seq(counts_lst)) {
    for(j in seq_along(counts_lst[[i]])) {
      # Metadata gets lost when subsetting
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
      technology = attr(counts_lst[[i]][[j]], "technology")
      assay = attr(counts_lst[[i]][[j]], "assay")
      sample = attr(counts_lst[[i]][[j]], "sample")
      experiment = attr(counts_lst[[i]][[j]], "orig.ident")
      path = attr(counts_lst[[i]][[j]], "path")
      
      # Sample row indices
      set.seed(getOption("random_seed"))
      rows = sample(seq(nrow(counts_lst[[i]][[j]])), downsample_features_n)
      
      # Subset counts
      counts_lst[[i]][[j]] = counts_lst[[i]][[j]][rows, ]
      
      # Subset feature metadata
      feature_metadata = feature_metadata[rows,]

      # Restore metadata
      attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]][[j]], "technology") = technology
      attr(counts_lst[[i]][[j]], "assay") = assay
      attr(counts_lst[[i]][[j]], "sample") = sample
      attr(counts_lst[[i]][[j]], "orig.ident") = experiment
      attr(counts_lst[[i]][[j]], "path") = path
    }
  }
}

  
# Message
if (!is.null(downsample_barcodes_n) & !is.null(downsample_features_n)) {
  CalloutBox(x="Your data has been downsampled to at most {downsample_barcodes_n} barcodes and at most {downsample_features_n} features.", type="note")
} else if (!is.null(downsample_barcodes_n)) {
  CalloutBox(x="Your data has been downsampled to at most {downsample_barcodes_n} barcodes.", type="note")
} else if (!is.null(downsample_barcodes_n)) {
  CalloutBox(x="Your data has been downsampled to at most {downsample_features_n} features.", type="note")
}
```

It is also possible to store the raw and normalized count on disk and only load them into memory when needed. This significantly reduces the memory footprint of downstream analyses. Furthermore, the resulting Seurat object is much smaller because the counts are stored in extra directories. However, this also means that these directories always have to be provided in addition to the Seurat object when running analyses.

```{r}
#| label: read_data_write_on_disk_matrices
#| results: asis

######################################################
# If requested, write counts to matrix directory     #
# else convert to in-memory matrix of type dgCMatrix #
######################################################

# - allows to analyse big datasets
# - requires Seurat v5 and BPcells
on_disk_counts = param("on_disk_counts")
on_disk_use_tmp = param("on_disk_use_tmp")
if (on_disk_use_tmp) {
  on_disk_path = tempdir()
} else {
  on_disk_path = file.path(module_dir, "tmp")
}

for (i in seq(counts_lst)) {
  for(j in seq_along(counts_lst[[i]])) {
    # Metadata gets lost when saving
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
    technology = attr(counts_lst[[i]][[j]], "technology")
    assay = attr(counts_lst[[i]][[j]], "assay")
    sample = attr(counts_lst[[i]][[j]], "sample")
    experiment = attr(counts_lst[[i]][[j]], "orig.ident")
    path = attr(counts_lst[[i]][[j]], "path")
    
    if (on_disk_counts) {
      # Write counts to matrix directory
      # Then open matrix directory for analysis
      counts_lst[[i]][[j]] = WriteCounts_MatrixDir(counts=counts_lst[[i]][[j]],
                                                   path=file.path(on_disk_path, paste(assay, "counts", experiment, sep=".")),
                                                   overwrite=TRUE)
    } else {
      # Convert to in-memory matrix
      counts_lst[[i]][[j]] = as(counts_lst[[i]][[j]], "dgCMatrix")
    }
    
    # Add attributes barcode_metadata and feature_metadata, as well as technology and assay
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]][[j]], "technology") = technology
    attr(counts_lst[[i]][[j]], "assay") = assay
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
  
  if (on_disk_counts) {
    CalloutBox(x="Raw and normalized counts will be stored on disk. The names of the corresponding directories have the following structure: counts/data.assay.dataset.", type="note")
  } else {
    CalloutBox(x="Raw and normalized counts will be kept memory and stored in the Seurat object.", type="note")
  }
}
```

Next, we create the Seurat object. 

::: {.callout-tip title="What is Seurat and which information is contained in a Seurat object?" collapse="true"}

Seurat is an R-package that is used for the analysis of single-cell RNA-seq data. We read pre-processed data as described above, and convert it to a count matrix in R. In the case of single-cell RNA-seq 10x data, the count matrix contains the number of unique RNA molecules (UMIs) per gene and cell. In the case of single-cell RNA-seq SmartSeq-2 data, the count matrix contains the number of reads per gene and cell.

In addition to the count matrix, the workflow stores additional information in the Seurat object, including but not limited to the normalized data, dimensionality reduction and cluster results.

:::

```{r}
#| label: read_data_seurat_object
#| results: asis

# Create Seurat object with assays and metadata for barcodes (cells, nuclei or spots) and features (genes, peaks or proteins)

# List all assays in all datasets
all_assays = purrr::map_depth(counts_lst, 2, attr, "assay") %>% 
  purrr::flatten() %>% 
  unlist() %>% 
  unique()

#################
# Default assay #
#################

# Collect default assay datasets
default_assay = param("default_assay")
has_assay = purrr::map(counts_lst, purrr::pluck_exists, default_assay) %>% purrr::flatten_lgl()
assay_counts_lst = purrr::map(counts_lst[has_assay], purrr::pluck, default_assay)
names(assay_counts_lst) = purrr::map(assay_counts_lst, attr, "orig.ident") %>% unlist()

# Collect barcode metadata (from all assays/datasets)
barcode_metadata = purrr::map_dfr(seq_along(counts_lst), function(i) {
  metadata = data.frame(rowname=colnames(assay_counts_lst[[i]]))
  metadata$orig.ident = attr(assay_counts_lst[[i]], "orig.ident")
  metadata$sample = attr(assay_counts_lst[[i]], "sample")
  metadata$technology = attr(assay_counts_lst[[i]], "technology")
  
  bc_meta = attr(assay_counts_lst[[i]], "barcode_metadata") %>% 
    tibble::rownames_to_column()
  bc_meta$sample = NULL
  bc_meta$technology = NULL
  bc_meta$orig.ident = NULL
  metadata = dplyr::left_join(metadata, bc_meta, by="rowname")
  rownames(metadata) = metadata$rowname
  
  metadata = metadata %>% dplyr::select(-rowname)
  return(metadata)
})
barcode_metadata$orig.ident = factor(barcode_metadata$orig.ident, levels=unique(barcode_metadata$orig.ident))
barcode_metadata$sample = factor(barcode_metadata$sample, levels=unique(barcode_metadata$sample))
barcode_metadata$technology = factor(barcode_metadata$technology, levels=unique(barcode_metadata$technology))

# Collect feature metadata for default assay datasets
# Group by feature name and make unique values
feature_metadata = purrr::map(seq_along(assay_counts_lst), function(i) {
  metadata = attr(assay_counts_lst[[i]], "feature_metadata") %>% tibble::rownames_to_column()
  return(metadata)
}) %>% dplyr::bind_rows()

feature_metadata = feature_metadata %>% 
  dplyr::group_by(rowname) %>%
  dplyr::summarise_all(dplyr::first, na_rm=TRUE) %>% 
  as.data.frame()
rownames(feature_metadata) = feature_metadata$rowname
feature_metadata = feature_metadata %>% dplyr::select(-rowname)

# Remove since we do not need these attributes
for(i in seq_along(assay_counts_lst)) {
  attr(assay_counts_lst[[i]], "barcode_metadata") = NULL
  attr(assay_counts_lst[[i]], "feature_metadata") = NULL
}

# Create Seurat default assay object
assay_obj = SeuratObject::CreateAssay5Object(counts=assay_counts_lst)
if (length(assay_counts_lst) == 1) {
  n = names(assay_counts_lst)
  assay_obj = split(assay_obj, rep(n, ncol(assay_obj)))
}
rownames(assay_obj@meta.data) = rownames(assay_obj)

# Add to Seurat default assay object
assay_obj = SeuratObject::AddMetaData(assay_obj, metadata=feature_metadata)

# Create Seurat object
sc = Seurat::CreateSeuratObject(counts=assay_obj, meta.data=barcode_metadata, assay=default_assay, names.delim=NULL, names.field=NULL)

# Fix NA values in barcode metadata
n =  paste("nCount", default_assay, sep="_")
i = is.na(sc[[]][, n]) %>% which()
sc[[]][i, n] = 0

n =  paste("nFeature", default_assay, sep="_")
i = is.na(sc[[]][, n]) %>% which()
sc[[]][i, n] = 0

################
# Other assays #
################

# Add other assays to Seurat object
for(a in setdiff(all_assays, default_assay)) {
  # Collect assay datasets
  has_assay = purrr::map(counts_lst, purrr::pluck_exists, a) %>% purrr::flatten_lgl()
  assay_counts_lst = purrr::map(counts_lst[has_assay], purrr::pluck, a)
  names(assay_counts_lst) = purrr::map(assay_counts_lst, attr, "orig.ident") %>% unlist()
  
  # Collect feature metadata for default assay datasets
  # Group by feature name and make unique values
  feature_metadata = purrr::map(seq_along(assay_counts_lst), function(i) {
    metadata = attr(assay_counts_lst[[i]], "feature_metadata") %>% tibble::rownames_to_column()
    return(metadata)
  }) %>% dplyr::bind_rows()

  feature_metadata = feature_metadata %>% 
    dplyr::group_by(rowname) %>%
    dplyr::summarise_all(dplyr::first, na_rm=TRUE) %>% 
    as.data.frame()
  rownames(feature_metadata) = feature_metadata$rowname
  feature_metadata = feature_metadata %>% dplyr::select(-rowname)

  # Remove since we do not need these attributes
  for(i in seq_along(assay_counts_lst)) {
    attr(assay_counts_lst[[i]], "barcode_metadata") = NULL
    attr(assay_counts_lst[[i]], "feature_metadata") = NULL
  }
  
  # Create Seurat default assay object
  assay_obj = SeuratObject::CreateAssay5Object(counts=assay_counts_lst)
  if (length(assay_counts_lst) == 1) {
    n = names(assay_counts_lst)
    assay_obj = split(assay_obj, rep(n, ncol(assay_obj)))
  }
  rownames(assay_obj@meta.data) = rownames(assay_obj)

  # Add to Seurat default assay object
  assay_obj = SeuratObject::AddMetaData(assay_obj, metadata=feature_metadata)
  sc[[a]] = assay_obj
  
  # Fix NA values in barcode metadata
  n =  paste("nCount", a, sep="_")
  i = is.na(sc[[]][, n]) %>% which()
  sc[[]][i, n] = 0
  
  n =  paste("nFeature", a, sep="_")
  i = is.na(sc[[]][, n]) %>% which()
  sc[[]][i, n] = 0
  
}

#########
# Other #
#########

# Parse plate information if requested
parse_plate_information = param("parse_plate_information")
plate_information_regex = param("plate_information_regex")
plate_information_update_identity = param("plate_information_update_identity")
  
if (parse_plate_information & any(grepl(pattern=plate_information_regex, x=sc$orig_barcode))) {
  # Parse plate information
  plate_information = ParsePlateInformation(sc$orig_barcode, pattern=plate_information_regex)
  rownames(plate_information) = colnames(sc)
  rest = plate_information$Rest
  plate_information$Rest = NULL
  
  # Add to seurat metadata
  sc = Seurat::AddMetaData(sc, plate_information)
  
  # If requested, update cell identity (name of the single-cell experiment) - but only for cells where plate information was parsed
  if (plate_information_update_identity) {
    have_plate_information = !is.na(plate_information$PlateRow) & !is.na(plate_information$PlateCol)
    sc$orig.ident = ifelse(have_plate_information, rest, as.character(sc$orig.ident))
    sc$orig.ident = factor(sc$orig.ident, levels=unique(sc$orig.ident))
    
   for (assay in Seurat::Assays(sc)) { 
      # Join layers and then split again
      # First, join layers to "counts"
      layer_orig = SeuratObject::Layers(sc, assay=assay)
      sc[[assay]] = SeuratObject::JoinLayers(object=sc[[assay]])
    
      # If the original data had just one layer, remove it 
      if (length(layer_orig) == 1) SeuratObject::LayerData(sc, layer=layer_orig, assay=assay) = NULL

      # Split the "counts" layer according to samples
      sc[[assay]] <- split(sc[[assay]], f=sc$orig.ident)
    }
  }
}

# Set default identity
Seurat::Idents(sc) = "orig.ident"

# Add datasets to Seurat object
sc@misc$datasets = datasets

# Remove cells with zero counts
names_nonzero_cells = sc[[]] %>% dplyr::filter(.data[[paste0("nCount_", default_assay)]] > 0) %>% rownames()
n_zero_cells = sc[[]] %>% dplyr::filter(.data[[paste0("nCount_", default_assay)]] == 0) %>% nrow()
if (n_zero_cells > 0) {
  sc = subset(sc, cells=names_nonzero_cells)
  CalloutBox("Removed {n_zero_cells} cells with zero counts.", type="note")
}
```

This is a summary representation of the object:

```{r}
#| label: read_data_print_seurat_object

# Print
sc
```

```{r}
#| label: read_data_colors

# Add colours
set.seed(1)
sc = ScAddColours(sc, colours=list(
  orig.ident=setNames(zeileis_28 %>% sample(size=length(levels(sc$orig.ident))), levels(sc$orig.ident)),
  sample=setNames(zeileis_28 %>% sample(size=length(levels(sc$sample))), levels(sc$sample)),
  technology=setNames(zeileis_28 %>% sample(size=length(levels(sc$technology))), levels(sc$technology)),
  seurat_clusters=setNames(godsnot_102 %>% sample(size=length(godsnot_102)), 1:length(godsnot_102)),
  Phase=c(G1="#FDD086", S="#9FD7E1", G2M="#FF8DA1")
))
```

For spatial datasets, we read the image information and add it to the Seurat object.

```{r}
#| label: read_data_read_images

# Read images

# Add images (10x Visium and 10x Xenium only)
for (i in seq_along(counts_lst)) {
  technology = attr(counts_lst[[i]][[1]], "technology")
  orig_ident = attr(counts_lst[[i]][[1]], "orig.ident")
  path = attr(counts_lst[[i]][[1]], "path")
  default_assay = param("default_assay")
  
  # Two technologies have image data
  if (technology == "10x_visium") {
    # 10x Visium: image is in subdirectory 'spatial'
    image_dir = file.path(dirname(path), "spatial")
    assertthat::assert_that(dir.exists(image_dir),
                                msg=FormatString("10x Visium dataset {path} does not have a directory 'spatial' with spatial data."))
  } else if (technology == "10x_xenium") {
    # 10x Xenium_ image is in the same directory
    image_dir = dirname(path)
  } else {
    # other: skip
    next
  }
  
  # For renaming and reordering of barcodes in image
  barcodes = sc[[]] %>%
    dplyr::filter(orig.ident==orig_ident) %>%
    dplyr::select(orig_barcode)
  barcodes = setNames(rownames(barcodes), barcodes$orig_barcode)
  j = match(barcodes, Cells(sc))
  barcodes = barcodes[order(j)]
  
  # Read image
  spatial_coordinate_type = param("spatial_coordinate_type")
  with_progress({
      image = ReadImage(image_dir=image_dir, 
                        technology=technology, 
                        assay=default_assay, 
                        barcodes=barcodes,
                        coordinate_type=spatial_coordinate_type)
  }, enable=verbose)
  
  # Add image (10x visium) or field of view (10x xenium)
  if (technology == "10x_visium") {
    key = "image"
  } else if (technology == "10x_xenium") {
    key = "fov"
  }
  key = paste0(key, "." , orig_ident)
  image@key = paste0(gsub("[^[:alnum:]]", "", key), "_")
  sc[[key]] = image
  
  # If additional barcode metadata is provided, add as well
  barcode_metadata = attr(image, "barcode_metadata")
  if (!is.null(barcode_metadata)) {
    sc = Seurat::AddMetaData(sc, barcode_metadata)
  }
  
  sc
}
```

## Basic statistics

Finally, we calculate some basic statistics such as the precentage of counts in the top n features.

```{r}
#| label: read_data_basic_stats

# Calculate basic statistics for barcodes (cells, nuclei or spots)
# Top n features for calculation of percentage of counts
top_n = c(5, 50)

############
# Barcodes #
############
# Calculate for each assay
for (a in Seurat::Assays(sc)) {

  # Add percentage of counts in top n features
  m = paste0("pCountsTop", top_n) %>% paste(a, sep="_")
  sc[[m]] = 0

  # And each layer (dataset)
  for (l in SeuratObject::Layers(sc[[a]], "^counts")) {

    # Get data for layer (sample)
    counts = sc[[a]][l]
    bcs = counts %>% colnames()
    total_counts = sc[[]][bcs, paste0("nCount_", a), drop=TRUE]
    names(total_counts) = bcs
    
    # Get counts of the top 50 features per barcode
    with_progress({
      top_n_counts = SumTopN(counts, top_n=top_n, margin=2, chunk_size=10000)
    }, enable=verbose)
    
    # Then calculate percentages for topn
    top_n_perc = purrr::map(top_n_counts, function(x) {
      p = ifelse(x>0, x/total_counts*100, 0)
      return(p)
    })

    # Add to metadata
    for(i in seq(top_n)) {
      m = paste0("pCountsTop", top_n[i]) %>% paste(a, sep="_")
      sc[[m]][names(top_n_perc[[i]]), ] = top_n_perc[[i]]
    }
  }
}

############
# Features #
############

# Calculate basic statistics for features (genes, peaks or proteins)

# Calculate for each assay
for (a in Seurat::Assays(sc)) {
  # And each layer (dataset)
  for (l in SeuratObject::Layers(sc[[a]], "^counts")) {
    # Get data for layer (sample)
    counts = sc[[a]][l]
    bcs = counts %>% colnames()
    total_counts = sc[[]][bcs, paste0("nCount_", a), drop=TRUE]
    counts_perc = t(t(counts)/total_counts)*100
    
    # Calculate feature filters
    num_bcs_expr = rowSums(counts >= 1)

    # Calculate median percentage of counts per feature
    with_progress({
      counts_median = CalculateMedians(counts_perc, margin=1, chunk_size=2000)
    }, enable=verbose)
    
    # Add to metadata
    n = gsub(x=l, pattern="counts\\.", replacement="")
    sc[[a]][[paste("nBcs", n, sep="_")]] = num_bcs_expr
    sc[[a]][[paste("medianPerc", n, sep="_")]] = counts_median
  }
}
```

The following table shows available metadata (columns) of the first 5 cells (rows). These metadata provide additional information about the cells in the dataset, such as the sample a cell belongs to ("orig.ident"), the number of mapped reads (“nCounts_RNA”), or the above mentioned number of unique features detected ("nFeature_RNA").

```{r}
#| label: tbl-read_data_barcode_metadata
#| tbl-cap: "Barcode metadata"

# This is the metadata for barcodes (cells, nuclei or spots)

df = sc[[]] %>% head(5)
gt(df, rownames_to_stub=TRUE)
```

The second table shows available metadata (columns) of the first 5 genes (rows) for each assay.

```{r}
#| label: read_data_feature_metadata
#| results: asis

# This is the metadata for features (genes, peaks or proteins)

# How to dynamically generate tables: https://stackoverflow.com/questions/73585417/iterating-to-create-tabs-with-gt-in-quarto
# String to create a chunk for each assay ({{assay}} is placeholder)
chunk_template = "
  
##### {{assay}}

\`\`\`{r}
#| label: tbl-read_data_feature_metadata_{{assay}}
#| tbl-cap: 'Feature metadata for assay {{assay}}'

sc[['{{assay}}']][[]] %>% head(5) %>% gt(rownames_to_stub=TRUE)
\`\`\`

"

cat("::: panel-tabset\n")
for (a in Seurat::Assays(sc)) {
  chunk_filled =  knitr::knit_expand(text=chunk_template, assay=a)
  
  if(interactive()) {
    print(EvalKnitrChunk(chunk_filled))
  } else {
    chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
    cat(chunk_filled, sep='\n')
  }
}
cat(":::\n")
```

## Preliminary QC of the data

```{r}
#| label: read_data_basic_summary
#| results: asis

# Plot basic sumary for barcodes (cells, nuclei or spots)

orig_idents = levels(sc$orig.ident)

# Do this per assay
for (a in Seurat::Assays(sc)) {
  cat("### Assay", a, "\n\n")
  
  barcode_qc = c("nCount", "nFeature")
  barcode_qc = paste(barcode_qc, a, sep="_")
  
  # VIOLIN PLOTS
  # Make violin plots and combine into patchworked object
  plist = PlotBarcodeQC(sc, qc=barcode_qc, log10="nCount")
  p = patchwork::wrap_plots(plist, ncol=2)
  
  # Make chunk template for violin plots
  chunk_template = "
\`\`\`{r}
#| label: fig-read_data_basic_summary_{{assay}}
#| fig-cap: {{caption}}
#| fig-height: 4

p
\`\`\`
"
  
  # Fill out and print violin plots
  chunk_filled =  knitr::knit_expand(text=chunk_template, assay=a, caption=FormatString("Number of counts and features for assay {a}"))
  if(interactive()) {
    print(EvalKnitrChunk(chunk_filled))
  } else {
    chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
    cat(chunk_filled, '\n')
  }
  
  ## SPATIAL PLOTS
  if (length(SeuratObject::Images(sc)) > 0) {
    
    # For each QC metric
    for(q in barcode_qc) {
      
      # Make spatial plots split by sample
      plist = FeaturePlotSpatial(sc, features=q, combine=FALSE) %>% purrr::flatten()
      plist = purrr::map(plist, function(p) {
        p =  p + theme(legend.title=element_blank())
        p = p + viridis::scale_fill_viridis(trans=ifelse(grepl(pattern="nCount", q), "log10", "identity"))
        return(p)
      })
      
      # Make captions
      orig_idents = SeuratObject::Images(sc) %>% gsub("^(fov|image)\\.", "", .)
      captions = GeneratePlotCaptions(q, assay_names=SeuratObject::Assays(sc)) %>% paste("for dataset {orig_idents}") %>% FormatString()
      
      # Make chunk template for spatial plots
      chunk_template = "
\`\`\`{r}
#| label: fig-read_data_basic_summary_spatial_{{qc}}_dataset_{{dataset}}
#| fig-cap: {{caption}}
#| dev: png

plist[[{{i}}]]
\`\`\`
"
      # Fill out and print spatial plots
      cat("::: panel-tabset\n")
      for(i in seq_along(plist)) {
        cat ("##### ", orig_idents[i], "\n\n")
        chunk_filled =  knitr::knit_expand(text=chunk_template, qc=q, i=i, dataset=orig_idents[i], caption=captions[i])
        if(interactive()) {
          print(EvalKnitrChunk(chunk_filled))
        } else {
          chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
          cat(chunk_filled, '\n')
        }
      }
      cat(":::\n")
    }
  }
}
```

## Output

### Annotation 

```{r}
#| label: read_data_save_annotation
#| results: asis

# Write annotation
outfile = paste0("annotation_", default_assay, ".xlsx")
openxlsx::write.xlsx(x=sc[[default_assay]][[]], file=file.path(module_dir, "results", outfile), rowNames=TRUE)

# Note
CalloutBox("Feature annotation for the default assay {default_assay} is written to {outfile}.", type="note")
```

## Software

```{r}
#| label: read_data_save_software  

sessioninfo = ScrnaseqSessionInfo(getwd()) %>% as.data.frame()
gt(sessioninfo)
```

## Parameter

```{r}
#| label: read_data_save_parameter  

paraminfo = ScrnaseqParamsInfo(param()) %>% as.data.frame()
gt(paraminfo)
```

```{r}
#| label: read_data_save_seurat
#| results: asis

# Save Seurat object and layer data
outdir = file.path(module_dir, "sc")
with_progress({
  SaveSeuratRdsWrapper(sc,
                       outdir=outdir,
                       copy_disk_data=on_disk_counts,
                       relative_paths=FALSE,
                       compress=FALSE
                       )
}, enable=verbose)
```

```{r}
#| label: read_data_finish

# Stop multisession workers
plan(sequential)
```
